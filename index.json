[{"categories":[],"content":"Khi biết đến Linux, tôi đã yêu em lúc nào không hay","date":"27 Sep 2023","objectID":"/linux/","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/"},{"categories":[],"content":"Xin chào xin chào là OG đâyy ! Ở bài trước OG đã kể cho các bạn nghe về các khái niệm cơ bản trong ngành Data rồi. Thế thì ở số này mình sẽ nói nhiều hơn về viên gạch đầu tiên khi mình đến với Data Engineer. Đó chính là việc mình biết đến hệ điều hành Linux, kể từ đó cuộc đời mình đổi thay. Lets goo! ","date":"27 Sep 2023","objectID":"/linux/:0:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#"},{"categories":[],"content":"Quá khứ bi đátTrước khi đến với người anh bạn cánh cụt cư tê kia, mình đã từng làm việc với chiếc laptop cùi bắp (intel pentium) win 8.1. Khi đó OG vừa bước chân vào giảng đường đại học, và cứ nghĩ rằng thế này đã đủ xài, có thể hơi chậm chạp nhưng có lẽ sẽ sống ổn. Ôi thật ngây thơ 😢 ngay nửa học kì đầu tiên của năm học mới, cái laptop đó đã dốc từng nhịp thở và chạy chương trình bằng cả tính mạng. Đó là trải nghiệm học tập không mấy dễ chịu khi phải đối mặt với con lap ì ạch như vậy trong suốt một khoảng thời gian dài. Và một combo hủy diệt đi kèm với sự ì ạch đó: hệ điều hành WINDOW, đặc biệt là phiên bản 8 và 8.1 . OG trước đây là một fan của window, phải thú thực là vậy. Giao diện dễ dùng, thân thiện. Nhưng có 1 vấn đề, nó quá nặng và chậm chạp (ít nhất là đối với laptop của OG). Còn cả về lỗi và các bản update mà chỉ khi dùng window, OG mới cảm nhận được sự đau khổ. Thế là có 1 ngày nọ, một người bạn đã chỉ mình chuyển sang Linux và cuộc sống lập trình của mình bước sang một trang mới toanh. ","date":"27 Sep 2023","objectID":"/linux/:1:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#quá-khứ-bi-đát"},{"categories":[],"content":"Linux - Tiếng sét ái tìnhTrước khi được giới thiệu về Linux, OG hoàn toàn không biết gì về nó trước đó. Từ Terminal, Distro,… Hoàn toàn là tờ giấy trắng tinh. Tuy nhiên OG vẫn chọn hệ điều hành này vì nó NHANH, rất nhanh là đằng khác. Ngay khoảnh khắc đó, OG đã không biết mình đã mê đắm hđh này mất rồi 😘 Và sau một thời gian học tập và làm việc với anh bạn 🐧 cư tê thì OG đã không còn muốn quay lại cửa sổ (window) lần nào nữa. Vậy Linux có gì hay hãy đọc tiếp nhé ","date":"27 Sep 2023","objectID":"/linux/:2:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#linux---tiếng-sét-ái-tình"},{"categories":[],"content":"Hệ điều hành chất chơi người dơiLinux là một hệ điều hành được phát triển từ năm 1991 bởi Linus Torvalds dựa trên hệ điều hành Unix. Chú cánh cụt này được viết bằng C và đảm nhận nhiệm vụ nhận các request từ chương trình trên máy tính và chuyển chúng đến phần cứng. Điều đặc biệt nhất có lẽ là nó được phát hành miễn phí (open source). Nghĩa là bạn có thể dễ dàng cài Linux một cách quang minh chính đại mà không cần phải crack hay “Đi cửa sau” với nhiều rủi ro lỗi như khi sử dụng hđh khác. Thậm chí bạn có thể chỉnh sửa, đóng góp xây dựng thêm cho hệ điều hành này nếu muốn. Không những vậy, cộng đồng hỗ trợ của hệ điều hành này cũng rộng lớn và vô cùng “bá đạo” về nhiều khía cạnh 😂 Bạn sẽ có thể gần như ngay lập tức được giải đáp hầu hết các thắc mắc hay trục trặc khi sử dụng linux. Cộng đồng này đa số là các lập trình viên, nhà phát triển,… Vì chú cánh cụt này hệ điều hành ưu thích và thích hợp để phát triển sản phẩm hoặc xây dựng hệ thống nhờ là ứng dụng mã nguồn mở và tính bảo mật cao. Khi chuyển sang dùng Linux, bạn cũng không cần phải quá lo lắng về việc thay đổi môi trường và thích nghi với hệ điều hành này. Vì cơ bản Linux vẫn giống với những hệ điều hành trước đây bạn dùng thôi. Bạn vẫn có thể thao tác với giao diện hệ điều hành, vẫn sử dụng được các phần mềm như xử lý văn bản, edit video hay ảnh,… Nghĩa là các thao tác sử dụng thông thường, Linux vẫn đáp ứng tốt và còn rất nhanh. ","date":"27 Sep 2023","objectID":"/linux/:2:1","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#hệ-điều-hành-chất-chơi-người-dơi"},{"categories":[],"content":"ShellHẳn là có thể đâu đó, bạn thấy một anh chàng ngầu lòi nào đó bấm vài dòng lệnh, Bụp enter. Rồi tự nhiên một đống chữ xuất hiện dồn dập trên cái nền terminal tím. Ồ có thể anh ta đang sử dụng shell đấy. Hoặc là một lập trình viên, bạn đã quá chán với giao diện Command Prompt và PowerShell ? Hãy chuyển sang Linux và bạn sẽ bước vào thế giới huyền diệu khi sử dụng terminal. Terminal của OGUbuntu 22.04.3 LTS \" Terminal của OG Shell là một chương trình phát triển dành cho các máy tính chạy trên hệ điều hành Unix và Linux. Phần mềm này cũng cấp giao diện người dùng nhập và giao tiếp với máy tính dưới dạng văn bản. Và trên các máy tính Ubuntu thì Shell cũng có thể gọi là Terminal. Đối với người dùng Linux, việc thao tác với terminal là điều gần như hiển nhiên. Đặc biệt thích hợp cho các “cót đơ” hay nhà phát triển. Việc chạy lệnh trên Terminal ngoài việc trông có vẻ “nguy hiểm” hơn, trải nghiệm sử dụng shell cũng được cho là thoải mái hơn trên window rất nhiều. Không những vậy, với một cộng đồng cực lớn, việc làm quen với shell dường như đã trở nên dễ dàng hơn bao giờ hết. Việc sử dụng shell như thế nào có lẽ OG sẽ để dành lại vào 1 bài khác nhé hihi ","date":"27 Sep 2023","objectID":"/linux/:2:2","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#shell"},{"categories":[],"content":"Đa dạng phiên bảnVì là sản phẩm mã nguồn mở, Linux có rất nhiều phiên bản khác nhau. Các Linux Distro là phần đã góp phần vào sự thú vị nói chung của hệ điều hành này và giúp cho chúng ta có đa dạng sự chọn lựa cho phiên bản mình yêu thích. Hiện có khoảng 600 bản Distro và hơn nửa trong số đó được liên tục phát triển và cải thiện. Có đa dạng các distro và phù hợp với đa dạng các nền tảng khác nhau từ desktop, laptop, di động,… Và cũng nhắm tới phục vụ cho đa dạng các đối tượng khác nhau. Hãy cùng xem qua một số distro phổ biến nhất nào Ubuntu: đây là phiên bản đông đảo người dùng sủ dụng nhất. Đây chính là một nhánh của Debian Linux. Ubuntu có giao diện dễ nhìn và dễ tiếp cận, do đó người dùng mới không khó để làm quen và sử dụng. Hơn nữa là cộng đồng sử dụng lớn nên bạn sẽ dễ dàng tìm được giải đáp cho các thắc mắc mình trong khi sử dụng. Và OG cũng đãng sử dụng Distro này 😄 Ubuntu 23.04 Lunar LobsterUbuntu \" Ubuntu 23.04 Lunar Lobster Linux Mint: Là một trong những distro tốt nhất dành do người dùng Linux mới. Giao diện Desktop của Mint rất là Window, tạo cảm giác vô cùng quen thuộc và dễ thích nghi đối với người dùng mới chuyển hoặc ưu thích Window. Đây cũng là một trong những điểm đặc cắc của distro này. Ngoài ra Linux Mint dựa trên Ubuntu, do đó cũng có thể chạy ứng dụng dành cho Ubuntu. Linux Mint 21 “Vanessa”Mint \" Linux Mint 21 “Vanessa” Fedora: Trước đây gọi là Fedora core, được phát triển dựa trên cộng đồng theo Fedora Project và được bảo trợ bởi Red Hat (Công ty con của IBM). Đây là một trong những bản phân phối có tốc độ nhanh và ổn định nhất. Được ưu chuộng bởi các nhà phát triển. Fedora Linux 35Fedora \" Fedora Linux 35 CentOS: Một trong những phân phối Linux dành cho môi trường server, CentOS dựa trên mã nguồn mở của Red Hat Enterprise Linux (RHEL) và miễn phí. CentOS có độ ổn định cao và được nhiều công ty ưu chuộng. CentOS 7.0CentOS \" CentOS 7.0 Arch Linux: Bản phân phối này là một “Rolling Release”, đại loại như là nó sẽ luôn được cung cấp bản cập nhật phần mềm sớm nhất có thể và cải thiện tốt nhất. Điều đặc biệt của distro này là khả năng cá nhân hóa rất cao, bạn thậm chí sẽ không nhận được giao diện đồ họa khi mới bắt đầu. Đổi lại bạn sẽ có thể thiết kế, xây dựng theo sở thích của chính mình. Arch LinuxArch Linux \" Arch Linux Và còn rất rất nhiều phân phối khác thú vị nữa mà kể ra chắc phải thành sách mất 😂 ","date":"27 Sep 2023","objectID":"/linux/:2:3","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#đa-dạng-phiên-bản"},{"categories":[],"content":"Linux vs WindowHiện nay có 3 hệ điều hành phổ biến là Window, Linux, MacOS. Đối với Mac thì OG chưa trải nghiệm (vì mình nghèo 😢). Do đó chúng ta sẽ so sánh trải nghiệm giữa 2 hệ điều hành này theo cảm nhận của OG sau một thời gian sử dụng Linux và cả Window nhé Đối tượng sử dụng: Đối với Window, đã quá phổ biến rồi, do đó đối tượng hướng đến cũng rất rộng rãi già trẻ lớn bé đều có thể sử dụng dễ dàng sử dụng. Ngược lại, những người sử dụng Linux thường là các lập trình viên và các nhà phát triển. Họ là người hiểu về hệ thống và có thể kiểm soát hệ thống. Bash Shell: Đầu tiên, đối với việc lập trình cần phải thao tác nhiều với hệ thống thì việc sử dụng cmd hay PowerShell gần như là 1 trải nghiệm “đồ đá” đối với OG. Window 10 có hỗ trợ cmd và PowerShell nhưng cả 2 loại này đều có giao diện cũ kĩ và cú pháp dài dòng, khó nhớ. Trong khi đó, sử dụng Bash Shell ở Linux đưa lại trải nghiệm mượt mà hơn với câu lệnh dễ nhớ, màu sắc cũng đa dạng hơn và tất nhiên là cũng dễ dùng hơn rất nhiều. Xử lý, can thiệp mã nguồn: Trong khi sử dụng Window, việc truy cập mã nguồn gần như là bất khả thi. Thì đối với người dùng Linux lại ngược lại, bạn hoàn toàn có khả năng truy cập hay chỉnh sửa mã nguồn đến tận nhân. Việc cấu hình hệ thống theo sở thích cá nhân hoàn toàn là điều khả thi và tạo tính linh hoạt khi sử dụng. Xử lý văn bản, hình ảnh: Về phương diện này, phải nói là không thể qua được Microsoft Office 365 của Window, tất nhiên Linux cũng có LibreOffice, tuy nhiên phần mềm này vẫn còn khá hạn chế và kém. Ngoài ra việc sử dụng các công cụ chỉnh sửa hình ảnh, video như Adobe Photoshops hay Adobe Premiere trên linux cũng là không thể. Gaming: Thành thật mà nói, nếu bạn sử dụng linux, thì hãy tập bỏ dần thói quen chơi game đi 😂 vì hầu hết các game hỗ trợ tốt nhất trên hệ điều hành Window Độ bảo mật: Tất nhiên, không thể phủ nhận Window là một hệ điều hành cực kỳ bảo mật. Nhưng nó luôn là mục tiêu của các cuộc tấn công vì độ phổ biến quá lớn của nó. Trong khi người dùng Linux hầu hết là các lập trình viên và tất nhiên là cả Hacker. Okay nói quá trời nói thì tóm gọn qua bảng sau đây: Tiêu chí Window Linux Đối tượng Người dùng phổ thông với nhu cầu cơ bản: lướt web, công việc văn phòng,… Lập trình viên, nhà phát triển, hacker,… Giao diện Command Line Thô sơ, khó dùng với Command Prompt, PowerShell Mạnh mẽ với nhiều loại shells (Bash, Zsh,…) Khả năng tùy chỉnh mã nguồn Không thể Hoàn toàn có thể, giúp cá nhân hóa và tạo tính linh hoạt Xử lý văn bản, phim ảnh Rất tốt với bộ Office 365, Adobe Photoshops,… Có thể sử dụng LibreOffice chỉnh sửa cơ bản nhưng chức năng ít Game Hỗ trợ tốt Khá khó khăn Độ bảo mật Tốt, nhưng dễ bị nhắm tới Tốt và có tính bảo mật cao Việc học lập trình hay quản lý hệ thông, có lẽ Linux sẽ là lựa chọn hoàn hảo hơn (hoặc nếu có điều kiện, hãy mua Mac 😄 thật đấy !). Còn nếu bạn là người dùng phổ thông, không có nhu cầu phải biết về lập trình, quản lý hạ tầng,… thì cứ cửa sổ mà dùng thôi hihi ","date":"27 Sep 2023","objectID":"/linux/:2:4","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#linux-vs-window"},{"categories":[],"content":"Muốn thì tìm cáchNếu bạn là một người dùng Mac, chúc mừng bạn đã quay vào ô an toàn 😄 sướng nhất bạn rồi hi. Còn nếu sử dụng Window mà bất giác có một tình yêu với Linux thì phải làm sao đây ? Không lẽ phải bỏ Win cài lại Linux sao ? Câu trả lời là Không. Hiện nay có 3 cách chính để bạn có thể sử dụng Linux mà không cần phải từ bỏ chiếc cửa sổ của mình: Dùng máy ảo: Việc đầu tiên mà hầu hết mọi người nghĩ tới khi dùng một hệ điều hành khác với hệ điều hành chính của máy chính là dùng một con máy ảo. Bạn có thể sử dụng VirtualBox hoặc VMware PLayer một cách miễn phí để cài đặt. Nhưng với OG, sử dụng máy ảo chả khác nào một cực hình, vì nó chậm kinh khủng khiếp. Vì loại máy ảo này chạy trên hệ điều hành máy và sử dụng công nghệ ảo hóa của CPU để tạo ra các máy ảo khác. Cơ bản là phải thông qua một OS trung gian (Window), do đó tốc độ của nó thực sự như rùa. Dùng dual boot: Cơ bản cách này là cài đặt cho máy tính bạn chạy được cả 2 hệ điều hành cùng một lúc, cách này được khá nhiều người sử dụng và tất nhiên là nhanh hơn việc sử dụng máy ảo. Dùng WSL: hay còn được gọi là Window Subsystem for Linux, về bản chất mà nói thì đây cũng là một loại máy ảo. Tuy nhiên nó là loại máy ảo chạy trên một nền tảng ảo hóa của CPU là Hyper-V, và không hề thông qua hệ điều hành trung gian nào. Có thể hiểu là công nghệ Hyper-V là công nghệ ảo hóa của CPU cung cấp các tầng ảo hóa, kể cả cho chính Window của bạn. Việc sử dụng máy ảo loại này thực sự nhanh hơn rất nhiều và cũng không cần phải restart máy để chuyển sang OS khác như cách dual boot mà đơn giản chỉ là tắt wsl trên PowerShell đi là xong. Quá đã phải không nào. Đối với OG thì mình đang dùng cách 3 để có thể sử dụng Linux trên Window một cách hoàn toàn miễn phí và trải nghiệm vô cùng ổn định. Microsoft đã tạo ra WSL để giúp giải tỏa cơn khát Linux của người dùng Window và giúp cho cuộc sống các lập trình viên thêm ngọt ngào dễ sống hơn 😂 Hoặc bạn có thể ghé thăm page Ubunchuu trường ú để tìm hiểu tất tần tật mọi thứ về Linux và Ubuntu nhé. Đây là kênh do một nhóm các sinh viên đam mê với linux của HCMUS thành lập nhằm tạo cộng đồng linux giúp đỡ lẫn nhau. Và tất nhiên là hướng đến các bạn người mới, newbie với Ubuntu rồi. ","date":"27 Sep 2023","objectID":"/linux/:3:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#muốn-thì-tìm-cách"},{"categories":[],"content":"Cúng cùiVà đó là tất cả cảm nhận của OG về việc sử dụng Linux, cũng như là giới thiệu một chút về chú cánh cụt 🐧 cư tê này. Tất cả chỉ là cảm nhận cá nhân của OG trong quá trình sử dụng Linux, hy vọng rằng sẽ giúp bạn cảm thấy thú vị. -Mew- ","date":"27 Sep 2023","objectID":"/linux/:4:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#cúng-cùi"},{"categories":[],"content":"Data lú #1 Thằng nhóc thích code và data Ngành Data có gì hot mà mình lại dính Read more... #2 Linux - Tiếng sét ái tình Khi biết đến Linux, tôi đã yêu em lúc nào không hay Read more... ","date":"24 Sep 2023","objectID":"/blogs/:0:0","series":[""],"tags":[],"title":"Blogs","uri":"/blogs/#data-lú"},{"categories":[],"content":"Warning Đây là kiến thức tích góp từ nhiều nguồn và tìm hiểu của nhóm OG, tất nhiên không thể tránh khỏi sai sót. Hy vọng bài viết lần này thú vị và giúp bạn đọc thư giãn, tham khảo. PhongHuynh0394 Stock-Analysis Hé lô hé lô là OG đâyy, ở phần 1, ta đã giảm chiều dữ liệu bằng PCA rồi, tiếp đến phần này, chúng ta sẽ dùng K-means để phân cụm rồi tìm ra điểm chung của dữ liệu nhé, cuối cùng là phân tích các “sự kiện” đã diễn ra trong từng cụm. Lét gô ! Một lần nữa xin cảm ơn thầy Nguyễn Hoàng Đức và thầy Ngô Minh Mẫn đã hỗ trợ để đồ án được hoàn thiện. Đồng thời cảm ơn các thành viên nhóm 7 đã cùng làm việc hết mình. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:0:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#"},{"categories":[],"content":"K-Means ClusteringNhắc lại chút xíu, ở Phần 1 chúng ta đã dùng PCA làm giảm chiều dữ liệu nhưng xét về mặt ý nghĩa, dữ liệu sau PCA chưa thể phân tích được mà ta sẽ chỉ dùng nó để áp vào một kỹ thuật tiếp theo. Kỹ thuật này sẽ “gộp nhóm” dữ liệu và trả lời câu hỏi “làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau, sao cho dữ liệu trong cùng một cụm có tính chất giống nhau?” Bản chất của việc phân nhóm này dựa trên những biến động, sự kiện nào đó trên thị trường mà chúng ta chưa biết nhưng nó chi phối trực tiếp hay gián tiếp, nhiều hay ít đến với các cổ phiếu có liên quan. Chính vì lý do đó, chúng ta tiến hành sử dụng thuật toán K-Means Clustering để phân cụm dữ liệu phục vụ mục tiêu đề ra. Trong bài này chúng ta sẽ thực hiện thuật K-means step by step, nào bắt đầu thoaii 😄 ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#k-means-clustering"},{"categories":[],"content":"K-Means step by stepStep 1: Lựa chọn số clusters $K$ # initialize labels N, d = X_pca.shape pre_labels = np.zeros((N, 1)) # Hyper-parameters K_CLUSTERS = 3 # \u003c\u003c N Step 2: Chọn K điểm ngẫu nhiên từ dữ liệu làm trọng tâm import random # random K-samples to be centroids k_indices = random.sample(range(0, N), K_CLUSTERS) centroids = X_pca[k_indices] # shape: (K, d) Step 3: Gán tất cả các điểm cho tâm cụm gần nhất Hàm assign_cluster() giúp ta phân mỗi điểm dữ liệu vào cluster có center gần nó nhất với K điểm bất kỳ được chọn làm các center ban đầu def assign_cluster(distances: np.ndarray) -\u003e np.ndarray: # distances: (N, K) return np.argmin(distances, axis=1) # return min value from distances array Cụ thể: Mảng distances chứa khoảng cách từ điểm $i$ đến cụm $k$, đây là khoảng cách Euclid với công thức distance$(x,y)=||x-y||$ Hàm np.argmin() trả về giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất. Nói cách khác, nó giúp chúng ta tìm ra được vị trí mà điểm dữ liệu phải thuộc về dựa trên quy tắc gần cụm nào nhất thì chọn cụm đó. Ví dụ: # Example for np.argmin() # k = 0 1 2 D = np.array([[1, 0, 3], # min = 0 and it locate in 1 -\u003e x_1 y_1 = 1 [-1, 2, 1]]) # min =-1 and it locate in 0 -\u003e x_2 y_2 = 0 np.argmin(D, axis=1) # --\u003e array([1,0]) Step 4: Tính toán lại các trọng tâm của các cụm mới được hình thành Hàm update_centroids() dùng cập nhật lại tâm cụm và trả về 1 bộ các tâm cụm mới bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó. def update_centroids(X, labels): new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K_CLUSTERS)]) return new_centroids # (3, 13) Cuối cùng: lặp lại step 3 và step 4 Question Khi nào thì bài toán hội tụ vậy OG ? Khi nào thì chúng ta mới dừng lại thuật toán ? –\u003e Câu trả lời là khi việc phân cụm không còn sự thay đổi nào nữa hoặc giá trị hàm mất mát không thay đổi nhiều sau mỗi lần update tâm cụm. Chúng ta sẽ viết một hàm kiểm tra tính hội tụ của bài toán là has_convert(). Hàm này kiểm tra cụm trước và sau có giống nhau không. # Check convergence def has_convert(pre_labels: np.ndarray, cur_labels: np.ndarray) -\u003e bool: return (pre_labels == cur_labels).all() Hàm get_total_wcv() là một hàm mất mát, tính tổng các phương sai bên trong của 1 cluster Nếu ta coi $m_k$ là center (representation) của mỗi cluster và ước lượng tất cả các điểm được phân vào cluster này bởi $m_k$, thì một điểm dữ liệu $x_i$ được phân vào cluster $k$ sẽ bị sai số là $(x_i-m_k)$. Chúng ta mong muốn sai số này có trị tuyệt đối bé nhất nên ta sẽ tìm cách để đại lượng sau đây đạt min: $||x_i - m_k||^2_2$ def get_total_wcv(X, labels, centroids): # Total within cluster variance WCVs = [ np.sum(np.linalg.norm(X[labels == k] - centroids[k], axis=1) ** 2) \\ for k in range(K_CLUSTERS) ] return np.sum(WCVs) Nhìn chung về điều kiện hội tụ có thể thấy mối liên hệ giữa các điều kiện là gần tương đồng như nhau. Khi có ít điểm dữ liệu được gán sang cluster khác có thể khiến điểm trung tâm không thay đổi nhiều và từ đó hàm mất mát cũng sẽ ít bị ảnh hưởng. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:1","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#k-means-step-by-step"},{"categories":[],"content":"The K-means algorithm is written in object-oriented formTa sẽ kết hợp tất cả các bước thuật toán vào một đối tượng KMeansClustering import numpy as np import pandas as pd import random import matplotlib.pyplot as plt from scipy.spatial import distance class KMeansClustering: \"\"\" An instance of K-Means Clustering algorithm \"\"\" def __init__(self, n_clusters=29): \"\"\" n_clusters: number of clusters _centroids: center/ centroid of clusters inertia_: sum of squared distances of samples to their closest cluster center labels_: labels of input samples X: input data \"\"\" self.n_clusters = n_clusters def fit(self, X: np.ndarray) -\u003e None: \"\"\" K-means execution \"\"\" N, p = X.shape self.X = X # random K-samples to be centroids k_indices = random.sample(range(0, N), self.n_clusters) self._centroids = self.X[k_indices] # initialize labels pre_labels = np.zeros((N, 1)) # training it = 0 while True: # compute distances from X_i to distances = self._calc_dists(self.X, self._centroids) # assign new labels self.labels_ = self._assign_cluster(distances) # assign new labels # check convergence if self._has_convert(pre_labels, self.labels_): break # update centroids self._update_centroids(self.X, self.labels_) pre_labels = self.labels_ it += 1 # compute total Within Cluster Variance (WCV) self.inertia_ = self._calc_total_WCV(self.X, self.labels_, self._centroids) def _calc_dists(self, X, centroids): return distance.cdist(X, centroids, \"euclidean\") def _assign_cluster(self, distances): return np.argmin(distances, axis=1) def _update_centroids(self, X, labels): self._centroids = np.array( [X[labels == k].mean(axis=0) for k in range(self.n_clusters)] ) def _calc_total_WCV(self, X, labels, centroids): WCVs = [ np.sum(np.linalg.norm(X[labels == k] - centroids[k], axis=1) ** 2) for k in range(self.n_clusters) ] return np.sum(WCVs) def _has_convert(self, pre_labels, cur_labels): return (pre_labels == cur_labels).all() def _predict(self, X_test): dist_test = self._calc_dists(X_test, self._centroids) test_labels = self._assign_cluster(dist_test) return test_labels def predict(self, X_test): return self._predict(X_test) Trước khi thật sự áp dụng thuật toán này cho dataset của chúng ta, có một câu hỏi đặt ra là: Thuật toán này thực hiện với đầu vào là số $K$ tức là số lượng cluster, thế thì $K$ bằng bao nhiêu là tốt nhất? ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#the-k-means-algorithm-is-written-in-object-oriented-form"},{"categories":[],"content":"Finding the optimal ‘K’ in a K-Means clusteringCó một phương pháp tên là Elbow aka cái khuỷa tay 😄 Phương pháp Elbow là một cách giúp ta lựa chọn được số lượng các cụm phù hợp dựa vào đồ thị trực quan hoá bằng cách nhìn vào sự suy giảm của hàm biến dạng và lựa chọn ra điểm khuỷu tay (elbow point). Đối với mỗi giá trị của $K$, ta tính toán WCSS (Tổng bình phương trong cụm). WCSS là tổng bình phương khoảng cách giữa mỗi điểm và tâm trong một cụm. def elbow_method(X, k_clusters = list(range(1,9))): total_wcss = [] for k in k_clusters: # Train with k cluster kmeans_model = KMeansClustering(n_clusters=k) kmeans_model.fit(X) # calculate WCSS total_wcss.append(kmeans_model.inertia_) plt.figure() plt.plot(k_clusters, total_wcss, marker='o', color='r') plt.ylabel('WCSS') plt.xlabel('Number of clusters K') plt.grid() plt.show() elbow_method(X_pca) Khi ta vẽ đồ thị WCSS với giá trị K, đồ thị trông giống như một khuỷu tay. Khi số cụm tăng lên, giá trị WCSS sẽ bắt đầu giảm. Giá trị WCSS lớn nhất khi $K=1$ Giá trị WCSS lớn nhất khi $K=1$elbow_method \" Giá trị WCSS lớn nhất khi $K=1$ Chúng ta có thể thấy rằng biểu đồ sẽ thay đổi nhanh chóng tại $K=2$ và do đó tạo ra hình dạng khuỷu tay. Từ thời điểm này, đồ thị di chuyển gần như song song với trục $X$ Giá trị $K$ tương ứng với điểm này là giá trị tối ưu của $K$ hoặc cụm tối ưu. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#finding-the-optimal-k-in-a-k-means-clustering"},{"categories":[],"content":"Evaluation Metrics of K-Means ClusteringSilhouette Score: cho chúng ta biết những điểm dữ liệu hay những quan sát nào nằm gọn bên trong cụm (tốt) hay nằm gần ngoài rìa cụm (không tốt) để đánh giá hiệu quả phân cụm. Giả sử có 2 cluster A, B thì Silhouette score là: $$ s_i = \\frac{(b_i - a_i)}{max(b_i - a_i)} $$ Với $a_i, b_i$ lần lượt là khoảng cách của điể $i$ đến tâm cụm A và B Giá trị này nằm trong khoảng [-1, 1]: Điểm dữ liệu có Silhouette cao, gần bằng 1: nằm đúng trong cluster. Điểm dữ liệu có Silhouette gần bằng 0: nằm giữa 2 cluster Điểm dữ liệu có Silhouette thấp, có giá trị âm: thì khả năng đã nằm sai cluster. from sklearn import metrics kmeans_train = KMeansClustering(2) kmeans_train.fit(X_pca) labels = kmeans_train.labels_ print(f'Silhouette Score (n = 2): {metrics.silhouette_score(X_pca,labels)}') #--\u003e Silhouette Score (n = 2): 0.265365408366845 kmeans_train = KMeansClustering(3) kmeans_train.fit(X_pca) labels = kmeans_train.labels_ print(f'Silhouette Score (n = 3): {metrics.silhouette_score(X_pca,labels)}') # --\u003e Silhouette Score (n = 3): 0.22528216696570613 Ở Phương pháp Elbow, chúng ta cũng đã thấy được $K$ tối ưu cho bài toán chính là khi chọn $K=2$. Nhưng ở bước này, ta cũng thấy được rõ ràng hơn giữa 2 sự lựa chọn $K=2$ và $K=3$ bằng cách so sánh chỉ số Silhouette giữa bọn chúng. So sánh: $0.265(K=2) \u003e 0.225(K=3)$ Từ đó, một lần nữa ta thấy được $K$ tối ưu nhất cho bài toán này là $K=2$ ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:4","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#evaluation-metrics-of-k-means-clustering"},{"categories":[],"content":"VisualizeSau tất cả các bước trên, dữ liệu đã được chia thành 2 label chính tương ứng với từng màu sắc được biểu thị dưới hình vẽ sau đây: # K means import plotly.express as px K_CLUSTERS = 2 kmeans_train = KMeansClustering(K_CLUSTERS) kmeans_train.fit(X_pca) fig = px.scatter_matrix( X_pca, labels=pca_scree, dimensions=range(4), color=kmeans_train.labels_ ) fig.update_traces(diagonal_visible=False) fig.show() K-means cluster with 2 Clusters and various PCKmeans Clustering \" K-means cluster with 2 Clusters and various PC Sau khi chia cụm, chúng ta đến với bước cuối cùng thôi ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:5","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#visualize"},{"categories":[],"content":"Data AnalysisSau khi K-means, ta có được một tập nhãn (labels), ta sẽ gán các nhãn này vào bộ data có giá trị và tiến hành phân tích từng cụm. # Data #set label value_data = new_market.copy() value_data['label'] = kmeans_train.labels_ value_data['label'].replace({0: 'A', 1: 'B'}, inplace=True) # get specific columns gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] mua = [col for col in value_data.columns.to_list() if 'total_mua' in col] ban = [col for col in value_data.columns.to_list() if 'total_ban' in col] # create new valuabel column value_data['total_ban_30'] = value_data[ban].sum(axis=1) value_data['total_mua_30'] = value_data[mua].sum(axis=1) value_data['total_volume_30'] = value_data['total_ban_30'] + value_data['total_mua_30'] value_data['mua_ban_ratio'] = value_data['total_mua_30'] / value_data['total_ban_30'] value_data bao gồm các cột: label: nhãn nhận được từ K means (gồm ‘A’ và ‘B’) gttb_{mã}: Trung bình giá của một {mã} cổ phiếu total_mua_{mã}: Tổng khối lượng mua của {mã} cổ phiếu total_ban_{mã}: Tổng khối lượng bán của {mã} cổ phiếu total_mua_30: Tổng khối lượng mua của tất cả 30 cổ phiếu total_ban_30: Tổng khối lượng bán của tất cả 30 cổ phiếu total_volume_30: Tổng khối lượng giao dịch của 30 cổ phiếu (total_mua_30 + total_ban_30) mua_ban_ratio: tỉ số total_mua_30 / total_ban_30 Gia KL: Giá phái sinh gần với VN30 Time-series Data theo phút (5154 dòng) ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#data-analysis"},{"categories":[],"content":"Analytical Overview import plotly.graph_objects as go import plotly.express as px from plotly.subplots import make_subplots fig = make_subplots(rows=2, cols=2, vertical_spacing=0.03) # plot A fig.add_trace(go.Bar(name='',x=value_data.index[value_data['label'] == 'A'], y=value_data['total_volume_30'][value_data['label'] == 'A']), row=1, col=1) fig.add_trace(go.Scatter(x=value_data.index[value_data['label'] == 'A'], y=value_data['Gia KL'][value_data['label'] == 'A'], mode='lines'), row=2, col=1) # plot B fig.add_trace(go.Bar(name='',x=value_data.index[value_data['label'] == 'B'], y=value_data['total_volume_30'][value_data['label'] == 'B']), row=1, col=2) fig.add_trace(go.Scatter(x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines'), row=2, col=2) # update layout fig.update_layout(title_text=\"Stock Data Visualization\", showlegend=False, barmode='stack') fig.update_yaxes(title_text=\"Total Volume 30\", row=1, col=1) fig.update_yaxes(title_text=\"Gia KL\", row=2, col=1) fig.update_yaxes(title_text=\"Total Volume 30\", row=1, col=2) fig.update_yaxes(title_text=\"Gia KL\", row=2, col=2) fig.show() Stock Data VisualizationStock Data Visualization \" Stock Data Visualization Cluster B chứa các dữ liệu trong giai đoạn từ 20/3 đến 6/4/2023 Nhìn nhận một cách tổng quát: Đối với cluster B, thị trường luôn trong thế “giằng co” với khoảng 6 đợt giảm mạnh và chừng ấy đợt phục hồi liên tục trong suốt 18 ngày. Mặc dù GiaKL duy trì được đà tăng (Giá KL từ 1036 tăng đến 1080) tạo tích cực cho thị trường song thế giằng co vẫn có chiều hướng kéo dài. Cluster A chứa các dữ liệu trong giai đoạn từ 1/4 đến 19/4/2023: Nhìn nhận một cách tổng quát: Đối với cluster A các khoảng giảm kéo dài trong nhiều ngày dẫn đến việc phục hồi gặp khó khăn đáng kể. Cụ thể, GiaKL có mức “vùng dậy” đến ngưỡng 1086 điểm tại 6/4 nhưng lại bị đẩy về 1054 vào cuối giai đoạn. Ta sẽ phân tích sâu hơn từng cluster một ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:1","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#analytical-overview"},{"categories":[],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label BMean Values by Label B \" Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKLB-Correlation between Total Volumn and GiaKL \" B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # --\u003e 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # --\u003e -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # --\u003e -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#cluster-b"},{"categories":[],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label BMean Values by Label B \" Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKLB-Correlation between Total Volumn and GiaKL \" B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # -- 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#biến-động-thị-trường"},{"categories":[],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label BMean Values by Label B \" Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKLB-Correlation between Total Volumn and GiaKL \" B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # -- 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#các-sự-kiện-ảnh-hưởng"},{"categories":[],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label BMean Values by Label B \" Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKLB-Correlation between Total Volumn and GiaKL \" B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # -- 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#kết-luận-xu-hướng"},{"categories":[],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() A-Correlation between Total Volumn and GiaKLA-Correlation between Total Volumn and GiaKL \" A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # --\u003e 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # --\u003e -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục du","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#cluster-a"},{"categories":[],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() A-Correlation between Total Volumn and GiaKLA-Correlation between Total Volumn and GiaKL \" A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # -- 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục du","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#biến-động-thị-trường-1"},{"categories":[],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() A-Correlation between Total Volumn and GiaKLA-Correlation between Total Volumn and GiaKL \" A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # -- 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục du","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#sự-kiện-ảnh-hưởng"},{"categories":[],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() A-Correlation between Total Volumn and GiaKLA-Correlation between Total Volumn and GiaKL \" A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # -- 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục du","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#kết-luận-xu-hướng-1"},{"categories":[],"content":"Kết thúcQua đồ án “Stock Analysis” chúng ta đã thực hành dùng các kỹ thuật như Cleaning data, Scaling, PCA, K-means Clustering… cùng các kỹ năng phân tích dữ liệu để làm rõ insight cùng với tình hình của thị trường chứng khoán phái sinh VN30 trong 31 ngày. Từ đây có thể nhận định xu hướng của chứng khoán trong thời gian tới, đó là xu hướng tích lũy. Một số hợp đồng có thể tham khảo như VN30F2306 và VN30F2Q được dẫn dắt bởi các cổ phiếu thuộc nhóm ngành xây dựng (DIG), ngân hàng (VCB), chứng khoán (SSI)… Cảm ơn bạn đã đọc đến giờ phút này, OG cảm động quá 😄 Hy vọng bài viết này sẽ giúp cho bạn cảm thấy thú vị. Chúc bạn một ngày tốt lành 😄 -Mew- ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:3:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#kết-thúc"},{"categories":[],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Stock Analysis using PCA and K-means Read more... ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:0:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#related"},{"categories":["projects"],"content":"Stock Analysis using PCA and K-means","date":"31 Aug 2023","objectID":"/stock_analysis/","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/"},{"categories":["projects"],"content":"Warning Đây là kiến thức tích góp từ nhiều nguồn và nghiên cứu của nhóm OG, tất nhiên không thể tránh khỏi sai sót. Hy vọng bài viết lần này thú vị và giúp bạn đọc thư giãn, tham khảo. Source PhongHuynh0394 Stock-Analysis Hello! OG đây. Ở project lần này mình sẽ phân tích gia trị cổ phiếu phái sinh VN30 Index bằng cách sử dụng PCA và K-means. Xin vô cùng cảm ơn sự đóng góp của 5 thành viên team OG và thầy Minh Mẫn và thầy Hoàng Đức đã tận tình hướng dẫn để team có thể hoàn thành đồ án một cách tốt nhất. Rồi bây giờ gét gô thooiii 😄 ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#"},{"categories":["projects"],"content":"IntroStock Analysis hay còn gọi là Market Analysis đề cập đến phương pháp mà nhà đầu tư hoặc nhà giao dịch sử dụng để đánh giá và điều tra một công cụ giao dịch cụ thể, lĩnh vực đầu tư hoặc toàn bộ thị trường chứng khoán. Không những thế, nó liên quan đến việc nghiên cứu dữ liệu thị trường trong quá khứ và hiện tại và tạo ra một phương pháp để chọn cổ phiếu phù hợp để giao dịch. Các nhà đầu tư sẽ đưa ra quyết định mua hoặc bán dựa trên thông tin phân tích chứng khoán. Trong project này ta sẽ phân tích, trực quan hóa bộ dữ liệu giả định được cung cấp bởi khách hàng để đánh giá thị trường chứng khoán trong khoảng thời gian 1 tháng của 30 công ty thuộc VN30 Dưới đây là tóm tắt sơ lược từng bước để xử lý và phân tích: EDA (Exploratory Data Analysis) Data Preprocessing PCA (Principle Component Analysis) K-Means Clustering Data Analysis References (chi tiết trong notebook ở github) Raw Data Source: df_merged.pkl Raw data là dữ liệu bảng giá cổ phiếu của 30 công ty thuộc VN30 Index + 1 trường giá phái sinh trong 1 tháng ","date":"31 Aug 2023","objectID":"/stock_analysis/:1:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#intro"},{"categories":["projects"],"content":"Exploratory Data AnalysisĐây là bước đầu tiên, chúng ta sẽ cùng nhau tìm hiểu sơ lược raw data cũng như tìm hiểu cái nhìn tổng quát về dữ liệu ta sắp phải phân tích để từ đó có cách tiền xử lý phù hợp. Làm gì thì làm cứ phải import packages để đọc data cái đã ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#exploratory-data-analysis"},{"categories":["projects"],"content":"Data AcquistionTa sẽ import một số packages quen thuộc để đọc file df_merged.pkl import pickle import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd.read_pickle('https://github.com/PhongHuynh0394/My-respository/blob/main/df_merged.pkl?raw=true') # Check the data type type(data) # --\u003e list Data nhận được từ pickle file là một list, bây giờ ta sẽ tìm kiếm cái nhìn tổng quan về dữ liệu này ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-acquistion"},{"categories":["projects"],"content":"A Brief View Dữ liệu lưu ở pickle là một list chứa 23 dataframe (df) Mỗi df có index theo datetime (nghĩa là đây là loại dữ liệu thuộc timeseries) Các columns lần lượt là từng mã cổ phiếu, chứa khối lượng/ giá của các lệnh mua/bán sát với lệnh khớp I và khối lượng của các lệnh mua/bán sát với giá khớp lệnh II print('So luong df:', len(data)) # --\u003e So luong df: 23 Raw data là giá lệnh mua/bán I II và khối lượng giao dịch của cổ phiếu 30 công ty VN30raw data \" Raw data là giá lệnh mua/bán I II và khối lượng giao dịch của cổ phiếu 30 công ty VN30 Thời gian thu thập được cập nhật với chu kì là 10 giây bắt đầu từ ngày 20 tháng 3 đến ngày 19 tháng 4, từ 2 giờ 15 đến 7 giờ 30 mỗi ngày. Nhưng có một số ngày bị miss trong bộ dữ liệu này (Chi tiết hơn trong notebook ở source code) Cùng xem qua về số lượng observations của mỗi bảng Tổng cộng ta có 181 fields và mỗi bảng khoảng 1345 observations (tổng cộng 30538 quan sát). Cũng khá nhiều phải không nào. Ta sẽ cùng tiền xử lý chúng nào ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#a-brief-view"},{"categories":["projects"],"content":"Data PreprocessingSau khi đã biết khái quát raw data, ta sẽ cần phải tiền xử lý những dữ liệu thô này trước khi có thể áp dụng các mô hình máy học hoặc giảm chiều dữ liệu ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-preprocessing"},{"categories":["projects"],"content":"Data CleaningHãy sử dụng method describe() của pandas để có cái nhìn sơ bộ nhất về df của chúng ta data[0].describe() Đầu tiên, ta sẽ drop duplicate và định dạng lại index thời gian market = pd.DataFrame(columns=data[0].columns.to_list()) #create empty df # Data cleaning for _, df in enumerate(data): df.drop_duplicates() cols = df.columns.to_list() #convert/ replace 0 for col in cols: df[col] = pd.to_numeric(df[col], errors='coerce') # #missing handling df.fillna(0, inplace=True) market = pd.concat([market,df]).copy() #concat all clean df into market #datetime format market.reset_index(inplace=True) market = market.rename(columns={'index': 'datetime'}) market['datetime'] = market['datetime'].dt.strftime('%Y-%m-%d%H:%M:%S') market['datetime'] = pd.to_datetime(market['datetime']) market = market.sort_values(\"datetime\", ascending=True) market.set_index('datetime', inplace=True) Kế tiếp hãy xử lý missing value bằng phương pháp nội suy (interpolation) với method padding, và sau đó sẽ dùng backfill Đây là phương pháp ước tính giá trị của các điểm dữ liệu chưa biết trong phạm vi của một tập hợp rời rạc chứa một số điểm dữ liệu đã biết. Nghe có vẻ lằng nhằng, đơn giản là thế này: .interpolate(method=‘pad’): fill null values bằng giá trị liền kề nó lần lượt từ trên xuống (nó giống như ffill()) .fillna(method=‘backfill’): Đây là phương pháp ngược lại bên trên, fill null bằng giá trị liền kề từ dưới lên Note Có rất nhiều phương pháp nội suy như linear (default) hay polynomial,… Nhưng OG chọn padding và backfill vì 2 phương pháp này có thể giữ cho data missing ở giá trị sát nhất với giá trị thực gần nhất và giúp cho kết quả sau khi fill sát với thực tế nhất. Ngoài ra 2 phương pháp này có thể fill được vị trí đầu và cuối cùng một cách hiệu quả. def handle_null(X: pd.DataFrame) -\u003e pd.DataFrame: ''' handle missing value ''' for col in X.columns.to_list(): X[col].interpolate(method='pad', inplace=True) X[col].fillna(method='backfill', inplace=True) return X ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-cleaning"},{"categories":["projects"],"content":"Data transformingOG nhận thấy rằng với các trường data hiện tại chưa thực sự giúp ích quá nhiều trong việc phân tích sau này (giá mua/bán và số lượng mua/bán + giá phái sinh (label) ) Do đó OG cần một dataframe mới với các trường mới có nhiều giá trị phân tích hơn: gttb_ (Giá trị trung bình): là column mới được tính trên bình quân giá cả mua vào, bán ra của từng cổ phiếu được giao dịch THÀNH CÔNG trên thị trường. total_ban \u0026 total_mua (Tổng bán/mua khối lượng 1): là column mới để tính tổng giá bán khối lượng 1 cũng như mua khối lượng 1 của từng cố phiếu được giao dịch trên thị trường. Gia_KL: sao chép giá khối lượng của từng mã cổ phiếu từ bộ dữ liệu ban đầu. (label) Cài đặt lại index thời gian: group by các time-series theo phút. def transform_raw(market: pd.DataFrame) -\u003e pd.DataFrame: # split stock name name = [col.split('_1')[-1] for col in market.columns.to_list() if 'mua_gia_1' in col] new_df = pd.DataFrame() for i in name: # calculate gttb (mean) new_df[f'gttb_{i}'] = ((market[f'mua_gia_1{i}'] * market[f'mua_kl_1{i}'] + market[f'ban_gia_1{i}'] * market[f'ban_kl_1{i}']) /(market[f'mua_kl_1{i}'] + market[f'ban_kl_1{i}'])).copy() # get ban_kl and mua_kl new_df[f'total_ban_{i}'] = market[f'ban_kl_1{i}'].copy() new_df[f'total_mua_{i}'] = market[f'mua_kl_1{i}'].copy() # get Gia KL new_df['Gia KL'] = market['Gia KL'].copy() new_df.set_index(market.index, inplace=True) gttb = [col for col in new_df.columns.to_list() if 'gttb' in col] + ['Gia KL'] mua_ban = [col for col in new_df.columns.to_list() if col not in gttb] # Group by minute result = new_df[gttb].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute]).mean() result = pd.concat([result,new_df[mua_ban].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute ]).sum()],axis=1) #Set index in minute index = pd.to_datetime([f\"{d}{h}:{m}:00\" for (d, h, m) in result.index]) result.index = index #handle missing value result = handle_null(result) return result Rồi giờ transform rồi kiểm tra lại số lượng quan sát ở bảng mới thôi # Check the length of new data len(new_market) # --\u003e 5154 Với kết quả mới, chỉ còn lại 5154 quan sát mà thôi, khi rút lại một số lượng quan sát lớn như vậy, ta sẽ phải chấp nhận rủi ro mất đi nhiều thông tin về dữ liệu mà cụ thể là dữ liệu theo giây (cứ 10 giây cập nhật). Nhưng đổi lại, data sẽ cô động hơn và bớt nhiễu vì với sự biến đổi của thị trường trong cả 1 tháng, sự thay đổi của các trường trong mỗi 10 giây là quá nhỏ và không đáng kể. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-transforming"},{"categories":["projects"],"content":"Data ScalingSau khi có bộ dataframe mới tốt hơn và sạch sẽ, bước kế tiếp sẽ là scale lại dữ liệu về một chuẩn để tăng hiệu quả của các thuật toán học máy Có một số phương pháp scale data như: Standardization, Normalization,… Ở project này, OG sẽ dùng phương pháp Normalization để scale data. Phương pháp chuẩn hóa này đưa tỷ lệ dữ liệu từ phạm vi ban đầu về chuẩn phạm vi từ 0 đến 1, giá trị được normalize theo công thức sau: $$ x' = \\frac{x - min}{max - min} $$ Với $x$ là giá trị cần được chuẩn hóa, $max$ và $min$ là lần lượt là giá trị lớn nhất và nhỏ nhất trong tất cả các observations của feature trong tập dữ liệu. Ta sẽ dùng MinMaxScaler của scikit-learn trong tác vụ này. from sklearn.preprocessing import MinMaxScaler # Normalization data using libraries min_max = MinMaxScaler() X = new_market.values X_std = min_max.fit_transform(X) print('Data after scaling: ') X_std # array([[9.10048201e-01, 9.43990665e-01, 9.59215952e-01, ..., # 1.31664615e-02, 1.45711006e-02, 2.90267046e-03], # [9.14492108e-01, 9.61493582e-01, 9.57989455e-01, ..., # 1.42007963e-02, 1.10109072e-04, 3.64335188e-03], # [9.12286536e-01, 9.57992999e-01, 9.56950233e-01, ..., # 3.58702686e-03, 1.43141794e-03, 4.40405173e-04], # ..., # [9.23665190e-01, 9.04317386e-01, 8.96622210e-01, ..., # 1.22024151e-01, 3.04635100e-03, 2.10293470e-02], # [9.24218272e-01, 8.89565349e-01, 8.97159958e-01, ..., # 1.05691866e-02, 1.13779375e-03, 2.88265204e-02], # [9.28532923e-01, 9.04317386e-01, 8.98196897e-01, ..., # 8.84087818e-03, 3.67030241e-04, 7.79383700e-03]] Như vậy là đã chuẩn bị hoàn tất cho bước tiếp theo rồi. Chúng ta sẽ bước vào thuật toán chính đầu tiên trong project này. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-scaling"},{"categories":["projects"],"content":"Principle Component Analysis (PCA)Chúng ta đã đi qua việc tiền xử lý dài ngoằn từ cleaning, transforming đến scaling. Vậy câu hỏi là: dữ liệu đã sẵn sàng để áp dụng cho các mô hình máy học hay chưa ? Câu trả lời cho trường hợp này là: Chưa. Tại sao vậy ? Bởi vì tập dữ liệu của chúng ta có quá nhiều features Feature của tập data là gì ? Dành cho bạn chưa biết, feature của tập data còn được gọi là các trường (hay field) của tập data đó. Đó là các cột, mỗi cột là một “tính chất” khác nhau của đối tượng aka quan sát (observation) thường là các hàng. Hiện tại có thể thấy cleaning data của chúng ta có 91 features: gttb_(cổ phiếu): 30 cột giá trị trung bình giao dịch của 30 cổ phiếu trong 1 phút total_ban_(cổ phiếu): 30 cột tổng khối lượng bán của 30 cổ phiếu trong 1 phút total_mua_(cổ phiếu): 30 cột tổng khối lượng mua của 30 cổ phiếu trong 1 phút Gia_KL: 1 cột giá phái sinh VN30 Index (label) Với số lượng feature lớn như vậy, sẽ vô cùng kém hiệu quả nếu ngay lập tức sử dụng train cho các mô hình machine learning. Giải pháp ở đây chính là ta sẽ giảm chiều dữ liệu xuống mức vừa đạt hiệu năng tốt khi training mà cũng không làm mất quá nhiều thông tin của dữ liệu. Vâng đúng vậy, phương pháp OG muốn giới thiệu chính là PCA hay còn được biết với tên việt hóa là Phân tích thành phần chính. Mục tiêu của phương pháp này là đưa bộ dữ liệu ban đầu sang hệ tọa độ mới dựa trên các thành phần chính. Dữ liệu ở hệ tọa độ mới có ít chiều hơn nhưng vẫn giữ được nhiều nhất thông tin có thể, từ đó giúp tăng tốc độ tính toán và giảm độ phức tạp mô hình hơn rất nhiều. Nói tóm tắt cho dễ hiểu Cơ bản là phương pháp này đưa bộ data của ta vào một “thế giới song song” có số chiều mới ít hơn (chiều aka features). Bạn có thể hiểu như là nhìn dữ liệu của mình ở một góc khác vậy. Ở phần này chúng ta sẽ sử dụng phương pháp này thông qua sự phân rã của ma trận hiệp phương sai (Eigen decomposition of covariance matrix) ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#principle-component-analysis-pca"},{"categories":["projects"],"content":"EigenVector và EigenValueMa trận hiệp phương sai được định nghĩa là: $$ S = \\frac{1}{N}\\hat{X}^T\\hat{X} $$ Với $\\hat{X} = X - \\hat{x}1^T$ là zero-corrected data hay dữ liệu đã được chuẩn hoá. Ta sẽ viết hàm get_eigenpairs() để tìm các vector riêng và giá trị riêng của ma trận hiệp phương sai: $$ Su_i = \\lambda_iu_i $$ Trong đó: các $(\\lambda_i,u_i)$ là các cặp trị riêng (không âm) và vector riêng của ma trận hiệp phương sai $S$ Tại sao lại cần tìm các vector riêng và giá trị riêng của ma trận hiệp phương sai ? Việc sử dụng các giá trị riêng để đánh giá sự quan trọng của mỗi thành phần chính được tạo ra từ việc giảm chiều dữ liệu. Các giá trị riêng càng lớn thì thành phần chính tương ứng càng quan trọng. Các vector riêng tương ứng với các giá trị riêng này được sử dụng để xác định hướng của các thành phần chính. Giá trị riêng (Eigenvalues $\\lambda_i$): Các hệ số được gắn với các vector riêng, cung cấp cho độ lớn của trục. Trong trường hợp này, chúng là thước đo hiệp phương sai của dữ liệu. Vector riêng (EigenVector $u_i$):Các vector (khác 0) không thay đổi hướng khi áp dụng bất kỳ phép biến đổi tuyến tính (linear transformation) nào, nó chỉ thay đổi theo hệ số vô hướng. Hàm sắp xếp các vector riêng (Sort eigenvalues): Bằng cách sắp xếp các vector riêng theo thứ tự của giá trị riêng, ta có thể chọn ra các vector riêng có giá trị riêng lớn nhất để xây dựng các thành phần chính của dữ liệu (đóng góp nhiều nhất vào việc giải thích sự biến thiên của dữ liệu). Các thành phần chính này có thể được sử dụng để tái cấu trúc dữ liệu ban đầu mà vẫn giữ được độ giống nhau của các điểm dữ liệu ban đầu. def get_eigenpairs(X: np.array) -\u003e list: ''' Input: X: np.array (init matrix) return eigenpairs containing eigenvalues and eigenvectors of covariance matrix ''' # Covariance matrix cov_mat = np.cov(X.T) # Eigenvalues and Eigenvectors evals, evecs = np.linalg.eigh(cov_mat) # Sort eigenvalues epairs = [(abs(eval), evec) for (eval, evec) in zip(evals, evecs.T)] epairs = sorted(epairs, key = lambda pair: pair[0], reverse = True) return epairs ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#eigenvector-và-eigenvalue"},{"categories":["projects"],"content":"Cumulative Sum of ComponentsTính tổng tích lũy của các thành phần trong PCA (Cumulative Sum of Explained Variance) để xác định tổng phần trăm phương sai được giải thích bởi các thành phần được giữ lại trong mô hình PCA. $$ r_K = \\frac{\\sum^K_{i=1}\\lambda_i}{\\sum^D_{j=1}\\lambda_j} $$ là lượng thông tin được giữ lại khi số chiều dữ liệu mới sau PCA là K. Hàm findNumVec() thực hiện việc lấy các giá trị riêng từ danh sách các eigenpairs và chuyển đổi chúng thành một mảng numpy. Sau đó, nó tính tổng tích lũy của các giá trị riêng, sử dụng hàm np.cumsum () chuẩn hóa tổng của chúng =\u003e cho ra một danh sách các giá trị (trong khoảng từ 0 đến 1) đại diện cho tỷ lệ phần trăm phương sai được giải thích bởi mỗi thành phần chính. Sau đó, hàm lặp qua danh sách tổng tích lũy và tìm chỉ mục của giá trị đầu tiên lớn hơn hoặc bằng tỷ lệ phần trăm phương sai mong muốn được giải thích. Chỉ số này đại diện cho số lượng thành phần chính cần thiết để giải thích tỷ lệ phần trăm phương sai đó, vì vậy hàm trả về giá trị này cộng với 1 (vì lập chỉ mục Python bắt đầu từ 0). def findNumVec(eigenpairs: list, percent = 0.9): ''' Find number of principal components (eigenvectors) -\u003e return the number of principal components when total accumulate \u003e= percent ''' # Get eigenvalues eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) # Cumulative sum and calculate percent cumsum = np.cumsum(eigenvals) cumsum /= cumsum[-1] # Find number of principal components that accumulate \u003e= percent for i, val in enumerate(cumsum): if val \u003e= percent: return i + 1 Ta sẽ thử tìm xem số thành phần chính cần để giữ được 80% dữ liệu: print(findNumVec(epairs, 0.8)) # --\u003e 28 ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#cumulative-sum-of-components"},{"categories":["projects"],"content":"Scree ChartTa sẽ vẽ một biểu đồ thể hiện quan hệ của số lượng thành phần chính và phần trăm phương sai giải thích tích lũy def screeplot(eigenpairs): ''' Scree plot ''' fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True) eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) cumsum = np.cumsum(eigenvals) # extracts the eigenvalues from the eigenpairs and calculates their cumulative sum cumsum /= cumsum[-1] name = [f'PCA {i}' for i in range(len(cumsum))] # line plot # the eigenvalues are plotted against the number of principal components axes[0].plot(range(len(eigenvals)), eigenvals, marker = '.', color = 'b', label = 'Eigenvalue') # the cumulative proportion of the variance explained by each component is plotted against the number of principal components axes[1].plot(range(len(cumsum)), cumsum, marker = '.', color = 'green', label = 'Cumulative propotion') # y axis label axes[0].set_ylabel('Eigen values') axes[1].set_ylabel('Cumulative explained variance') # item legend axes[0].legend() axes[1].legend() # grid axes[0].grid() axes[1].grid() # title fig.supxlabel('Number of components') plt.tight_layout() plt.show() #print the cumsum of eigenvalues print(pd.DataFrame(cumsum, columns = ['Cumulative total'], index = name)) result = { str(i): f\"PC {i+1}({var:.1f}%)\" for i, var in enumerate(cumsum*100) } return result pca_scree = screeplot(epairs) Scree plotScree plot \" Scree plot Giải thích Đường của giá trị riêng màu xanh nước biển trên biểu đồ cho ta biết độ lớn của mỗi thành phần chính và tầm quan trọng của chúng trong giải thích sự biến thiên của dữ liệu. Nếu giá trị riêng của một thành phần chính là lớn, thì thành phần đó có tầm quan trọng cao trong việc giải thích sự biến thiên của dữ liệu. Đường màu xanh lá thể hiện tổng tích lũy cho ta biết tổng phần trăm độ lớn của sự biến thiên của dữ liệu mà các thành phần chính có thể giải thích. Dựa vào biểu đồ trên có thể nhận thấy nếu chỉ có 2 chiều, ta chỉ giữ được khoảng 37% dữ liệu ban đầu. ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#scree-chart"},{"categories":["projects"],"content":"Visualize PCABây giờ, ta sẽ thực hiện chiếu dữ liệu ban đầu đã chuẩn hóa $\\hat{X}$ xuống không gian con tìm được và lấy ra ma trận các thành phần chính để tiếp tục công việc phân tích và xây dựng mô hình. Hàm getPC() trả về một ma trận các thành phần chính từ ma trận ban đầu, dựa trên số lượng thành phần đã cho hoặc số lượng thành phần giữ được 80% dữ liệu (nếu num_components không được đưa ra). Ma trận trọng số $W$ là ma trận chuyển đổi tuyến tính được sử dụng để chuyển đổi dữ liệu gốc vào không gian mới, trong đó mỗi thành phần chính được sắp xếp theo độ quan trọng giảm dần. Cụ thể, mỗi cột của ma trận $W$ là một vector riêng chuẩn hóa tương ứng với các giá trị riêng của ma trận hiệp phương sai $s$. Thực hiện việc nhân ma trận $W$ với hoán vị của ma trận đã chuẩn hóa $\\hat{X}$ (init_matrix). Ma trận kết quả sau đó tiếp tục được hoán vị để phù hợp với hình dạng ban đầu của init_matrix và trả về kết quả. def getPC(eigenpairs, init_matrix, num_components = None): ''' Return matrix of principal components from init_matrix ''' # default num_components = number which to keep 80% data if num_components is None: num_components = findNumVec(eigenpairs, 0.8) # extracts the eigen vectors corresponding to the top num_components eigenvalues from the eigenpairs list eigenvecs = [eigenvec for (_, eigenvec) in eigenpairs[:num_components]] W = np.array([e.T for e in eigenvecs]) # stacks the eigen vectors into a weight matrix W return (W @ init_matrix.T).T X_pca = getPC(epairs, X_std) Vậy là ta đã giảm được độ phức tạp cho bộ dữ liệu khá “nhọc nhằn” này. Hãy trực quan hóa lên biểu đồ để có một góc nhìn cụ thể và rõ ràng hơn. Biểu đồ scatter plot sau khi PCA có thể giúp cho chúng ta nhìn thấy cách dữ liệu được phân bố trên các thành phần chính (principal components) và kiểm tra xem liệu chúng ta có thể tìm thấy các cluster hoặc pattern nào trong dữ liệu. plt.scatter(X_pca[:,0], X_pca[:,1]) plt.xlabel('PC1') plt.ylabel('PC2') plt.title('Visualizing data through PCA', fontsize=18) plt.gca().set_aspect('equal', 'datalim') plt.grid() plt.show() Visualizing data via PCAVisualizing data via PCA \" Visualizing data via PCA Okayy dựa vào biểu đồ trên, cũng có thể thấy là dữ liệu ở không gian mới đã phân tách khá rõ ràng rồi. Điều này nghĩa là phương pháp PCA đã giảm số chiều của dữ liệu một cách hiệu quả. Bước tiếp theo chính là áp vào mô hình K-Means để phân cụm và tìm pattern. Chúng ta sẽ cùng chiến tiếp ở phần 2 nhé ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:4","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#visualize-pca"},{"categories":["projects"],"content":"To be ContinueChúng ta đã thực hiện các bước tiền xử lý dữ liệu và sau đó là thực hiện PCA để giảm chiều dữ liệu một cách hiệu quả. Bài sau phần 2, OG sẽ thực hiện training mô hình K-means clustering và cuối cùng là phân tích dữ liệu chứng khoáng. Đây là kiến thức tích góp từ nhiều nguồn và nghiên cứu của nhóm OG, tất nhiên không thể tránh khỏi sai sót. Hy vọng bài viết lần này thú vị và giúp bạn đọc thư giãn, tham khảo. -Mew- ","date":"31 Aug 2023","objectID":"/stock_analysis/:5:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#to-be-continue"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis P2 Stock Analysis using PCA and K-means Read more... ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#related"},{"categories":[],"content":"Ngành Data có gì hot mà mình lại dính","date":"30 Aug 2023","objectID":"/start_journey/","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/"},{"categories":[],"content":"Quảng Cáo Chào mừng đến với “Data lú” Giới thiệu với mọi người đây là series đầu tiên của kênh này kể mấy câu chuyện kì thú ảo ma canada của OG trong thế giới data rộng lớn 😂 Đùa chút thôi, đây sẽ là series vui vẻ về câu chuyện Data mà OG trải nghiệm, góp nhặt được. Hy vọng bạn sẽ thích nó hihi 😍 Gòi dzo Hellooo OG đâyy ! Chào mừng bạn đến với số đầu tiên, lần đầu còn bỡ ngỡ, nên mình sẽ kể cơ duyên đưa mình đến với ngành Data và quyết định dấn thân vào con đường trở thành một Data Engineer 😗 Gét Goo! ","date":"30 Aug 2023","objectID":"/start_journey/:0:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#"},{"categories":[],"content":"Ủa ngành Data Science ?Khoan Khoan … Bên trên là Engineer, qua đây là Science là sao OG ? Từ từ nào 😄 Mọi chuyện bắt đầu khi mình đậu vào một ngành được ca ngợi là ngành “quyến rũ” nhất thế kỷ 21 theo Harvard Business Review , đó là Data Science. Khúc này mình nghe cũng oách oách, nhưng chính xác Data Science là gì ? Và các nhà khoa học dữ liệu (data scientist) làm gì ? ","date":"30 Aug 2023","objectID":"/start_journey/:1:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#ủa-ngành-data-science-"},{"categories":[],"content":"Data Science là gì nhỉ?Ngành Khoa học dữ liệu hay Data Science là một lĩnh vực liên ngành ứng dụng các phương pháp khoa học, thuật toán và các phân tích thống kê để tìm kiếm ý nghĩa từ dữ liệu. Hay nói bằng cách dễ hiểu, Data Science là ngành tìm kiếm, phân tích dữ liệu để khai thác tất cả những giá trị mà dữ liệu mang lại để phục vụ nhiều mục đích khác nhau. Data Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán tương laiData Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán \" Data Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán tương lai Một nhà khoa học dữ liệu (Data Scientist) là người chịu trách nhiệm đưa ra các dẫn chứng từ dữ liệu, để từ đó đề xuất các giải pháp, kế hoạch hay định hướng từ ý nghĩa tìm được từ dữ liệu để giải quyết các bài toán kinh doanh khác nhau. Một data scientist cần phải biết kỹ năng gì? Lập trình: Python và R là 2 ngôn ngữ chính được sử dụng đối với ngành này. Python là một ngôn ngữ lập trình linh hoạt phổ biến với rất nhiều thư viện để xử lý dữ liệu như numpy, pandas, matplotlib,… Trong khi đó R tỏ là là một ngôn ngữ mạnh mẽ về phân tích và thống kê, ngoài ra R cũng thường được dùng trong nghiên cứu và học thuật. Thống kê và ứng dụng toán học: Nếu bạn không yêu thích toán học, chắc hẳn bạn cũng sẽ không thể làm điều đó với data science. Hẳn vậy, là một nhà khoa học dữ liệu, bạn cần có một nền tảng kiến thức toán học vững, đặc biệt là về xác suất thống kê và đại số tuyến tính,… SQL và DBMS: Ta phải tiếp xúc rất nhiều với hệ quản trị cơ sở dữ liệu (Database Management System hay DBMS), đó có thể là hệ quản trị cơ sở dữ liệu Quan Hệ (Relational Database Management System) như MySQL, Postgres, SQL server… hay NoSQL database như MongoDB, Cassandra,… Và để tương tác với database (RDBMS), điều không thể thiếu chính là SQL (Structured query language aka si cồ hay ét qui eo 😂 ). Cơ bản thì đây là ngôn ngữ dùng để truy suất dữ liệu, giao tiếp với database, đặc biệt là các RDBMS. AI, Machine learning: Khi có một lượng dữ liệu khổng lồ, một data scientist có thể sẽ dùng chúng để huấn luyện mô hình học máy hoặc mạng để giải các bài toán hồi quy và đưa ra được các dự đoán về xu hướng data hay giải quyết các bài toán phân loại. Có hiểu biết về các thuật toán máy học và kiến trúc mạng noron cũng là một điều cần có ở nhà khoa học dữ liệu. Đọc đến đây, có thể bạn sẽ có cảm giác “Dèjà vu” nhẹ … Sao nhiều chổ giống Data Analyst thế nhỉ ? Mà thiệt ra là không giống đâu nhé, hai ngành này chỉ là anh em xã hội với nhau mà thôi 😂 ","date":"30 Aug 2023","objectID":"/start_journey/:1:1","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-science-là-gì-nhỉ"},{"categories":[],"content":"Data Scientist vs Data AnalystSẵn tiện kể một chút về vai trò của một người Data Analyst. Về cơ bản, vai trò của họ cũng giống với các data scientist, họ cũng phân tích dữ liệu, cố gắng tìm kiếm và rút ra giá trị từ chúng. Nhưng sẽ có một số điểm khác biệt: Data Analyst Data Science Chuyên viên phân tích dữ liệu Nhà khoa học dữ liệu Vẫn làm công việc của DS nhưng với quy mô nhỏ Tỏa sáng với lượng data khổng lồ (BigData) Không cần nhiều kiến thức lập trình Cần kiến thức lập trình Cần có kiến thức về hoạt động kinh doanh nhiều hơn và vững về kiến thức thống kê Cần có kiến thức không chỉ toán thống kê, ứng dụng mà còn phải có kiến thức về computer science, AI/ML,… Dựa vào dữ liệu đưa ra các giá trị có ích và cái nhìn trực quan về dữ liệu Được yêu cầu phát triển “data product” để đưa ra quyết định có ích từ tập dữ liệu lớn Data Science and Data AnalyticSource: https://www.datascience-pm.com/wp-content/uploads/2021/05/data-scientist-vs-analyst-venn-diagram.png \" Data Science and Data Analytic Rồi okay nãy giờ là cả data science (DS) và data analytic (DA) rồi. Giờ là mới đến data engineer của tui nè hihi 😄 ","date":"30 Aug 2023","objectID":"/start_journey/:1:2","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-scientist-vs-data-analyst"},{"categories":[],"content":"Data Engineer là gì ?Tuy học Data Science, nhưng thực ra ngay từ những lúc còn mơn mởn cấp 3, OG đã từng có ước muốn trở thành một lập trình viên một tay cafe một tay chém code bình loạn thiên hạ 😂 Và thế là tìm được một ngành thích hợp được coi là “Software engineer cho data”, ngành này là một trong các ngành có xu hướng phát triển nhanh nhất trong nhóm ngành công nghệ. Vâng đó chính là Data Engineer Data EngineerData Engineer được coi là Software Engineer ở Data field \" Data Engineer Đầu tiên, Data Engineer hay DE được gọi là kỹ sư dữ liệu. Đây là vai trò đảm nhiệm việc phân tích nguồn dữ liệu, xây dựng và duy trì hệ thống cơ sở dữ liệu hiệu quả. Ngoài ra cũng là người đảm bảo chất lượng dữ liệu cho các phòng ban khác sử dụng. Cơ bản để là để cho DS và DA làm việc một cách hiệu quả nhất, họ cần có một nguồn data ổn định và sạch sẽ. Và người đảm nhiệm việc luân chuyển data đó tới cho họ chính là Data Engineer. Không chỉ có DS và DA mà data engineer phục vụ cho tất cả các phòng ban khác Data Engineer Nói tóm lại, Data Engineer là người xây dựng các đường ống dữ liệu (data pipeline) để truyền dữ liệu từ nơi này sang nơi khác một cách chất lượng nhất :)) Khái niệm cơ bản là thế thôi, nghe có vẻ đơn giản phải không. Hãy tiếp tục với mục tiếp theo để xem liệu ta cần gì để trở thành data engineer ","date":"30 Aug 2023","objectID":"/start_journey/:2:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-engineer-là-gì-"},{"categories":[],"content":"Data Engineer thì cần biết gì ?Một data Engineer về bản chất là xây dựng các data pipeline để luân chuyển dữ liệu. Để làm tốt việc đó, kỹ sư dữ liệu phải biết: Kỹ năng lập trình: Tất nhiên rồi, bạn là một nhân viên IT thì điều này là phải có. Các ngôn ngữ mà DE thường dùng là SQL, Python và R. Hệ cơ sở dữ liệu quan hệ và phi quan hệ: Dữ liệu có rất nhiều dạng: Structure/Semi/Unstructure data, do đó cũng cần có nhiều loại database quản lý chúng. Và DE làm việc rất nhiều với database. Họ sẽ là người trực tiếp tương tác kể cả với SQL và NoSQL database. ETL/ELT: ETL aka Extract Transform Load hay ELT aka Extract Load Transform là quy trình xử lý và luân chuyển dữ liệu từ nguồn đến đích. Một DE phải nắm được để thiết kế data pipeline một cách hiệu quả nhất Data Warehouse: hay được biết đến là kho chứa dữ liệu. Bạn có thể sẽ phải xây dựng, thiết kế cấu trúc data warehouse trên cloud platform và xây dựng các kết nối dữ liệu để tối ưu hóa tốc độ truy xuất và đảm bảo việc phân tích dữ liệu. Big Data: Bạn cũng cần phải biết các kiến trúc lưu trữ và xử lý tập dữ liệu lớn như Hadoop, Spark,… Cloud: Tất nhiên là phải có rồi, các cloud platform như Google Cloud Platform, AWS, Azure,… đã rất nổi tiếng trong việc hỗ trợ xây dựng và thiết kế hệ thống pipeline cũng như hỗ trợ tối đa việc xử lý bigdata cũng như deploy hệ thống hạ tầng một cách nhanh chóng. Bạn có thể sẽ phải làm việc với lượng dữ liệu khổng lồ và tập dữ liệu lớn. Và để xây dựng hệ thống xử lý được lượng dữ liệu đó, chắc chắn phải có sự góp mặt của các nền tảng đám mây. Well… Nhìn chung cũng nhiều thứ cần phải biết đấy nhỉ, tất nhiên đó chỉ là một số điều quan trọng nhất. Ngoài ra bạn cũng cần phải biết một số kiến thức khác về Unix và Linux, Docker, Git, Batch/Stream Processing,… Và còn ti tỉ thứ khác mà OG có kể đến răng long đầu bạc có lẽ cũng chưa hết 😂 ","date":"30 Aug 2023","objectID":"/start_journey/:2:1","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-engineer-thì-cần-biết-gì-"},{"categories":[],"content":"Tạm kếtHành trình nào khi bắt đầu cũng gian nan, cả bản thân OG khi bắt đầu cũng không biết gì cả. Nhưng khi nhấc ngón chân lên và đi thì mới cảm nhận được thế giới chứ 😄 Hy vọng bài viết này giúp bạn thư giãn và có một cái nhìn chung về ngành data nhé. Hẹn gặp lại trong bài tiếp theo hehe -Meww ","date":"30 Aug 2023","objectID":"/start_journey/:3:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#tạm-kết"},{"categories":[],"content":"Giới thiệu về trang web đáng yêu này","date":"30 Aug 2023","objectID":"/intro_blog/","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/"},{"categories":[],"content":"Hello! Hello! mình là OG đây. Đây là bài blog đầu tiên của mình ở đây. Có thể bạn đang tự hỏi rằng mình là ai và đây là nơi nào đúng không, vậy hãy cùng mình tìm hiểu thử nhé ","date":"30 Aug 2023","objectID":"/intro_blog/:0:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#"},{"categories":[],"content":"OG là ai ?Một lần nữa giới thiệu mình tên là Vĩnh Phong - một anh chàng thích tìm tòi điều mới… hmmm thế thôi :))) Để hiểu thêm về mình: About Còn OG (nickname) là biệt danh mình lấy cảm hứng từ một nhât vật hoạt hình rất hóm hỉnh đấy nhé hehe. Đó là tên một chú mèo xanh dương hay bị mấy con gián quậy :)) bạn thử đoán xem Đáp án Ủa lộn này là tui:))Tui \" Ủa lộn này là tui:)) Đây mới là đáp án Nếu bạn hong biết, thì con mèo xanh lè này tên là Oggy và nickname mình cũng vậy :)) Biết mình là ai rồi, thế thì… ","date":"30 Aug 2023","objectID":"/intro_blog/:1:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#OG-la-ai"},{"categories":[],"content":"Đây là nơi nào đây ?Đây trang mà mình đăng lên các bài Blogs về công nghệ, về ngành Data nói chung và về hành trình học tập của OG để trở thành một Data Engineer trong tương lai. Tất nhiên không chỉ như vậy Mình cũng viết blogs về đời sống, về những điều thú vị của cuộc sống xung quanh Và mình hy vọng trang cũng này sẽ là nơi mang lại sự thoải mái và thư giản cho mọi người ","date":"30 Aug 2023","objectID":"/intro_blog/:2:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#day-la-dau"},{"categories":[],"content":"Thế ở đây có gì hay?Tóm tắt các trang: Blogs: bạn có thể tìm thấy các bài blogs mình viết ở đây Projects: Những dự án mình đã làm About: Nếu bạn muốn hiểu thêm về mình Đó là tổng quan “các thứ có thể sẽ xuất hiện” ở trang web này. ","date":"30 Aug 2023","objectID":"/intro_blog/:2:1","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#thế-ở-đây-có-gì-hay"},{"categories":[],"content":"Cuối cùngOG hy vọng đây sẽ là nơi giúp bạn thư giãn hay học hỏi được nhiều điều thú vị nhé 😄 -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huỳnh Lưu Vĩnh Phong Facebook: Phong Huynh Instagram: phong_huynh Hoặc bạn cũng có thể ghé thăm Github của mình: PhongHuynh0394 Huỳnh Lưu Vĩnh Phong ","date":"30 Aug 2023","objectID":"/intro_blog/:3:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#cuoi-cung"},{"categories":null,"content":"Continuous of Football ETL series","date":"01 Aug 2023","objectID":"/football_etl_2/","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/"},{"categories":null,"content":"Source PhongHuynh0394 Football_ETL_Analysis Hello! Hello! OG đây, sau phần 1 chúng ta đã setup các kiểu và đảm bảo mọi thứ trơn tru rồi, ở phần này chúng ta sẽ chuẩn bị Data Source, và khởi chạy pipeline ở Implement sau đó sẽ Visualize cleaned data có được từ data warehouse thành Dashboard. Bắt đầu thôi nào ! ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#"},{"categories":null,"content":"Data Source","date":"01 Aug 2023","objectID":"/football_etl_2/:1:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#data-source"},{"categories":null,"content":"Chuẩn bị file làm raw dataCác file csv sử dụng làm dữ liệu được tải từ Football Database - Kaggle. Đây là dữ liệu thống kê của cầu thủ, đội bóng đến từ 5 giải bóng hàng đầu Châu Âu (Premier League, Laliga, Seria A, Budesliga, League 1) Ta sẽ có schema như sau: SchemaSchema \" Schema trong đó: games: bảng chứa thông tin thống kê của từng trận đấu (gameID) teams: Bảng chứa tên các đội bóng (teamID) players: Bảng chứa tên các cầu thủ (playerID) leagues: Bảng chưa tên các giải đấu (leagueID) appearances: Bảng thống kê của cầu thủ ở các game mà họ tham gia (gameID, playerID) teamstats: Bảng thống kê của đội bóng ở từng game (gameID, teamID) ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#chuẩn-bị-file-làm-raw-data"},{"categories":null,"content":"Load data vào MySQLCó nhiều cách để load data vào MySQL, ở đây mình sẽ sử dụng cách LOAD LOCAL_INFILE của MySQL luôn. Tip Hãy đảm bảo bạn đã make up lần đầu rồi nhé ! Hãy copy folder chứa các file csv vào de_mysql container: docker cp /football de_mysql:/tmp/dataset/ docker cp /load_data de_mysql:/tmp/dataset/ Sau đó tạo bảng trống sẵn trong MySQL: make mysql_create #Create table in mysql Tiếp tục với lệnh: make to_mysql_root # ----- You will access to MySQL container SET GLOBAL LOCAL_INFILE=TRUE; #Set local_infile variable to load data from local exit; # ----- Exit container make mysql_load #load data make mysql_create_relation #create table relation Thế là đã chuẩn bị xong dữ liệu cho MySQL. ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load-data-vào-mysql"},{"categories":null,"content":"Init PostgreSQL SchemaTa cũng cần phải tạo sẵn schema sẵn trong Posgres như sau: make to_psql CREATE SCHEMA IF NOT EXISTS analysis; exit; Thế là đã hoàn tất việc chuẩn bị data, giờ thì ta bắt đầu vào phần việc chính thôi ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#init-postgresql-schema"},{"categories":null,"content":"ImplementCông việc chính trong phần này là xây dựng các data pipeline bằng dagster. Cơ bản có thể hiểu là ta tạo các Asset và chuyển chúng từ database này sang database khác. ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#implement"},{"categories":null,"content":"ExtractionĐể có thể quản lý việc truy xuất dữ liệu từ MySQL và load vào MinIO để lưu tạm, ta sẽ xây dựng một I/O Manager phục vụ việc đó. Đầu tiên, hãy vào đường dẫn: ./etl_pipeline/etl_pipeline/resources/ Ta sẽ xây dựng MySQL io manager bằng cách tạo file mysql_io_manager.py với nội dung sau: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_mysql(config): conn_info = ( f\"mysql+pymysql://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class MySQLIOManager(IOManager): def __init__(self, config): self.config = config def handle_output(self, context: OutputContext, obj: pd.DataFrame): pass def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def extract_data(self, sql: str) -\u003e pd.DataFrame: with connect_mysql(self.config) as db_conn: pd_data = pd.read_sql_query(sql, db_conn) return pd_data Sau đó, tiếp tục đối với minio_io_manager.py: import os from contextlib import contextmanager from datetime import datetime from typing import Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from dagster import IOManager, InputContext, OutputContext from minio import Minio @contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\"endpoint_url\"), access_key=config.get(\"aws_access_key_id\"), secret_key=config.get(\"aws_secret_access_key\"), secure=False ) try: yield client except Exception: raise class MinIOIOManager(IOManager): def __init__(self, config): self._config= config def _get_path(self, context: Union[InputContext, OutputContext]): layer, schema, table = context.asset_key.path key = \"/\".join([layer, schema, table.replace(f\"{layer}_\", \"\")]) tmp_file_path = \"/tmp/file-{}-{}.parquet\".format( datetime.today().strftime(\"%Y%m%d%H%M%S\"), \"-\".join(context.asset_key.path) ) if context.has_asset_partitions: start, end = context.asset_partitions_time_window dt_format = \"%Y%m%d%H%M%S\" partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format) return os.path.join(key, f\"{partition_str}.pq\"), tmp_file_path else: return f\"{key}.pq\", tmp_file_path def handle_output(self, context: OutputContext, obj: pd.DataFrame): # convert to parquet format key_name, tmp_file_path = self._get_path(context) table = pa.Table.from_pandas(obj) pq.write_table(table, tmp_file_path) # upload to MinIO try: bucket_name = self._config.get(\"bucket\") with connect_minio(self._config) as client: # Make bucket if not exist. found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exists\") client.fput_object(bucket_name, key_name, tmp_file_path) row_count = len(obj) context.add_output_metadata({\"path\": key_name, \"tmp\": tmp_file_path}) # clean up tmp file os.remove(tmp_file_path) except Exception: raise def load_input(self, context: InputContext) -\u003e pd.DataFrame: bucket_name = self._config.get(\"bucket\") key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: #Make bucket if not exist found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exist\") client.fget_object(bucket_name, key_name, tmp_file_path) pd_data = pd.read_parquet(tmp_file_path) return pd_data except Exception: raise Sau khi đã tạo thành công các io manager cho mysql và minio, ta sẽ bắt đầu xây dựng bronze layer Note nho nhỏ Trong project này mình chia các giai đoạn transformation thành các layer: bronze layer: Giai đoạn chỏ mới load raw data, có thể hiểu đây là data chưa transform gì cả siler layer: Transform một phần từ bronze layer, ở đoạn này data đã được cleaning sơ gold layer: Sau khi transform một lần nữa từ silver layer, giai đoạn này sẽ truy vấn ra các thôn","date":"01 Aug 2023","objectID":"/football_etl_2/:2:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#extraction"},{"categories":null,"content":"TransformationTiếp tục tạo file silver_layer.py cùng folder với bronze layer: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teamstats\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"leagues\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], description='Statistic of teams in games', group_name=\"Silver_layer\", compute_kind=\"Pandas\" ) def silver_statsTeamOnGames(teamstats: pd.DataFrame, games: pd.DataFrame, leagues: pd.DataFrame) -\u003e Output[pd.DataFrame]: ts = teamstats.copy() gs = games.copy() lgs = leagues.copy() #Drop unsusable columns in games gs.drop(columns=gs.columns.to_list()[13:], inplace=True) #create StatperLeagueSeason result = pd.merge(ts, gs, on=\"gameID\") result = result.merge(lgs, on=\"leagueID\", how=\"left\") result.drop(columns=['season_y', 'date_y'],inplace=True) result = result.rename(columns={'season_x': 'season', 'date_x': 'date'}) return Output( result, metadata={ \"table\": \"statsTeamOnGames\", \"records\": len(result) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"appearances\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"players\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='statistic of players in games', compute_kind=\"Pandas\" ) def silver_playerAppearances(appearances: pd.DataFrame, games: pd.DataFrame, players: pd.DataFrame) -\u003e Output[pd.DataFrame]: app = appearances.copy() ga = games.copy() pla = players.copy() #Drop unusable column ga.drop(columns=ga.columns.to_list()[13:], inplace=True) #Merge player_appearances = pd.merge(app, pla, on=\"playerID\", how=\"left\") player_appearances = pd.merge(player_appearances, ga, on=\"gameID\", how=\"left\") #drop unecessary columns and rename player_appearances.drop(columns=['leagueID_y'],inplace=True) player_appearances.rename(columns={'leagueID_x': 'leagueID'}, inplace=True) return Output( player_appearances, metadata={ \"table\": \"playerAppearances\", \"records\": len(player_appearances) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teams\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='Teams', compute_kind=\"Pandas\" ) def silver_teams(teams: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( teams, metadata={ \"table\": 'teams', 'records': len(teams) } ) Lúc này mình có 3 silver assets, được join từ các bảng ở bronze Tiếp đến là gold_layer, lúc này ta sẽ tính các thông số thống kê của từng giải đâu từng mùa, các thống kê của cầu thủ trong 90 phút thi đấu, và cả thống kê của từng cầu thủ trong từng mùa giải gold_layer.py sẽ có nội dung: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_statsTeamOnGames\": AssetIn( key_prefix=[\"football\", \"silver\"] ) }, group_name=\"Gold_layer\", key_prefix=[\"football\", \"gold\"], description='Statistic of all league in each season', compute_kind=\"Pandas\" ) def gold_statsPerLeagueSeason(silver_statsTeamOnGames: pd.DataFrame) -\u003e Output[pd.DataFrame]: st = silver_statsTeamOnGames.copy() result = ( st.groupby(['name', 'season']) .agg({\"goals\": \"sum\", \"xGoals\": \"sum\", \"shots\": \"sum\", \"shotsOnTarget\": \"sum\", \"fouls\": \"sum\", \"yellowCards\": \"sum\", \"redCards\": \"sum\",'corners': 'sum', \"gameID\": 'count'}) .reset_index() ) result = result.rename(columns={'gameID':\"games\"}) result['goalPerGame']= result.goals/result.games result['season'] = result['season'].astype('string') return Output( result, metadata={ 'table': 'statPerLeagueSeason', 'records': len(result) } ) @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_playerAppearances\": AssetIn( key_prefix=[\"football\", \"silver\"] ) },","date":"01 Aug 2023","objectID":"/football_etl_2/:2:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#transformation"},{"categories":null,"content":"LoadTrước hết hãy tạo IO Manager cho Postgres để quản lý việc load cleaned data. Ta tạo file psql_io_manager.py ở vị trí mà ta đã tạo 2 io manager trước với nội dung: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_psql(config): conn_info = ( f\"postgresql+psycopg2://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class PostgreSQLIOManager(IOManager): def __init__(self, config): self._config = config def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def handle_output(self, context: OutputContext, obj: pd.DataFrame): schema, table = context.asset_key.path[-2], context.asset_key.path[-1] with connect_psql(self._config) as conn: # insert new data ls_columns = (context.metadata or {}).get(\"columns\", []) obj[ls_columns].to_sql( name=f\"{table}\", con=conn, schema=schema, if_exists=\"replace\", index=False, chunksize=10000, method=\"multi\" ) Sau đó, tạo một asset warehouse_layer.py: from dagster import multi_asset, Output, AssetIn, AssetOut, asset import pandas as pd @multi_asset( ins={ \"gold_statsPerLeagueSeason\": AssetIn( key_prefix=[\"football\", \"gold\"] ) }, outs={ \"statsperleagueseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerLeagueSeason\", 'analysis'], metadata={ \"columns\": [ \"name\", \"season\", \"goals\", \"xGoals\", \"shots\", \"shotsOnTarget\", \"fouls\", \"yellowCards\", \"redCards\", \"corners\", \"games\", \"goalPerGame\" ] } ), }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerLeagueSeason(gold_statsPerLeagueSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerLeagueSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerLeagueSeason\", \"records\": len(gold_statsPerLeagueSeason) } ) @multi_asset( ins={ \"gold_statsPerPlayerSeason\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsperplayerseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerPlayerSeason\", 'analysis'], metadata={ \"columns\": [ \"playerID\", \"name\", \"season\", \"goals\", \"shots\", \"xGoals\", \"xGoalsChain\", \"xGoalsBuildup\", \"assists\", \"keyPasses\", \"xAssists\", \"gDiff\", \"gDiffRatio\" ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerPlayerSeason(gold_statsPerPlayerSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerPlayerSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerPlayerSeason\", \"records\": len(gold_statsPerPlayerSeason) } ) @multi_asset( ins={ \"gold_statsPlayerPer90\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsplayerper90\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPlayerPer90\", 'analysis'], metadata={ \"columns\": [ 'playerID', 'name', 'total_goals', 'total_assists', 'total_time', 'goalsPer90', 'assistsPer90', 'scorers' ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPlayerPer90(gold_statsPlayerPer90: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPlayerPer90, metadata={ \"schema\": \"analysis\", \"table\": \"statsPlayerPer90\", \"records\": len(gold_statsPlayerPer90) } ) ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load"},{"categories":null,"content":"Run systemCuối cùng, ta sẽ kết hợp tất cả các asset lại giúp dagster nhận diện và quản lý với file __init__.py ở etl_pipeline/etl_pipeline/__init__.py import os from dagster import Definitions from .assets.silver_layer import * from .assets.gold_layer import * from .assets.bronze_layer import * from .assets.warehouse_layer import * from .resources.minio_io_manager import MinIOIOManager from .resources.mysql_io_manager import MySQLIOManager from .resources.psql_io_manager import PostgreSQLIOManager MYSQL_CONFIG = { \"host\": os.getenv(\"MYSQL_HOST\"), \"port\": os.getenv(\"MYSQL_PORT\"), \"database\": os.getenv(\"MYSQL_DATABASE\"), \"user\": os.getenv(\"MYSQL_USER\"), \"password\": os.getenv(\"MYSQL_PASSWORD\") } MINIO_CONFIG = { \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\"), \"bucket\": os.getenv(\"DATALAKE_BUCKET\"), \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"), \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\") } PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } ls_asset=[asset_factory(table) for table in tables] + [silver_statsTeamOnGames, silver_teams , silver_playerAppearances, gold_statsPerLeagueSeason, gold_statsPerPlayerSeason, gold_statsPlayerPer90, statsPerLeagueSeason, statsPerPlayerSeason, statsPlayerPer90] defs = Definitions( assets=ls_asset, resources={ \"mysql_io_manager\": MySQLIOManager(MYSQL_CONFIG), \"minio_io_manager\": MinIOIOManager(MINIO_CONFIG), \"psql_io_manager\": PostgreSQLIOManager(PSQL_CONFIG), } ) sau đó hãy dùng lệnh sau để cập nhật các assets docker restart etl_pipeline ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:4","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#run-system"},{"categories":null,"content":"Check UIHãy kiểm tra Dagit UI ở localhost:3001 để chắc chắn rằng mọi thứ vẫn ổn Ngoài ra cũng có thể check MinIO: localhost:9000 ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:5","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#check-ui"},{"categories":null,"content":"VisualizationCuối cùng là vẽ dashboard, đầu tiên ta cần phải lấy được data từ psql, hãy vào tạo file: ./streamlit/src/psql_connect.py: import os import psycopg2 from dotenv import load_dotenv import pandas as pd #load environment load_dotenv() #list table in database table = ['statsperleagueseason','statsperplayerseason', 'statsplayerper90'] PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } #create connection def init_connection(config): return psycopg2.connect( database=config['database'], user=config['user'], password=config['password'], host=config['host'], port=config['port'] ) def extract_data(): conn = init_connection(PSQL_CONFIG) return [pd.read_sql(f'SELECT * FROM analysis.{tab}', conn) for tab in table] Cuối cùng là tạo main.py ngay trong thư mục scr: import streamlit as st import pandas as pd import plotly.express as px import plotly.graph_objects as go from psql_connect import extract_data import numpy as np # #extract data from PostgreSQL ls_df = extract_data() l_season = ls_df[0] p_season = ls_df[1] p_match = ls_df[2] st.set_page_config(page_title = 'Dashboard Football', layout='wide', page_icon='chart_with_upwards_trend') #Overview def overview(table: pd.DataFrame, detail: str): if (st.checkbox('Do you want to see Data ?')): table col1, col2 = st.columns(2) co_df = table.columns.to_list() with col1: st.bar_chart(table.describe()) if (st.checkbox('Do you want to see describe each column ?')): for col in co_df: if table[col].dtypes not in ['int64', 'float64']: continue st.bar_chart(table[col].describe()) with col2: st.caption(f':red[Columns]: {len(co_df)}') st.caption(f':red[Records]: {len(table)}') st.caption(f':red[Description]: {detail}') st.caption(f':red[Columns name]:{co_df}') #league statistic def statleague(): Cards = l_season[['name','season','yellowCards', 'redCards', 'fouls']] #Card_fouls col1, col2 = st.columns(2) with col1: #Goals per games fig = px.bar(l_season, x=\"name\", y=\"goalPerGame\", color=\"name\", barmode=\"stack\", facet_col=\"season\", labels={\"name\": \"League\", \"goals/games\": \"GPG\"}) fig.update_layout(showlegend=False, title='Goals per Game') st.plotly_chart(fig) #fouls fig = px.line(Cards, x='season', y='fouls', color='name') fig.update_layout(title='Fouls of leagues', xaxis_title='Season', yaxis_title='Fouls', legend_title='League') st.plotly_chart(fig) with col2: #Red card fig = px.line(Cards, x='season', y='redCards', color='name') fig.update_layout(title='Red Cards of leagues', xaxis_title='Season', yaxis_title='RedCards', legend_title='League') st.plotly_chart(fig) #yellow card fig = px.line(Cards, x='season', y='yellowCards', color='name') fig.update_layout(title='Yellow Cards of leagues', xaxis_title='Season', yaxis_title='YellowCards', legend_title='League') st.plotly_chart(fig) #Player statistic def statplayer(): col1, col2 = st.columns(2) with col1: #Best offensive player top_player90= p_match[(p_match['goalsPer90'] \u003e 0.8) | (p_match['assistsPer90'] \u003e 0.4)] fig = px.scatter(p_match[['name','goalsPer90', 'assistsPer90']], x='goalsPer90', y='assistsPer90', hover_name='name') fig.add_trace( go.Scatter(x=top_player90['goalsPer90'], y=top_player90['assistsPer90'], mode='markers+text', marker_size=5, text=top_player90['name'], textposition='bottom center', textfont=dict(size=15)) ) fig.update_layout(title='Best offensive Players (2018-2020)', xaxis_title='Goals Per 90min', yaxis_title='Assists Per 90min') st.plotly_chart(fig) #goals-xgoal fig = px.scatter(p_season, x=\"xGoals\", y=\"goals\", color=(p_season['xGoals'] - p_season['goals'] \u003c 10), color_discrete_sequence=[\"red\", \"green\"], opacity=0.5) fig.update_layout(title=\"Goals (G) and Expected Goals (xG)\", xaxis_title=\"xG\", yaxis_title=\"G\", ) st.plotly_chart(fig) with col2: #Top score player topPlayer = p_season.groupby(['name']).agg({'goals': 'sum'}).sort_values('goals', ascending=False).res","date":"01 Aug 2023","objectID":"/football_etl_2/:3:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#visualization"},{"categories":null,"content":"ConclusionCuối cùng cũng đã xong một project xây dựng data pipeline cơ bản, trong lúc thực hiện chắc chắn sẽ có cả tấn lỗi xảy ra, nhưng OG tin là mọi gian khó đều sẽ vượt quan được, thứ đọng lại chính là kiến thức và kỹ năng của chúng ta. Chúc bạn thành công và đón xem tiếp các dự án tiếp theo của OG nhé ! -Mew- ","date":"01 Aug 2023","objectID":"/football_etl_2/:4:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#conclusion"},{"categories":null,"content":"Related Content Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#related-content"},{"categories":["projects"],"content":"A Data Engineer project building pipeline to analyze football data","date":"31 Jul 2023","objectID":"/football_etl/","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/"},{"categories":["projects"],"content":"Source PhongHuynh0394 Football_ETL_Analysis ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#"},{"categories":["projects"],"content":"IntroduceTrong project này, OG sẽ build end-to-end ETL data pipeline hoàn chỉnh để phân tích football data từ Kaggle với data pipeline như sau: Data pipelinedata pipeline \" Data pipeline Các bước cụ thể: Set up: Dùng Docker tạo container và các images cần thiết cho pipeline, trong đó có cả Dagster dùng xây dựng pipeline. Chuẩn bị data source: load các file csv (có được từ Kaggle) vào MySQL nhằm mục đích lưu trữ raw data (mô phỏng source data) Extract: Lấy data từ MySQL và load vào MinIO chuẩn bị cho bước transform Transform: Sử dụng Pandas để truy vấn các file từ MinIO Load: Cleaned và transformed data được load vào warehouse PostgreSQL Visualization: Sử dụng Streamlit để làm Dashboard ","date":"31 Jul 2023","objectID":"/football_etl/:1:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#introduce"},{"categories":["projects"],"content":"Set upBắt đầu với Docker, ta sẽ xây dựng lần lượt từng image bằng cách viết docker-compose.yml Tip nho nhỏ Hãy pull/build lần lượt từng loại framework lần lượt để chắc chắn rằng chúng hoạt động trơn tru nhất trước Hoặc bạn cũng có thể xem luôn phần hoàn chỉnh Hoàn chỉnh set up ","date":"31 Jul 2023","objectID":"/football_etl/:2:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#set-up"},{"categories":["projects"],"content":"MinIOMinIO là một server lưu trữ đối tượng dạng phân tán với hiệu năng cao và cung cấp các api giống với Amazon S3, ta có thể upload, download file,… một cách đơn giản. minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_network Note Về .env file, đây là file chứa thông tin các biến môi trường thiết lập cho từng image, mình sẽ nói sau ","date":"31 Jul 2023","objectID":"/football_etl/:2:1","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#minio"},{"categories":["projects"],"content":"MySQLMySQL là một trong số các phần mềm RDBMS (Relational DataBase Management Systems) phổ biến nhất, ta sẽ sử dụng database này để lưu raw data mô phỏng cho source data cần ingest de_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:2","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#mysql"},{"categories":["projects"],"content":"PostgeSQLPostgreSQL là một hệ thống quản trị cơ sở dữ liệu quan hệ-đối tượng (object-relational database management system), và ta sẽ dung nó làm data warehouse cho project lần này. de_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:3","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#postgesql"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagit"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster-deamon"},{"categories":["projects"],"content":"PipelineTất cả mọi việc xây dựng pipeline ta sẽ hoạt động ở đây Đầu tiên ta cần init một dagster project dagster project scaffold --name etl_pipeline và thư mục mới tạo sẽ trông như thế này: Tip Để chạy được lệnh dagster ở bước tạo dagster project, ta cần phải có dagster package, nếu chưa có hãy cài đặt bằng: pip install dagster –\u003e Nên cài đặt trong môi trường ảo Sau đó vào thư mục vừa tạo vào viết Dockerfile thôi: FROMpython:3.10-slim# Add repository codeWORKDIR/opt/dagster/appCOPY requirements.txt /opt/dagster/appRUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txtWORKDIR/opt/dagster/appCOPY . /opt/dagster/app/etl_pipeline# CMD allows this to be overridden from run launchers or executors that want to run other commands against your repositoryCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"etl_pipeline\"] cùng với requirements.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-aws==0.17.20 dagster-dbt==0.17.20 pandas==1.5.3 SQLAlchemy==1.4.46 pymysql==1.0.2 cryptography==39.0.0 pyarrow==10.0.1 boto3==1.26.57 fsspec==2023.1.0 s3fs==0.4.2 minio==7.1.13 Cuối cùng là viết vào docker-compose.yml: etl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:5","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#pipeline"},{"categories":["projects"],"content":"StreamlitCuối cùng là việc là Dashboard, Streamlit chắc chắc là công cụ siêu phù hợp làm việc này. Đây là framework hỗ trợ việc xây dựng giao diện ưu nhìn chỉ bằng Python, quá đã phải không nào :)) Hãy tạo folder ./streamlit/scr/ cùng với ./streamlit/Dockerfile: FROMpython:3.10EXPOSE8501WORKDIR/usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . . và streamlit/requirements.txt: pandas plotly matplotlib numpy streamlit psycopg2-binary sqlalchemy python-dotenv Cuối cùng là ghi trong yml streamlit:build:./streamymlcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:6","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#streamlit"},{"categories":["projects"],"content":"Hoàn chỉnh setupCuối cùng, file yaml sẽ trông như thế này: # version: '3.9'services:minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_networkde_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_networkde_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_networkstreamlit:build:./streamlitcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_networkde_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagsterde_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_networkde_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network# Pipelinesetl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_networknetworks:de_network:driver:bridgename:de_network Và .env file: # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_DB=football POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_HOST_AUTH_METHOD=trust # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=football # MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=football MYSQL_ROOT_PASSWORD=admin123 MYSQL_USER=admin MYSQL_PASSWORD=admin123 # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=warehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 Warning Nếu bạn chỉ đọc phần Hoàn chỉnh setup thì có thể hệ thống vễ sẽ gặp lỗi vì có thể thiếu các configuration cần thiết cho dagster, dagit hay pipeline. Bạn cần đọc qua phần Dagster, Dagit, Pipeline Chạy thử: sau khi hoàn tất toàn bộ set up dài ngoằn, cũng đã đến lúc chạy chương trình thôi. Note nho nhỏ Nếu bạn đã build lần lượt các images rồi, thì chỉ cần compose up thôi, nếu không, hãy build bằng lệnh docker compose build trước khi chạy compose up. docker compose --env-file .env up -d Lại là một tip có thể hữu ích Để đơn giản hóa việc ghi lệnh dài dòng, hãy tạo một make file tên Makefile với nội dung sau: include .env build: docker compose build up: docker compose --env-file .env up -d down: docker compose --env-file .env down restart: make down \u0026\u0026 make up to_psql: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} psql_create: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} -f /tmp/psql_schema.sql to_mysql: docker exec -it de_mysql mysql --local-infile=1 -u\"${MYSQL_U","date":"31 Jul 2023","objectID":"/football_etl/:2:7","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#hoàn-chỉnh-setup"},{"categories":["projects"],"content":"To be ContinueBài đến đây cũng quá là dài rồi, mình sẽ viết tiếp ở phần 2 :))) Chúc bạn một ngày tốt lành -Mew- ","date":"31 Jul 2023","objectID":"/football_etl/:3:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#to-be-continue"},{"categories":["projects"],"content":"Related Content Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... Football ETL Analysis P2 Continuous of Football ETL series Read more... ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#related-content"},{"categories":[],"content":"Data Engineering Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#data-engineering"},{"categories":[],"content":"Machine learning Basic Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#machine-learning-basic"},{"categories":null,"content":" Hello! Mình tên là Vĩnh Phong hay chính là OG (tác giả của các blogs ở trang này) Hiện tại mình là sinh viên ngành Khoa học Dữ liệu (Data Science) tại Đại học Khoa học Tự nhiên, ĐHQG-HCM (HCMUS). Tuy nhiên mình cũng yêu thích công nghệ, code và mình đang trên con đường học tập mỗi ngày để trở thành một Data Engineer. OG hồi cuối lớp 12OG hồi cúi lớp 12 :)) \" OG hồi cuối lớp 12 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#"},{"categories":null,"content":"Mình của hiện tạiĐầu tiên quan trọng nhất chính là việc học tại HCMUS. Ngoài ra, mình còn tự học về các chủ đề liên quan như Data pipeline, Data Streaming,… Việc luyện tập, học hỏi cũng ăn sâu dô máu mình lúc nào không hay :)) Mình đã từng đọc thấy đâu đó rằng: The most beautiful thing about learning is that no one take that away from you Và điều đó đã truyền cảm hứng mình rất nhiều, thúc đẩy bản thân tự học mỗi ngày và tự làm mới bản thân từng chút một. Tiếp đến chính là xây dựng trang web này và viết các bài blogs giúp cho các bạn có thể học thêm kiến thức ngành, biết thêm điều thú vị và thư giãn. Hiện tại thì OG vẫn còn ngồi trên ghế giảng đường, và vừa đặt những viên gạch đầu tiên trên con đường tự trưởng thành. Hãy luôn theo dõi OG nhé! -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huỳnh Lưu Vĩnh Phong Facebook: Phong Huynh Instagram: phong_huynh Hoặc bạn cũng có thể ghé thăm Github của mình: PhongHuynh0394 Huỳnh Lưu Vĩnh Phong ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#mình-của-hiện-tại"}]