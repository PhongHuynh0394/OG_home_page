[{"categories":["projects"],"content":"Source github: Stock Analysis Hello! OG ƒë√¢y. ·ªû project l·∫ßn n√†y m√¨nh s·∫Ω ph√¢n t√≠ch gia tr·ªã c·ªï phi·∫øu ph√°i sinh VN30 Index b·∫±ng c√°ch s·ª≠ d·ª•ng PCA v√† K-means. Xin v√¥ c√πng c·∫£m ∆°n s·ª± ƒë√≥ng g√≥p c·ªßa 5 th√†nh vi√™n team OG v√† th·∫ßy Minh M·∫´n v√† th·∫ßy Ho√†ng ƒê·ª©c ƒë√£ t·∫≠n t√¨nh h∆∞·ªõng d·∫´n ƒë·ªÉ team c√≥ th·ªÉ ho√†n th√†nh ƒë·ªì √°n m·ªôt c√°ch t·ªët nh·∫•t. R·ªìi b√¢y gi·ªù g√©t g√¥ thooiii üòÑ Stock AnalysisStock Analysis \" Stock Analysis ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#"},{"categories":["projects"],"content":"IntroStock Analysis hay c√≤n g·ªçi l√† Market Analysis ƒë·ªÅ c·∫≠p ƒë·∫øn ph∆∞∆°ng ph√°p m√† nh√† ƒë·∫ßu t∆∞ ho·∫∑c nh√† giao d·ªãch s·ª≠ d·ª•ng ƒë·ªÉ ƒë√°nh gi√° v√† ƒëi·ªÅu tra m·ªôt c√¥ng c·ª• giao d·ªãch c·ª• th·ªÉ, lƒ©nh v·ª±c ƒë·∫ßu t∆∞ ho·∫∑c to√†n b·ªô th·ªã tr∆∞·ªùng ch·ª©ng kho√°n. Kh√¥ng nh·ªØng th·∫ø, n√≥ li√™n quan ƒë·∫øn vi·ªác nghi√™n c·ª©u d·ªØ li·ªáu th·ªã tr∆∞·ªùng trong qu√° kh·ª© v√† hi·ªán t·∫°i v√† t·∫°o ra m·ªôt ph∆∞∆°ng ph√°p ƒë·ªÉ ch·ªçn c·ªï phi·∫øu ph√π h·ª£p ƒë·ªÉ giao d·ªãch. C√°c nh√† ƒë·∫ßu t∆∞ s·∫Ω ƒë∆∞a ra quy·∫øt ƒë·ªãnh mua ho·∫∑c b√°n d·ª±a tr√™n th√¥ng tin ph√¢n t√≠ch ch·ª©ng kho√°n. Trong project n√†y ta s·∫Ω ph√¢n t√≠ch, tr·ª±c quan h√≥a b·ªô d·ªØ li·ªáu gi·∫£ ƒë·ªãnh ƒë∆∞·ª£c cung c·∫•p b·ªüi kh√°ch h√†ng ƒë·ªÉ ƒë√°nh gi√° th·ªã tr∆∞·ªùng ch·ª©ng kho√°n trong kho·∫£ng th·ªùi gian 1 th√°ng c·ªßa 30 c√¥ng ty thu·ªôc VN30 D∆∞·ªõi ƒë√¢y l√† t√≥m t·∫Øt s∆° l∆∞·ª£c t·ª´ng b∆∞·ªõc ƒë·ªÉ x·ª≠ l√Ω v√† ph√¢n t√≠ch: EDA (Exploratory Data Analysis) Data Preprocessing PCA (Principle Component Analysis) K-Means Clustering Data Analysis References Raw Data Source: df_merged.pkl ","date":"31 Aug 2023","objectID":"/stock_analysis/:1:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#intro"},{"categories":["projects"],"content":"Exploratory Data Analysisƒê√¢y l√† b∆∞·ªõc ƒë·∫ßu ti√™n, ch√∫ng ta s·∫Ω c√πng nhau Warning B·∫£n demo n√™n m√¨nh ch∆∞a ghi g√¨ :v B·∫°n ƒë·ªçc notebook ·ªü source github tr∆∞·ªõc nh√©! ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#exploratory-data-analysis"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#related"},{"categories":[],"content":"Hellooo OG ƒë√¢yy ! ·ªû b√†i n√†y, m√¨nh s·∫Ω k·ªÉ c∆° duy√™n ƒë∆∞a m√¨nh ƒë·∫øn v·ªõi ng√†nh Data v√† quy·∫øt ƒë·ªãnh d·∫•n th√¢n v√†o con ƒë∆∞·ªùng tr·ªü th√†nh m·ªôt Data Engineer üòÑ G√©t Goo! ","date":"30 Aug 2023","objectID":"/start_journey/:0:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#"},{"categories":[],"content":"·ª¶a ng√†nh Data Science ?Khoan Khoan ‚Ä¶ B√™n tr√™n l√† Engineer, qua ƒë√¢y l√† Science l√† sao OG ? T·ª´ t·ª´ n√†o üòÑ M·ªçi chuy·ªán b·∫Øt ƒë·∫ßu khi m√¨nh ƒë·∫≠u v√†o m·ªôt ng√†nh ƒë∆∞·ª£c ca ng·ª£i l√† ng√†nh ‚Äúquy·∫øn r≈©‚Äù nh·∫•t th·∫ø k·ª∑ 21 theo Harvard Business Review , ƒë√≥ l√† Data Science. Kh√∫c n√†y m√¨nh nghe c≈©ng o√°ch o√°ch, nh∆∞ng ch√≠nh x√°c Data Science l√† g√¨ ? V√† c√°c nh√† khoa h·ªçc d·ªØ li·ªáu (data scientist) l√†m g√¨ ? ","date":"30 Aug 2023","objectID":"/start_journey/:1:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#·ªßa-ng√†nh-data-science-"},{"categories":[],"content":"Data Science l√† g√¨ nh·ªâ?Ng√†nh Khoa h·ªçc d·ªØ li·ªáu hay Data Science l√† m·ªôt lƒ©nh v·ª±c li√™n ng√†nh ·ª©ng d·ª•ng c√°c ph∆∞∆°ng ph√°p khoa h·ªçc, thu·∫≠t to√°n v√† c√°c ph√¢n t√≠ch th·ªëng k√™ ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a t·ª´ d·ªØ li·ªáu. Hay n√≥i b·∫±ng c√°ch d·ªÖ hi·ªÉu, Data Science l√† ng√†nh t√¨m ki·∫øm, ph√¢n t√≠ch d·ªØ li·ªáu ƒë·ªÉ khai th√°c t·∫•t c·∫£ nh·ªØng gi√° tr·ªã m√† d·ªØ li·ªáu mang l·∫°i ƒë·ªÉ ph·ª•c v·ª• nhi·ªÅu m·ª•c ƒë√≠ch kh√°c nhau. Data Science l√† ·ª©ng d·ª•ng khoa h·ªçc ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a c·ªßa d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n t∆∞∆°ng laiData Science l√† ·ª©ng d·ª•ng khoa h·ªçc ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a c·ªßa d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n \" Data Science l√† ·ª©ng d·ª•ng khoa h·ªçc ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a c·ªßa d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n t∆∞∆°ng lai M·ªôt nh√† khoa h·ªçc d·ªØ li·ªáu (Data Scientist) l√† ng∆∞·ªùi ch·ªãu tr√°ch nhi·ªám ƒë∆∞a ra c√°c d·∫´n ch·ª©ng t·ª´ d·ªØ li·ªáu, ƒë·ªÉ t·ª´ ƒë√≥ ƒë·ªÅ xu·∫•t c√°c gi·∫£i ph√°p, k·∫ø ho·∫°ch hay ƒë·ªãnh h∆∞·ªõng t·ª´ √Ω nghƒ©a t√¨m ƒë∆∞·ª£c t·ª´ d·ªØ li·ªáu ƒë·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n kinh doanh kh√°c nhau. M·ªôt data scientist c·∫ßn ph·∫£i bi·∫øt k·ªπ nƒÉng g√¨? L·∫≠p tr√¨nh: Python v√† R l√† 2 ng√¥n ng·ªØ ch√≠nh ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªëi v·ªõi ng√†nh n√†y. Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh linh ho·∫°t ph·ªï bi·∫øn v·ªõi r·∫•t nhi·ªÅu th∆∞ vi·ªán ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu nh∆∞ numpy, pandas, matplotlib,‚Ä¶ Trong khi ƒë√≥ R t·ªè l√† l√† m·ªôt ng√¥n ng·ªØ m·∫°nh m·∫Ω v·ªÅ ph√¢n t√≠ch v√† th·ªëng k√™, ngo√†i ra R c≈©ng th∆∞·ªùng ƒë∆∞·ª£c d√πng trong nghi√™n c·ª©u v√† h·ªçc thu·∫≠t. Th·ªëng k√™ v√† ·ª©ng d·ª•ng to√°n h·ªçc: N·∫øu b·∫°n kh√¥ng y√™u th√≠ch to√°n h·ªçc, ch·∫Øc h·∫≥n b·∫°n c≈©ng s·∫Ω kh√¥ng th·ªÉ l√†m ƒëi·ªÅu ƒë√≥ v·ªõi data science. H·∫≥n v·∫≠y, l√† m·ªôt nh√† khoa h·ªçc d·ªØ li·ªáu, b·∫°n c·∫ßn c√≥ m·ªôt n·ªÅn t·∫£ng ki·∫øn th·ª©c to√°n h·ªçc v·ªØng, ƒë·∫∑c bi·ªát l√† v·ªÅ x√°c su·∫•t th·ªëng k√™ v√† ƒë·∫°i s·ªë tuy·∫øn t√≠nh,‚Ä¶ SQL v√† DBMS: Ta ph·∫£i ti·∫øp x√∫c r·∫•t nhi·ªÅu v·ªõi h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu (Database Management System hay DBMS), ƒë√≥ c√≥ th·ªÉ l√† h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu Quan H·ªá (Relational Database Management System) nh∆∞ MySQL, Postgres, SQL server‚Ä¶ hay NoSQL database nh∆∞ MongoDB, Cassandra,‚Ä¶ V√† ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi database (RDBMS), ƒëi·ªÅu kh√¥ng th·ªÉ thi·∫øu ch√≠nh l√† SQL (Structured query language aka si c·ªì hay √©t qui eo üòÇ ). C∆° b·∫£n th√¨ ƒë√¢y l√† ng√¥n ng·ªØ d√πng ƒë·ªÉ truy su·∫•t d·ªØ li·ªáu, giao ti·∫øp v·ªõi database, ƒë·∫∑c bi·ªát l√† c√°c RDBMS. AI, Machine learning: Khi c√≥ m·ªôt l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì, m·ªôt data scientist c√≥ th·ªÉ s·∫Ω d√πng ch√∫ng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc m√°y ho·∫∑c m·∫°ng ƒë·ªÉ gi·∫£i c√°c b√†i to√°n h·ªìi quy v√† ƒë∆∞a ra ƒë∆∞·ª£c c√°c d·ª± ƒëo√°n v·ªÅ xu h∆∞·ªõng data hay gi·∫£i quy·∫øt c√°c b√†i to√°n ph√¢n lo·∫°i. C√≥ hi·ªÉu bi·∫øt v·ªÅ c√°c thu·∫≠t to√°n m√°y h·ªçc v√† ki·∫øn tr√∫c m·∫°ng noron c≈©ng l√† m·ªôt ƒëi·ªÅu c·∫ßn c√≥ ·ªü nh√† khoa h·ªçc d·ªØ li·ªáu. ƒê·ªçc ƒë·∫øn ƒë√¢y, c√≥ th·ªÉ b·∫°n s·∫Ω c√≥ c·∫£m gi√°c ‚ÄúD√®j√† vu‚Äù nh·∫π ‚Ä¶ Sao nhi·ªÅu ch·ªï gi·ªëng Data Analyst th·∫ø nh·ªâ ? M√† thi·ªát ra l√† kh√¥ng gi·ªëng ƒë√¢u nh√©, hai ng√†nh n√†y ch·ªâ l√† anh em x√£ h·ªôi v·ªõi nhau m√† th√¥i üòÇ ","date":"30 Aug 2023","objectID":"/start_journey/:1:1","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-science-l√†-g√¨-nh·ªâ"},{"categories":[],"content":"Data Scientist vs Data AnalystS·∫µn ti·ªán k·ªÉ m·ªôt ch√∫t v·ªÅ vai tr√≤ c·ªßa m·ªôt ng∆∞·ªùi Data Analyst. V·ªÅ c∆° b·∫£n, vai tr√≤ c·ªßa h·ªç c≈©ng gi·ªëng v·ªõi c√°c data scientist, h·ªç c≈©ng ph√¢n t√≠ch d·ªØ li·ªáu, c·ªë g·∫Øng t√¨m ki·∫øm v√† r√∫t ra gi√° tr·ªã t·ª´ ch√∫ng. Nh∆∞ng s·∫Ω c√≥ m·ªôt s·ªë ƒëi·ªÉm kh√°c bi·ªát: Data Analyst: hay th∆∞·ªùng ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† chuy√™n vi√™n ph√¢n t√≠ch d·ªØ li·ªáu, h·ªç l√†m c√¥ng vi·ªác c·ªßa data science nh∆∞ng h·ªç kh√¥ng b·∫Øt bu·ªôc ph·∫£i bi·∫øt nhi·ªÅu v·ªÅ l·∫≠p tr√¨nh. H·ªç ƒë∆∞·ª£c ƒë√≤i h·ªèi ph·∫£i c√≥ nhi·ªÅu ki·∫øn th·ª©c h∆°n v·ªÅ ho·∫°t ƒë·ªông kinh doanh v√† v·ªØng v·ªÅ ki·∫øn th·ª©c th·ªëng k√™. Data Scientist: Nh√† khoa h·ªçc d·ªØ li·ªáu, h·ªç ƒë·∫£m nh·∫≠n c√¥ng vi·ªác v·ªõi quy m√¥ l·ªõn h∆°n r·∫•t nhi·ªÅu (Bigdata) v√† h·ªç ƒë∆∞·ª£c ƒë√†o t·∫°o ƒë·ªÉ ph√°t tri·ªÉn ‚Äúdata product‚Äù nh·∫±m ƒë∆∞a ra quy·∫øt ƒë·ªãnh c√≥ √≠ch cho c√¥ng ty t·ª´ l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì t·ª´ t·∫≠p d·ªØ li·ªáu l·ªõn. Ngo√†i ra nh√† khoa h·ªçc d·ªØ li·ªáu c≈©ng c·∫ßn ph·∫£i bi·∫øt m·ªôt s·ªë ng√¥n ng·ªØ l·∫≠p tr√¨nh nh·∫•t ƒëinh v√† c√≥ ki·∫øn th·ª©c v·ªÅ computer science. Data Science and Data AnalyticSource: https://www.datascience-pm.com/wp-content/uploads/2021/05/data-scientist-vs-analyst-venn-diagram.png \" Data Science and Data Analytic R·ªìi okay n√£y gi·ªù l√† c·∫£ data science (DS) v√† data analytic (DA) r·ªìi. Gi·ªù l√† m·ªõi ƒë·∫øn data engineer c·ªßa tui n√® hihi üòÑ ","date":"30 Aug 2023","objectID":"/start_journey/:1:2","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-scientist-vs-data-analyst"},{"categories":[],"content":"Data Engineer l√† g√¨ ?Tuy h·ªçc Data Science, nh∆∞ng th·ª±c ra ngay t·ª´ nh·ªØng l√∫c c√≤n m∆°n m·ªün c·∫•p 3, OG ƒë√£ t·ª´ng c√≥ ∆∞·ªõc mu·ªën tr·ªü th√†nh m·ªôt l·∫≠p tr√¨nh vi√™n m·ªôt tay cafe m·ªôt tay ch√©m code b√¨nh lo·∫°n thi√™n h·∫° üòÇ V√† th·∫ø l√† t√¨m ƒë∆∞·ª£c m·ªôt ng√†nh th√≠ch h·ª£p ƒë∆∞·ª£c coi l√† ‚ÄúSoftware engineer cho data‚Äù, ng√†nh n√†y l√† m·ªôt trong c√°c ng√†nh c√≥ xu h∆∞·ªõng ph√°t tri·ªÉn nhanh nh·∫•t trong nh√≥m ng√†nh c√¥ng ngh·ªá. V√¢ng ƒë√≥ ch√≠nh l√† Data Engineer Data EngineerData Engineer ƒë∆∞·ª£c coi l√† Software Engineer ·ªü Data field \" Data Engineer ƒê·∫ßu ti√™n, Data Engineer hay DE ƒë∆∞·ª£c g·ªçi l√† k·ªπ s∆∞ d·ªØ li·ªáu. ƒê√¢y l√† vai tr√≤ ƒë·∫£m nhi·ªám vi·ªác ph√¢n t√≠ch ngu·ªìn d·ªØ li·ªáu, x√¢y d·ª±ng v√† duy tr√¨ h·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu hi·ªáu qu·∫£. Ngo√†i ra c≈©ng l√† ng∆∞·ªùi ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho c√°c ph√≤ng ban kh√°c s·ª≠ d·ª•ng. C∆° b·∫£n ƒë·ªÉ l√† ƒë·ªÉ cho DS v√† DA l√†m vi·ªác m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t, h·ªç c·∫ßn c√≥ m·ªôt ngu·ªìn data ·ªïn ƒë·ªãnh v√† s·∫°ch s·∫Ω. V√† ng∆∞·ªùi ƒë·∫£m nhi·ªám vi·ªác lu√¢n chuy·ªÉn data ƒë√≥ t·ªõi cho h·ªç ch√≠nh l√† Data Engineer. Kh√¥ng ch·ªâ c√≥ DS v√† DA m√† data engineer ph·ª•c v·ª• cho t·∫•t c·∫£ c√°c ph√≤ng ban kh√°c Data Engineer N√≥i t√≥m l·∫°i, Data Engineer l√† ng∆∞·ªùi x√¢y d·ª±ng c√°c ƒë∆∞·ªùng ·ªëng d·ªØ li·ªáu (data pipeline) ƒë·ªÉ truy·ªÅn d·ªØ li·ªáu t·ª´ n∆°i n√†y sang n∆°i kh√°c m·ªôt c√°ch ch·∫•t l∆∞·ª£ng nh·∫•t :)) Kh√°i ni·ªám c∆° b·∫£n l√† th·∫ø th√¥i, nghe c√≥ v·∫ª ƒë∆°n gi·∫£n ph·∫£i kh√¥ng. H√£y ti·∫øp t·ª•c v·ªõi m·ª•c ti·∫øp theo ƒë·ªÉ xem li·ªáu ta c·∫ßn g√¨ ƒë·ªÉ tr·ªü th√†nh data engineer ","date":"30 Aug 2023","objectID":"/start_journey/:2:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-engineer-l√†-g√¨-"},{"categories":[],"content":"Data Engineer th√¨ c·∫ßn bi·∫øt g√¨ ?M·ªôt data Engineer v·ªÅ b·∫£n ch·∫•t l√† x√¢y d·ª±ng c√°c data pipeline ƒë·ªÉ lu√¢n chuy·ªÉn d·ªØ li·ªáu. ƒê·ªÉ l√†m t·ªët vi·ªác ƒë√≥, k·ªπ s∆∞ d·ªØ li·ªáu ph·∫£i bi·∫øt: K·ªπ nƒÉng l·∫≠p tr√¨nh: T·∫•t nhi√™n r·ªìi, b·∫°n l√† m·ªôt nh√¢n vi√™n IT th√¨ ƒëi·ªÅu n√†y l√† ph·∫£i c√≥. C√°c ng√¥n ng·ªØ m√† DE th∆∞·ªùng d√πng l√† SQL, Python v√† R. H·ªá c∆° s·ªü d·ªØ li·ªáu quan h·ªá v√† phi quan h·ªá: D·ªØ li·ªáu c√≥ r·∫•t nhi·ªÅu d·∫°ng: Structure/Semi/Unstructure data, do ƒë√≥ c≈©ng c·∫ßn c√≥ nhi·ªÅu lo·∫°i database qu·∫£n l√Ω ch√∫ng. V√† DE l√†m vi·ªác r·∫•t nhi·ªÅu v·ªõi database. H·ªç s·∫Ω l√† ng∆∞·ªùi tr·ª±c ti·∫øp t∆∞∆°ng t√°c k·ªÉ c·∫£ v·ªõi SQL v√† NoSQL database. ETL/ELT: ETL aka Extract Transform Load hay ELT aka Extract Load Transform l√† quy tr√¨nh x·ª≠ l√Ω v√† lu√¢n chuy·ªÉn d·ªØ li·ªáu t·ª´ ngu·ªìn ƒë·∫øn ƒë√≠ch. M·ªôt DE ph·∫£i n·∫Øm ƒë∆∞·ª£c ƒë·ªÉ thi·∫øt k·∫ø data pipeline m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t Data Warehouse: hay ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† kho ch·ª©a d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ s·∫Ω ph·∫£i x√¢y d·ª±ng, thi·∫øt k·∫ø c·∫•u tr√∫c data warehouse tr√™n cloud platform v√† x√¢y d·ª±ng c√°c k·∫øt n·ªëi d·ªØ li·ªáu ƒë·ªÉ t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô truy xu·∫•t v√† ƒë·∫£m b·∫£o vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu. Big Data: B·∫°n c≈©ng c·∫ßn ph·∫£i bi·∫øt c√°c ki·∫øn tr√∫c l∆∞u tr·ªØ v√† x·ª≠ l√Ω t·∫≠p d·ªØ li·ªáu l·ªõn nh∆∞ Hadoop, Spark,‚Ä¶ Cloud: T·∫•t nhi√™n l√† ph·∫£i c√≥ r·ªìi, c√°c cloud platform nh∆∞ Google Cloud Platform, AWS, Azure,‚Ä¶ ƒë√£ r·∫•t n·ªïi ti·∫øng trong vi·ªác h·ªó tr·ª£ x√¢y d·ª±ng v√† thi·∫øt k·∫ø h·ªá th·ªëng pipeline c≈©ng nh∆∞ h·ªó tr·ª£ t·ªëi ƒëa vi·ªác x·ª≠ l√Ω bigdata c≈©ng nh∆∞ deploy h·ªá th·ªëng h·∫° t·∫ßng m·ªôt c√°ch nhanh ch√≥ng. B·∫°n c√≥ th·ªÉ s·∫Ω ph·∫£i l√†m vi·ªác v·ªõi l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì v√† t·∫≠p d·ªØ li·ªáu l·ªõn. V√† ƒë·ªÉ x√¢y d·ª±ng h·ªá th·ªëng x·ª≠ l√Ω ƒë∆∞·ª£c l∆∞·ª£ng d·ªØ li·ªáu ƒë√≥, ch·∫Øc ch·∫Øn ph·∫£i c√≥ s·ª± g√≥p m·∫∑t c·ªßa c√°c n·ªÅn t·∫£ng ƒë√°m m√¢y. Well‚Ä¶ Nh√¨n chung c≈©ng nhi·ªÅu th·ª© c·∫ßn ph·∫£i bi·∫øt ƒë·∫•y nh·ªâ, t·∫•t nhi√™n ƒë√≥ ch·ªâ l√† m·ªôt s·ªë ƒëi·ªÅu quan tr·ªçng nh·∫•t. Ngo√†i ra b·∫°n c≈©ng c·∫ßn ph·∫£i bi·∫øt m·ªôt s·ªë ki·∫øn th·ª©c kh√°c v·ªÅ Unix v√† Linux, Docker, Git, Batch/Stream Processing,‚Ä¶ V√† c√≤n ti t·ªâ th·ª© kh√°c m√† OG c√≥ k·ªÉ ƒë·∫øn rƒÉng long ƒë·∫ßu b·∫°c c√≥ l·∫Ω c≈©ng ch∆∞a h·∫øt üòÇ ","date":"30 Aug 2023","objectID":"/start_journey/:2:1","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-engineer-th√¨-c·∫ßn-bi·∫øt-g√¨-"},{"categories":[],"content":"T·∫°m k·∫øtH√†nh tr√¨nh n√†o khi b·∫Øt ƒë·∫ßu c≈©ng gian nan, c·∫£ b·∫£n th√¢n OG khi b·∫Øt ƒë·∫ßu c≈©ng kh√¥ng bi·∫øt g√¨ c·∫£. Nh∆∞ng khi nh·∫•c ng√≥n ch√¢n l√™n v√† ƒëi th√¨ m·ªõi c·∫£m nh·∫≠n ƒë∆∞·ª£c th·∫ø gi·ªõi ch·ª© üòÑ Hy v·ªçng b√†i vi·∫øt n√†y gi√∫p b·∫°n th∆∞ gi√£n v√† c√≥ m·ªôt c√°i nh√¨n chung v·ªÅ ng√†nh data nh√©. H·∫πn g·∫∑p l·∫°i trong b√†i ti·∫øp theo hehe -Meww ","date":"30 Aug 2023","objectID":"/start_journey/:3:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#t·∫°m-k·∫øt"},{"categories":[],"content":"Gi·ªõi thi·ªáu v·ªÅ trang web ƒë√°ng y√™u n√†y","date":"30 Aug 2023","objectID":"/intro_blog/","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/"},{"categories":[],"content":"Hello! Hello! m√¨nh l√† OG ƒë√¢y. ƒê√¢y l√† b√†i blog ƒë·∫ßu ti√™n c·ªßa m√¨nh ·ªü ƒë√¢y. C√≥ th·ªÉ b·∫°n ƒëang t·ª± h·ªèi r·∫±ng m√¨nh l√† ai v√† ƒë√¢y l√† n∆°i n√†o ƒë√∫ng kh√¥ng, v·∫≠y h√£y c√πng m√¨nh t√¨m hi·ªÉu th·ª≠ nh√© ","date":"30 Aug 2023","objectID":"/intro_blog/:0:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#"},{"categories":[],"content":"OG l√† ai ?M·ªôt l·∫ßn n·ªØa gi·ªõi thi·ªáu m√¨nh t√™n l√† Vƒ©nh Phong - m·ªôt anh ch√†ng th√≠ch t√¨m t√≤i ƒëi·ªÅu m·ªõi‚Ä¶ hmmm th·∫ø th√¥i :))) ƒê·ªÉ hi·ªÉu th√™m v·ªÅ m√¨nh: About C√≤n OG (nickname) l√† bi·ªát danh m√¨nh l·∫•y c·∫£m h·ª©ng t·ª´ m·ªôt nh√¢t v·∫≠t ho·∫°t h√¨nh r·∫•t h√≥m h·ªânh ƒë·∫•y nh√© hehe. ƒê√≥ l√† t√™n m·ªôt ch√∫ m√®o xanh d∆∞∆°ng hay b·ªã m·∫•y con gi√°n qu·∫≠y :)) b·∫°n th·ª≠ ƒëo√°n xem ƒê√°p √°n ·ª¶a l·ªôn n√†y l√† tui:))Tui \" ·ª¶a l·ªôn n√†y l√† tui:)) ƒê√¢y m·ªõi l√† ƒë√°p √°n N·∫øu b·∫°n hong bi·∫øt, th√¨ con m√®o xanh l√® n√†y t√™n l√† Oggy v√† nickname m√¨nh c≈©ng v·∫≠y :)) Bi·∫øt m√¨nh l√† ai r·ªìi, th·∫ø th√¨‚Ä¶ ","date":"30 Aug 2023","objectID":"/intro_blog/:1:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#OG-la-ai"},{"categories":[],"content":"ƒê√¢y l√† n∆°i n√†o ƒë√¢y ?ƒê√¢y trang m√† m√¨nh ƒëƒÉng l√™n c√°c b√†i blogs v·ªÅ c√¥ng ngh·ªá, v·ªÅ ng√†nh Data n√≥i chung v√† v·ªÅ h√†nh tr√¨nh h·ªçc t·∫≠p c·ªßa OG ƒë·ªÉ tr·ªü th√†nh m·ªôt Data Engineer trong t∆∞∆°ng lai. T·∫•t nhi√™n kh√¥ng ch·ªâ nh∆∞ v·∫≠y M√¨nh c≈©ng vi·∫øt blogs v·ªÅ ƒë·ªùi s·ªëng, v·ªÅ nh·ªØng ƒëi·ªÅu th√∫ v·ªã c·ªßa cu·ªôc s·ªëng xung quanh V√† m√¨nh hy v·ªçng trang c≈©ng n√†y s·∫Ω l√† n∆°i mang l·∫°i s·ª± tho·∫£i m√°i v√† th∆∞ gi·∫£n cho m·ªçi ng∆∞·ªùi ","date":"30 Aug 2023","objectID":"/intro_blog/:2:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#day-la-dau"},{"categories":[],"content":"Th·∫ø ·ªü ƒë√¢y c√≥ g√¨ hay?T√≥m t·∫Øt c√°c trang: Blogs: b·∫°n c√≥ th·ªÉ t√¨m th·∫•y c√°c b√†i blogs m√¨nh vi·∫øt ·ªü ƒë√¢y Projects: Nh·ªØng d·ª± √°n m√¨nh ƒë√£ l√†m About: N·∫øu b·∫°n mu·ªën hi·ªÉu th√™m v·ªÅ m√¨nh ƒê√≥ l√† t·ªïng quan ‚Äúc√°c th·ª© c√≥ th·ªÉ s·∫Ω xu·∫•t hi·ªán‚Äù ·ªü trang web n√†y. ","date":"30 Aug 2023","objectID":"/intro_blog/:2:1","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#th·∫ø-·ªü-ƒë√¢y-c√≥-g√¨-hay"},{"categories":[],"content":"Cu·ªëi c√πngOG hy v·ªçng ƒë√¢y s·∫Ω l√† n∆°i gi√∫p b·∫°n th∆∞ gi√£n hay h·ªçc h·ªèi ƒë∆∞·ª£c nhi·ªÅu ƒëi·ªÅu th√∫ v·ªã nh√© üòÑ -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Hu·ª≥nh L∆∞u Vƒ©nh Phong Facebook: Phong Huynh Instagram: phong_huynh Ho·∫∑c b·∫°n c≈©ng c√≥ th·ªÉ gh√© thƒÉm Github c·ªßa m√¨nh: PhongHuynh0394 ","date":"30 Aug 2023","objectID":"/intro_blog/:3:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#cuoi-cung"},{"categories":null,"content":"Continuous of Football ETL series","date":"01 Aug 2023","objectID":"/football_etl_2/","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/"},{"categories":null,"content":"Hello! Hello! OG ƒë√¢y, sau ph·∫ßn 1 ch√∫ng ta ƒë√£ setup c√°c ki·ªÉu v√† ƒë·∫£m b·∫£o m·ªçi th·ª© tr∆°n tru r·ªìi, ·ªü ph·∫ßn n√†y ch√∫ng ta s·∫Ω chu·∫©n b·ªã Data Source, v√† kh·ªüi ch·∫°y pipeline ·ªü Implement sau ƒë√≥ s·∫Ω Visualize cleaned data c√≥ ƒë∆∞·ª£c t·ª´ data warehouse th√†nh Dashboard. B·∫Øt ƒë·∫ßu th√¥i n√†o ! ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#"},{"categories":null,"content":"Data Source","date":"01 Aug 2023","objectID":"/football_etl_2/:1:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#data-source"},{"categories":null,"content":"Chu·∫©n b·ªã file l√†m raw dataC√°c file csv s·ª≠ d·ª•ng l√†m d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i t·ª´ Football Database - Kaggle. ƒê√¢y l√† d·ªØ li·ªáu th·ªëng k√™ c·ªßa c·∫ßu th·ªß, ƒë·ªôi b√≥ng ƒë·∫øn t·ª´ 5 gi·∫£i b√≥ng h√†ng ƒë·∫ßu Ch√¢u √Çu (Premier League, Laliga, Seria A, Budesliga, League 1) Ta s·∫Ω c√≥ schema nh∆∞ sau: SchemaSchema \" Schema trong ƒë√≥: games: b·∫£ng ch·ª©a th√¥ng tin th·ªëng k√™ c·ªßa t·ª´ng tr·∫≠n ƒë·∫•u (gameID) teams: B·∫£ng ch·ª©a t√™n c√°c ƒë·ªôi b√≥ng (teamID) players: B·∫£ng ch·ª©a t√™n c√°c c·∫ßu th·ªß (playerID) leagues: B·∫£ng ch∆∞a t√™n c√°c gi·∫£i ƒë·∫•u (leagueID) appearances: B·∫£ng th·ªëng k√™ c·ªßa c·∫ßu th·ªß ·ªü c√°c game m√† h·ªç tham gia (gameID, playerID) teamstats: B·∫£ng th·ªëng k√™ c·ªßa ƒë·ªôi b√≥ng ·ªü t·ª´ng game (gameID, teamID) ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#chu·∫©n-b·ªã-file-l√†m-raw-data"},{"categories":null,"content":"Load data v√†o MySQLC√≥ nhi·ªÅu c√°ch ƒë·ªÉ load data v√†o MySQL, ·ªü ƒë√¢y m√¨nh s·∫Ω s·ª≠ d·ª•ng c√°ch LOAD LOCAL_INFILE c·ªßa MySQL lu√¥n. Tip H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ make up l·∫ßn ƒë·∫ßu r·ªìi nh√© ! H√£y copy folder ch·ª©a c√°c file csv v√†o de_mysql container: docker cp /football de_mysql:/tmp/dataset/ docker cp /load_data de_mysql:/tmp/dataset/ Sau ƒë√≥ t·∫°o b·∫£ng tr·ªëng s·∫µn trong MySQL: make mysql_create #Create table in mysql Ti·∫øp t·ª•c v·ªõi l·ªánh: make to_mysql_root # ----- You will access to MySQL container SET GLOBAL LOCAL_INFILE=TRUE; #Set local_infile variable to load data from local exit; # ----- Exit container make mysql_load #load data make mysql_create_relation #create table relation Th·∫ø l√† ƒë√£ chu·∫©n b·ªã xong d·ªØ li·ªáu cho MySQL. ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load-data-v√†o-mysql"},{"categories":null,"content":"Init PostgreSQL SchemaTa c≈©ng c·∫ßn ph·∫£i t·∫°o s·∫µn schema s·∫µn trong Posgres nh∆∞ sau: make to_psql CREATE SCHEMA IF NOT EXISTS analysis; exit; Th·∫ø l√† ƒë√£ ho√†n t·∫•t vi·ªác chu·∫©n b·ªã data, gi·ªù th√¨ ta b·∫Øt ƒë·∫ßu v√†o ph·∫ßn vi·ªác ch√≠nh th√¥i ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#init-postgresql-schema"},{"categories":null,"content":"ImplementC√¥ng vi·ªác ch√≠nh trong ph·∫ßn n√†y l√† x√¢y d·ª±ng c√°c data pipeline b·∫±ng dagster. C∆° b·∫£n c√≥ th·ªÉ hi·ªÉu l√† ta t·∫°o c√°c Asset v√† chuy·ªÉn ch√∫ng t·ª´ database n√†y sang database kh√°c. ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#implement"},{"categories":null,"content":"Extractionƒê·ªÉ c√≥ th·ªÉ qu·∫£n l√Ω vi·ªác truy xu·∫•t d·ªØ li·ªáu t·ª´ MySQL v√† load v√†o MinIO ƒë·ªÉ l∆∞u t·∫°m, ta s·∫Ω x√¢y d·ª±ng m·ªôt I/O Manager ph·ª•c v·ª• vi·ªác ƒë√≥. ƒê·∫ßu ti√™n, h√£y v√†o ƒë∆∞·ªùng d·∫´n: ./etl_pipeline/etl_pipeline/resources/ Ta s·∫Ω x√¢y d·ª±ng MySQL io manager b·∫±ng c√°ch t·∫°o file mysql_io_manager.py v·ªõi n·ªôi dung sau: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_mysql(config): conn_info = ( f\"mysql+pymysql://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class MySQLIOManager(IOManager): def __init__(self, config): self.config = config def handle_output(self, context: OutputContext, obj: pd.DataFrame): pass def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def extract_data(self, sql: str) -\u003e pd.DataFrame: with connect_mysql(self.config) as db_conn: pd_data = pd.read_sql_query(sql, db_conn) return pd_data Sau ƒë√≥, ti·∫øp t·ª•c ƒë·ªëi v·ªõi minio_io_manager.py: import os from contextlib import contextmanager from datetime import datetime from typing import Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from dagster import IOManager, InputContext, OutputContext from minio import Minio @contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\"endpoint_url\"), access_key=config.get(\"aws_access_key_id\"), secret_key=config.get(\"aws_secret_access_key\"), secure=False ) try: yield client except Exception: raise class MinIOIOManager(IOManager): def __init__(self, config): self._config= config def _get_path(self, context: Union[InputContext, OutputContext]): layer, schema, table = context.asset_key.path key = \"/\".join([layer, schema, table.replace(f\"{layer}_\", \"\")]) tmp_file_path = \"/tmp/file-{}-{}.parquet\".format( datetime.today().strftime(\"%Y%m%d%H%M%S\"), \"-\".join(context.asset_key.path) ) if context.has_asset_partitions: start, end = context.asset_partitions_time_window dt_format = \"%Y%m%d%H%M%S\" partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format) return os.path.join(key, f\"{partition_str}.pq\"), tmp_file_path else: return f\"{key}.pq\", tmp_file_path def handle_output(self, context: OutputContext, obj: pd.DataFrame): # convert to parquet format key_name, tmp_file_path = self._get_path(context) table = pa.Table.from_pandas(obj) pq.write_table(table, tmp_file_path) # upload to MinIO try: bucket_name = self._config.get(\"bucket\") with connect_minio(self._config) as client: # Make bucket if not exist. found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exists\") client.fput_object(bucket_name, key_name, tmp_file_path) row_count = len(obj) context.add_output_metadata({\"path\": key_name, \"tmp\": tmp_file_path}) # clean up tmp file os.remove(tmp_file_path) except Exception: raise def load_input(self, context: InputContext) -\u003e pd.DataFrame: bucket_name = self._config.get(\"bucket\") key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: #Make bucket if not exist found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exist\") client.fget_object(bucket_name, key_name, tmp_file_path) pd_data = pd.read_parquet(tmp_file_path) return pd_data except Exception: raise Sau khi ƒë√£ t·∫°o th√†nh c√¥ng c√°c io manager cho mysql v√† minio, ta s·∫Ω b·∫Øt ƒë·∫ßu x√¢y d·ª±ng bronze layer Note nho nh·ªè Trong project n√†y m√¨nh chia c√°c giai ƒëo·∫°n transformation th√†nh c√°c layer: bronze layer: Giai ƒëo·∫°n ch·ªè m·ªõi load raw data, c√≥ th·ªÉ hi·ªÉu ƒë√¢y l√† data ch∆∞a transform g√¨ c·∫£ siler layer: Transform m·ªôt ph·∫ßn t·ª´ bronze layer, ·ªü ƒëo·∫°n n√†y data ƒë√£ ƒë∆∞·ª£c cleaning s∆° gold layer: Sau khi transform m·ªôt l·∫ßn n·ªØa t·ª´ silver layer, giai ƒëo·∫°n n√†y s·∫Ω truy v·∫•n ra c√°c th√¥n","date":"01 Aug 2023","objectID":"/football_etl_2/:2:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#extraction"},{"categories":null,"content":"TransformationTi·∫øp t·ª•c t·∫°o file silver_layer.py c√πng folder v·ªõi bronze layer: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teamstats\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"leagues\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], description='Statistic of teams in games', group_name=\"Silver_layer\", compute_kind=\"Pandas\" ) def silver_statsTeamOnGames(teamstats: pd.DataFrame, games: pd.DataFrame, leagues: pd.DataFrame) -\u003e Output[pd.DataFrame]: ts = teamstats.copy() gs = games.copy() lgs = leagues.copy() #Drop unsusable columns in games gs.drop(columns=gs.columns.to_list()[13:], inplace=True) #create StatperLeagueSeason result = pd.merge(ts, gs, on=\"gameID\") result = result.merge(lgs, on=\"leagueID\", how=\"left\") result.drop(columns=['season_y', 'date_y'],inplace=True) result = result.rename(columns={'season_x': 'season', 'date_x': 'date'}) return Output( result, metadata={ \"table\": \"statsTeamOnGames\", \"records\": len(result) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"appearances\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"players\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='statistic of players in games', compute_kind=\"Pandas\" ) def silver_playerAppearances(appearances: pd.DataFrame, games: pd.DataFrame, players: pd.DataFrame) -\u003e Output[pd.DataFrame]: app = appearances.copy() ga = games.copy() pla = players.copy() #Drop unusable column ga.drop(columns=ga.columns.to_list()[13:], inplace=True) #Merge player_appearances = pd.merge(app, pla, on=\"playerID\", how=\"left\") player_appearances = pd.merge(player_appearances, ga, on=\"gameID\", how=\"left\") #drop unecessary columns and rename player_appearances.drop(columns=['leagueID_y'],inplace=True) player_appearances.rename(columns={'leagueID_x': 'leagueID'}, inplace=True) return Output( player_appearances, metadata={ \"table\": \"playerAppearances\", \"records\": len(player_appearances) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teams\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='Teams', compute_kind=\"Pandas\" ) def silver_teams(teams: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( teams, metadata={ \"table\": 'teams', 'records': len(teams) } ) L√∫c n√†y m√¨nh c√≥ 3 silver assets, ƒë∆∞·ª£c join t·ª´ c√°c b·∫£ng ·ªü bronze Ti·∫øp ƒë·∫øn l√† gold_layer, l√∫c n√†y ta s·∫Ω t√≠nh c√°c th√¥ng s·ªë th·ªëng k√™ c·ªßa t·ª´ng gi·∫£i ƒë√¢u t·ª´ng m√πa, c√°c th·ªëng k√™ c·ªßa c·∫ßu th·ªß trong 90 ph√∫t thi ƒë·∫•u, v√† c·∫£ th·ªëng k√™ c·ªßa t·ª´ng c·∫ßu th·ªß trong t·ª´ng m√πa gi·∫£i gold_layer.py s·∫Ω c√≥ n·ªôi dung: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_statsTeamOnGames\": AssetIn( key_prefix=[\"football\", \"silver\"] ) }, group_name=\"Gold_layer\", key_prefix=[\"football\", \"gold\"], description='Statistic of all league in each season', compute_kind=\"Pandas\" ) def gold_statsPerLeagueSeason(silver_statsTeamOnGames: pd.DataFrame) -\u003e Output[pd.DataFrame]: st = silver_statsTeamOnGames.copy() result = ( st.groupby(['name', 'season']) .agg({\"goals\": \"sum\", \"xGoals\": \"sum\", \"shots\": \"sum\", \"shotsOnTarget\": \"sum\", \"fouls\": \"sum\", \"yellowCards\": \"sum\", \"redCards\": \"sum\",'corners': 'sum', \"gameID\": 'count'}) .reset_index() ) result = result.rename(columns={'gameID':\"games\"}) result['goalPerGame']= result.goals/result.games result['season'] = result['season'].astype('string') return Output( result, metadata={ 'table': 'statPerLeagueSeason', 'records': len(result) } ) @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_playerAppearances\": AssetIn( key_prefix=[\"football\", \"silver\"] ) },","date":"01 Aug 2023","objectID":"/football_etl_2/:2:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#transformation"},{"categories":null,"content":"LoadTr∆∞·ªõc h·∫øt h√£y t·∫°o IO Manager cho Postgres ƒë·ªÉ qu·∫£n l√Ω vi·ªác load cleaned data. Ta t·∫°o file psql_io_manager.py ·ªü v·ªã tr√≠ m√† ta ƒë√£ t·∫°o 2 io manager tr∆∞·ªõc v·ªõi n·ªôi dung: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_psql(config): conn_info = ( f\"postgresql+psycopg2://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class PostgreSQLIOManager(IOManager): def __init__(self, config): self._config = config def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def handle_output(self, context: OutputContext, obj: pd.DataFrame): schema, table = context.asset_key.path[-2], context.asset_key.path[-1] with connect_psql(self._config) as conn: # insert new data ls_columns = (context.metadata or {}).get(\"columns\", []) obj[ls_columns].to_sql( name=f\"{table}\", con=conn, schema=schema, if_exists=\"replace\", index=False, chunksize=10000, method=\"multi\" ) Sau ƒë√≥, t·∫°o m·ªôt asset warehouse_layer.py: from dagster import multi_asset, Output, AssetIn, AssetOut, asset import pandas as pd @multi_asset( ins={ \"gold_statsPerLeagueSeason\": AssetIn( key_prefix=[\"football\", \"gold\"] ) }, outs={ \"statsperleagueseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerLeagueSeason\", 'analysis'], metadata={ \"columns\": [ \"name\", \"season\", \"goals\", \"xGoals\", \"shots\", \"shotsOnTarget\", \"fouls\", \"yellowCards\", \"redCards\", \"corners\", \"games\", \"goalPerGame\" ] } ), }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerLeagueSeason(gold_statsPerLeagueSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerLeagueSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerLeagueSeason\", \"records\": len(gold_statsPerLeagueSeason) } ) @multi_asset( ins={ \"gold_statsPerPlayerSeason\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsperplayerseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerPlayerSeason\", 'analysis'], metadata={ \"columns\": [ \"playerID\", \"name\", \"season\", \"goals\", \"shots\", \"xGoals\", \"xGoalsChain\", \"xGoalsBuildup\", \"assists\", \"keyPasses\", \"xAssists\", \"gDiff\", \"gDiffRatio\" ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerPlayerSeason(gold_statsPerPlayerSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerPlayerSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerPlayerSeason\", \"records\": len(gold_statsPerPlayerSeason) } ) @multi_asset( ins={ \"gold_statsPlayerPer90\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsplayerper90\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPlayerPer90\", 'analysis'], metadata={ \"columns\": [ 'playerID', 'name', 'total_goals', 'total_assists', 'total_time', 'goalsPer90', 'assistsPer90', 'scorers' ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPlayerPer90(gold_statsPlayerPer90: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPlayerPer90, metadata={ \"schema\": \"analysis\", \"table\": \"statsPlayerPer90\", \"records\": len(gold_statsPlayerPer90) } ) ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load"},{"categories":null,"content":"Run systemCu·ªëi c√πng, ta s·∫Ω k·∫øt h·ª£p t·∫•t c·∫£ c√°c asset l·∫°i gi√∫p dagster nh·∫≠n di·ªán v√† qu·∫£n l√Ω v·ªõi file __init__.py ·ªü etl_pipeline/etl_pipeline/__init__.py import os from dagster import Definitions from .assets.silver_layer import * from .assets.gold_layer import * from .assets.bronze_layer import * from .assets.warehouse_layer import * from .resources.minio_io_manager import MinIOIOManager from .resources.mysql_io_manager import MySQLIOManager from .resources.psql_io_manager import PostgreSQLIOManager MYSQL_CONFIG = { \"host\": os.getenv(\"MYSQL_HOST\"), \"port\": os.getenv(\"MYSQL_PORT\"), \"database\": os.getenv(\"MYSQL_DATABASE\"), \"user\": os.getenv(\"MYSQL_USER\"), \"password\": os.getenv(\"MYSQL_PASSWORD\") } MINIO_CONFIG = { \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\"), \"bucket\": os.getenv(\"DATALAKE_BUCKET\"), \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"), \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\") } PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } ls_asset=[asset_factory(table) for table in tables] + [silver_statsTeamOnGames, silver_teams , silver_playerAppearances, gold_statsPerLeagueSeason, gold_statsPerPlayerSeason, gold_statsPlayerPer90, statsPerLeagueSeason, statsPerPlayerSeason, statsPlayerPer90] defs = Definitions( assets=ls_asset, resources={ \"mysql_io_manager\": MySQLIOManager(MYSQL_CONFIG), \"minio_io_manager\": MinIOIOManager(MINIO_CONFIG), \"psql_io_manager\": PostgreSQLIOManager(PSQL_CONFIG), } ) sau ƒë√≥ h√£y d√πng l·ªánh sau ƒë·ªÉ c·∫≠p nh·∫≠t c√°c assets docker restart etl_pipeline ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:4","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#run-system"},{"categories":null,"content":"Check UIH√£y ki·ªÉm tra Dagit UI ·ªü localhost:3001 ƒë·ªÉ ch·∫Øc ch·∫Øn r·∫±ng m·ªçi th·ª© v·∫´n ·ªïn Ngo√†i ra c≈©ng c√≥ th·ªÉ check MinIO: localhost:9000 ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:5","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#check-ui"},{"categories":null,"content":"VisualizationCu·ªëi c√πng l√† v·∫Ω dashboard, ƒë·∫ßu ti√™n ta c·∫ßn ph·∫£i l·∫•y ƒë∆∞·ª£c data t·ª´ psql, h√£y v√†o t·∫°o file: ./streamlit/src/psql_connect.py: import os import psycopg2 from dotenv import load_dotenv import pandas as pd #load environment load_dotenv() #list table in database table = ['statsperleagueseason','statsperplayerseason', 'statsplayerper90'] PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } #create connection def init_connection(config): return psycopg2.connect( database=config['database'], user=config['user'], password=config['password'], host=config['host'], port=config['port'] ) def extract_data(): conn = init_connection(PSQL_CONFIG) return [pd.read_sql(f'SELECT * FROM analysis.{tab}', conn) for tab in table] Cu·ªëi c√πng l√† t·∫°o main.py ngay trong th∆∞ m·ª•c scr: import streamlit as st import pandas as pd import plotly.express as px import plotly.graph_objects as go from psql_connect import extract_data import numpy as np # #extract data from PostgreSQL ls_df = extract_data() l_season = ls_df[0] p_season = ls_df[1] p_match = ls_df[2] st.set_page_config(page_title = 'Dashboard Football', layout='wide', page_icon='chart_with_upwards_trend') #Overview def overview(table: pd.DataFrame, detail: str): if (st.checkbox('Do you want to see Data ?')): table col1, col2 = st.columns(2) co_df = table.columns.to_list() with col1: st.bar_chart(table.describe()) if (st.checkbox('Do you want to see describe each column ?')): for col in co_df: if table[col].dtypes not in ['int64', 'float64']: continue st.bar_chart(table[col].describe()) with col2: st.caption(f':red[Columns]: {len(co_df)}') st.caption(f':red[Records]: {len(table)}') st.caption(f':red[Description]: {detail}') st.caption(f':red[Columns name]:{co_df}') #league statistic def statleague(): Cards = l_season[['name','season','yellowCards', 'redCards', 'fouls']] #Card_fouls col1, col2 = st.columns(2) with col1: #Goals per games fig = px.bar(l_season, x=\"name\", y=\"goalPerGame\", color=\"name\", barmode=\"stack\", facet_col=\"season\", labels={\"name\": \"League\", \"goals/games\": \"GPG\"}) fig.update_layout(showlegend=False, title='Goals per Game') st.plotly_chart(fig) #fouls fig = px.line(Cards, x='season', y='fouls', color='name') fig.update_layout(title='Fouls of leagues', xaxis_title='Season', yaxis_title='Fouls', legend_title='League') st.plotly_chart(fig) with col2: #Red card fig = px.line(Cards, x='season', y='redCards', color='name') fig.update_layout(title='Red Cards of leagues', xaxis_title='Season', yaxis_title='RedCards', legend_title='League') st.plotly_chart(fig) #yellow card fig = px.line(Cards, x='season', y='yellowCards', color='name') fig.update_layout(title='Yellow Cards of leagues', xaxis_title='Season', yaxis_title='YellowCards', legend_title='League') st.plotly_chart(fig) #Player statistic def statplayer(): col1, col2 = st.columns(2) with col1: #Best offensive player top_player90= p_match[(p_match['goalsPer90'] \u003e 0.8) | (p_match['assistsPer90'] \u003e 0.4)] fig = px.scatter(p_match[['name','goalsPer90', 'assistsPer90']], x='goalsPer90', y='assistsPer90', hover_name='name') fig.add_trace( go.Scatter(x=top_player90['goalsPer90'], y=top_player90['assistsPer90'], mode='markers+text', marker_size=5, text=top_player90['name'], textposition='bottom center', textfont=dict(size=15)) ) fig.update_layout(title='Best offensive Players (2018-2020)', xaxis_title='Goals Per 90min', yaxis_title='Assists Per 90min') st.plotly_chart(fig) #goals-xgoal fig = px.scatter(p_season, x=\"xGoals\", y=\"goals\", color=(p_season['xGoals'] - p_season['goals'] \u003c 10), color_discrete_sequence=[\"red\", \"green\"], opacity=0.5) fig.update_layout(title=\"Goals (G) and Expected Goals (xG)\", xaxis_title=\"xG\", yaxis_title=\"G\", ) st.plotly_chart(fig) with col2: #Top score player topPlayer = p_season.groupby(['name']).agg({'goals': 'sum'}).sort_values('goals', ascending=False).res","date":"01 Aug 2023","objectID":"/football_etl_2/:3:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#visualization"},{"categories":null,"content":"ConclusionCu·ªëi c√πng c≈©ng ƒë√£ xong m·ªôt project x√¢y d·ª±ng data pipeline c∆° b·∫£n, trong l√∫c th·ª±c hi·ªán ch·∫Øc ch·∫Øn s·∫Ω c√≥ c·∫£ t·∫•n l·ªói x·∫£y ra, nh∆∞ng OG tin l√† m·ªçi gian kh√≥ ƒë·ªÅu s·∫Ω v∆∞·ª£t quan ƒë∆∞·ª£c, th·ª© ƒë·ªçng l·∫°i ch√≠nh l√† ki·∫øn th·ª©c v√† k·ªπ nƒÉng c·ªßa ch√∫ng ta. Ch√∫c b·∫°n th√†nh c√¥ng v√† ƒë√≥n xem ti·∫øp c√°c d·ª± √°n ti·∫øp theo c·ªßa OG nh√© ! -Mew- ","date":"01 Aug 2023","objectID":"/football_etl_2/:4:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#conclusion"},{"categories":null,"content":"Related Content Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#related-content"},{"categories":["projects"],"content":"A Data Engineer project building pipeline to analyze football data","date":"31 Jul 2023","objectID":"/football_etl/","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/"},{"categories":["projects"],"content":"Source code: Football_ETL_Analysis ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#"},{"categories":["projects"],"content":"IntroduceTrong project n√†y, OG s·∫Ω build end-to-end ETL data pipeline ho√†n ch·ªânh ƒë·ªÉ ph√¢n t√≠ch football data t·ª´ Kaggle v·ªõi data pipeline nh∆∞ sau: Data pipelinedata pipeline \" Data pipeline C√°c b∆∞·ªõc c·ª• th·ªÉ: Set up: D√πng Docker t·∫°o container v√† c√°c images c·∫ßn thi·∫øt cho pipeline, trong ƒë√≥ c√≥ c·∫£ Dagster d√πng x√¢y d·ª±ng pipeline. Chu·∫©n b·ªã data source: load c√°c file csv (c√≥ ƒë∆∞·ª£c t·ª´ Kaggle) v√†o MySQL nh·∫±m m·ª•c ƒë√≠ch l∆∞u tr·ªØ raw data (m√¥ ph·ªèng source data) Extract: L·∫•y data t·ª´ MySQL v√† load v√†o MinIO chu·∫©n b·ªã cho b∆∞·ªõc transform Transform: S·ª≠ d·ª•ng Pandas ƒë·ªÉ truy v·∫•n c√°c file t·ª´ MinIO Load: Cleaned v√† transformed data ƒë∆∞·ª£c load v√†o warehouse PostgreSQL Visualization: S·ª≠ d·ª•ng Streamlit ƒë·ªÉ l√†m Dashboard ","date":"31 Jul 2023","objectID":"/football_etl/:1:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#introduce"},{"categories":["projects"],"content":"Set upB·∫Øt ƒë·∫ßu v·ªõi Docker, ta s·∫Ω x√¢y d·ª±ng l·∫ßn l∆∞·ª£t t·ª´ng image b·∫±ng c√°ch vi·∫øt docker-compose.yml Tip nho nh·ªè H√£y pull/build l·∫ßn l∆∞·ª£t t·ª´ng lo·∫°i framework l·∫ßn l∆∞·ª£t ƒë·ªÉ ch·∫Øc ch·∫Øn r·∫±ng ch√∫ng ho·∫°t ƒë·ªông tr∆°n tru nh·∫•t tr∆∞·ªõc Ho·∫∑c b·∫°n c≈©ng c√≥ th·ªÉ xem lu√¥n ph·∫ßn ho√†n ch·ªânh Ho√†n ch·ªânh set up ","date":"31 Jul 2023","objectID":"/football_etl/:2:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#set-up"},{"categories":["projects"],"content":"MinIOMinIO l√† m·ªôt server l∆∞u tr·ªØ ƒë·ªëi t∆∞·ª£ng d·∫°ng ph√¢n t√°n v·ªõi hi·ªáu nƒÉng cao v√† cung c·∫•p c√°c api gi·ªëng v·ªõi Amazon S3, ta c√≥ th·ªÉ upload, download file,‚Ä¶ m·ªôt c√°ch ƒë∆°n gi·∫£n. minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_network Note V·ªÅ .env file, ƒë√¢y l√† file ch·ª©a th√¥ng tin c√°c bi·∫øn m√¥i tr∆∞·ªùng thi·∫øt l·∫≠p cho t·ª´ng image, m√¨nh s·∫Ω n√≥i sau ","date":"31 Jul 2023","objectID":"/football_etl/:2:1","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#minio"},{"categories":["projects"],"content":"MySQLMySQL l√† m·ªôt trong s·ªë c√°c ph·∫ßn m·ªÅm RDBMS (Relational DataBase Management Systems) ph·ªï bi·∫øn nh·∫•t, ta s·∫Ω s·ª≠ d·ª•ng database n√†y ƒë·ªÉ l∆∞u raw data m√¥ ph·ªèng cho source data c·∫ßn ingest de_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:2","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#mysql"},{"categories":["projects"],"content":"PostgeSQLPostgreSQL l√† m·ªôt h·ªá th·ªëng qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu quan h·ªá-ƒë·ªëi t∆∞·ª£ng (object-relational database management system), v√† ta s·∫Ω dung n√≥ l√†m data warehouse cho project l·∫ßn n√†y. de_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:3","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#postgesql"},{"categories":["projects"],"content":"DagsterDagster l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü h·ªó tr·ª£ Orchestrate Task (qu·∫£n l√Ω, t·ªï ch·ª©c, ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• v√† c√¥ng vi·ªác), h·ªó tr·ª£ gi√∫p x√¢y d·ª±ng data pipeline kh√° t·ªët. M√¨nh th·∫•y c√¥ng c·ª• n√†y g·ªçn nh·∫π h∆°n Apache Airflow v√† c≈©ng ƒë∆°n gi·∫£n h∆°n. ƒê·∫ßu ti√™n h√£y vi·∫øt trong docker-compose.yml v·ªÅ dagster nh∆∞ sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster ƒê·ªëi v·ªõi Dagster, ta s·∫Ω t·ª± config b·∫±ng Dockerfile, h√£y t·∫°o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau ƒë√≥ t·∫°o Dockerfile c√≥ n·ªôi dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt v√† requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit l√† giao di·ªán UI c·ªßa Dagster, v√† ta s·∫Ω thao t√°c tr√™n Dagit ƒë·ªÉ qu·∫£n l√Ω c√°c assets, jobs,‚Ä¶ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau ƒë√≥ h√£y t·∫°o 1 folder ./dagster_home, trong ƒë√≥ t·∫°o 2 file config cho workspace c·ªßa dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage v√† workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster Deamonƒê·ªÉ qu·∫£n l√Ω c√°c Schedules, sensors,‚Ä¶ ta c·∫ßn ph·∫£i c√≥ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster"},{"categories":["projects"],"content":"DagsterDagster l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü h·ªó tr·ª£ Orchestrate Task (qu·∫£n l√Ω, t·ªï ch·ª©c, ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• v√† c√¥ng vi·ªác), h·ªó tr·ª£ gi√∫p x√¢y d·ª±ng data pipeline kh√° t·ªët. M√¨nh th·∫•y c√¥ng c·ª• n√†y g·ªçn nh·∫π h∆°n Apache Airflow v√† c≈©ng ƒë∆°n gi·∫£n h∆°n. ƒê·∫ßu ti√™n h√£y vi·∫øt trong docker-compose.yml v·ªÅ dagster nh∆∞ sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster ƒê·ªëi v·ªõi Dagster, ta s·∫Ω t·ª± config b·∫±ng Dockerfile, h√£y t·∫°o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau ƒë√≥ t·∫°o Dockerfile c√≥ n·ªôi dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt v√† requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit l√† giao di·ªán UI c·ªßa Dagster, v√† ta s·∫Ω thao t√°c tr√™n Dagit ƒë·ªÉ qu·∫£n l√Ω c√°c assets, jobs,‚Ä¶ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau ƒë√≥ h√£y t·∫°o 1 folder ./dagster_home, trong ƒë√≥ t·∫°o 2 file config cho workspace c·ªßa dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage v√† workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster Deamonƒê·ªÉ qu·∫£n l√Ω c√°c Schedules, sensors,‚Ä¶ ta c·∫ßn ph·∫£i c√≥ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagit"},{"categories":["projects"],"content":"DagsterDagster l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü h·ªó tr·ª£ Orchestrate Task (qu·∫£n l√Ω, t·ªï ch·ª©c, ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• v√† c√¥ng vi·ªác), h·ªó tr·ª£ gi√∫p x√¢y d·ª±ng data pipeline kh√° t·ªët. M√¨nh th·∫•y c√¥ng c·ª• n√†y g·ªçn nh·∫π h∆°n Apache Airflow v√† c≈©ng ƒë∆°n gi·∫£n h∆°n. ƒê·∫ßu ti√™n h√£y vi·∫øt trong docker-compose.yml v·ªÅ dagster nh∆∞ sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster ƒê·ªëi v·ªõi Dagster, ta s·∫Ω t·ª± config b·∫±ng Dockerfile, h√£y t·∫°o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau ƒë√≥ t·∫°o Dockerfile c√≥ n·ªôi dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt v√† requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit l√† giao di·ªán UI c·ªßa Dagster, v√† ta s·∫Ω thao t√°c tr√™n Dagit ƒë·ªÉ qu·∫£n l√Ω c√°c assets, jobs,‚Ä¶ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau ƒë√≥ h√£y t·∫°o 1 folder ./dagster_home, trong ƒë√≥ t·∫°o 2 file config cho workspace c·ªßa dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage v√† workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster Deamonƒê·ªÉ qu·∫£n l√Ω c√°c Schedules, sensors,‚Ä¶ ta c·∫ßn ph·∫£i c√≥ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster-deamon"},{"categories":["projects"],"content":"PipelineT·∫•t c·∫£ m·ªçi vi·ªác x√¢y d·ª±ng pipeline ta s·∫Ω ho·∫°t ƒë·ªông ·ªü ƒë√¢y ƒê·∫ßu ti√™n ta c·∫ßn init m·ªôt dagster project dagster project scaffold --name etl_pipeline v√† th∆∞ m·ª•c m·ªõi t·∫°o s·∫Ω tr√¥ng nh∆∞ th·∫ø n√†y: Tip ƒê·ªÉ ch·∫°y ƒë∆∞·ª£c l·ªánh dagster ·ªü b∆∞·ªõc t·∫°o dagster project, ta c·∫ßn ph·∫£i c√≥ dagster package, n·∫øu ch∆∞a c√≥ h√£y c√†i ƒë·∫∑t b·∫±ng: pip install dagster ‚Äì\u003e N√™n c√†i ƒë·∫∑t trong m√¥i tr∆∞·ªùng ·∫£o Sau ƒë√≥ v√†o th∆∞ m·ª•c v·ª´a t·∫°o v√†o vi·∫øt Dockerfile th√¥i: FROMpython:3.10-slim# Add repository codeWORKDIR/opt/dagster/appCOPY requirements.txt /opt/dagster/appRUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txtWORKDIR/opt/dagster/appCOPY . /opt/dagster/app/etl_pipeline# CMD allows this to be overridden from run launchers or executors that want to run other commands against your repositoryCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"etl_pipeline\"] c√πng v·ªõi requirements.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-aws==0.17.20 dagster-dbt==0.17.20 pandas==1.5.3 SQLAlchemy==1.4.46 pymysql==1.0.2 cryptography==39.0.0 pyarrow==10.0.1 boto3==1.26.57 fsspec==2023.1.0 s3fs==0.4.2 minio==7.1.13 Cu·ªëi c√πng l√† vi·∫øt v√†o docker-compose.yml: etl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:5","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#pipeline"},{"categories":["projects"],"content":"StreamlitCu·ªëi c√πng l√† vi·ªác l√† Dashboard, Streamlit ch·∫Øc ch·∫Øc l√† c√¥ng c·ª• si√™u ph√π h·ª£p l√†m vi·ªác n√†y. ƒê√¢y l√† framework h·ªó tr·ª£ vi·ªác x√¢y d·ª±ng giao di·ªán ∆∞u nh√¨n ch·ªâ b·∫±ng Python, qu√° ƒë√£ ph·∫£i kh√¥ng n√†o :)) H√£y t·∫°o folder ./streamlit/scr/ c√πng v·ªõi ./streamlit/Dockerfile: FROMpython:3.10EXPOSE8501WORKDIR/usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . . v√† streamlit/requirements.txt: pandas plotly matplotlib numpy streamlit psycopg2-binary sqlalchemy python-dotenv Cu·ªëi c√πng l√† ghi trong yml streamlit:build:./streamymlcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:6","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#streamlit"},{"categories":["projects"],"content":"Ho√†n ch·ªânh setupCu·ªëi c√πng, file yaml s·∫Ω tr√¥ng nh∆∞ th·∫ø n√†y: # version: '3.9'services:minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_networkde_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_networkde_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_networkstreamlit:build:./streamlitcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_networkde_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagsterde_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_networkde_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network# Pipelinesetl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_networknetworks:de_network:driver:bridgename:de_network V√† .env file: # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_DB=football POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_HOST_AUTH_METHOD=trust # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=football # MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=football MYSQL_ROOT_PASSWORD=admin123 MYSQL_USER=admin MYSQL_PASSWORD=admin123 # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=warehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 Warning N·∫øu b·∫°n ch·ªâ ƒë·ªçc ph·∫ßn Ho√†n ch·ªânh setup th√¨ c√≥ th·ªÉ h·ªá th·ªëng v·ªÖ s·∫Ω g·∫∑p l·ªói v√¨ c√≥ th·ªÉ thi·∫øu c√°c configuration c·∫ßn thi·∫øt cho dagster, dagit hay pipeline. B·∫°n c·∫ßn ƒë·ªçc qua ph·∫ßn Dagster, Dagit, Pipeline Ch·∫°y th·ª≠: sau khi ho√†n t·∫•t to√†n b·ªô set up d√†i ngo·∫±n, c≈©ng ƒë√£ ƒë·∫øn l√∫c ch·∫°y ch∆∞∆°ng tr√¨nh th√¥i. Note nho nh·ªè N·∫øu b·∫°n ƒë√£ build l·∫ßn l∆∞·ª£t c√°c images r·ªìi, th√¨ ch·ªâ c·∫ßn compose up th√¥i, n·∫øu kh√¥ng, h√£y build b·∫±ng l·ªánh docker compose build tr∆∞·ªõc khi ch·∫°y compose up. docker compose --env-file .env up -d L·∫°i l√† m·ªôt tip c√≥ th·ªÉ h·ªØu √≠ch ƒê·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác ghi l·ªánh d√†i d√≤ng, h√£y t·∫°o m·ªôt make file t√™n Makefile v·ªõi n·ªôi dung sau: include .env build: docker compose build up: docker compose --env-file .env up -d down: docker compose --env-file .env down restart: make down \u0026\u0026 make up to_psql: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} psql_create: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} -f /tmp/psql_schema.sql to_mysql: docker exec -it de_mysql mysql --local-infile=1 -u\"${MYSQL_U","date":"31 Jul 2023","objectID":"/football_etl/:2:7","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#ho√†n-ch·ªânh-setup"},{"categories":["projects"],"content":"To be ContinueB√†i ƒë·∫øn ƒë√¢y c≈©ng qu√° l√† d√†i r·ªìi, m√¨nh s·∫Ω vi·∫øt ti·∫øp ·ªü ph·∫ßn 2 :))) Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh -Mew- ","date":"31 Jul 2023","objectID":"/football_etl/:3:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#to-be-continue"},{"categories":["projects"],"content":"Related Content Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... Football ETL Analysis P2 Continuous of Football ETL series Read more... ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#related-content"},{"categories":[],"content":"Data Engineering Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#data-engineering"},{"categories":[],"content":"Machine learning Basic Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#machine-learning-basic"},{"categories":null,"content":" Hello! M√¨nh t√™n l√† Vƒ©nh Phong hay ch√≠nh l√† OG (t√°c gi·∫£ c·ªßa c√°c blogs ·ªü trang n√†y) Hi·ªán t·∫°i m√¨nh l√† sinh vi√™n ng√†nh Khoa h·ªçc D·ªØ li·ªáu (Data Science) t·∫°i ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n, ƒêHQG-HCM (HCMUS). Tuy nhi√™n m√¨nh c≈©ng y√™u th√≠ch c√¥ng ngh·ªá, code v√† m√¨nh ƒëang tr√™n con ƒë∆∞·ªùng h·ªçc t·∫≠p m·ªói ng√†y ƒë·ªÉ tr·ªü th√†nh m·ªôt Data Engineer. OG h·ªìi cu·ªëi l·ªõp 12OG h·ªìi c√∫i l·ªõp 12 :)) \" OG h·ªìi cu·ªëi l·ªõp 12 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#"},{"categories":null,"content":"M√¨nh c·ªßa hi·ªán t·∫°iƒê·∫ßu ti√™n quan tr·ªçng nh·∫•t ch√≠nh l√† vi·ªác h·ªçc t·∫°i HCMUS. Ngo√†i ra, m√¨nh c√≤n t·ª± h·ªçc v·ªÅ c√°c ch·ªß ƒë·ªÅ li√™n quan nh∆∞ Data pipeline, Data Streaming,‚Ä¶ Vi·ªác luy·ªán t·∫≠p, h·ªçc h·ªèi c≈©ng ƒÉn s√¢u d√¥ m√°u m√¨nh l√∫c n√†o kh√¥ng hay :)) M√¨nh ƒë√£ t·ª´ng ƒë·ªçc th·∫•y ƒë√¢u ƒë√≥ r·∫±ng: The most beautiful thing about learning is that no one take that away from you V√† ƒëi·ªÅu ƒë√≥ ƒë√£ truy·ªÅn c·∫£m h·ª©ng m√¨nh r·∫•t nhi·ªÅu, th√∫c ƒë·∫©y b·∫£n th√¢n t·ª± h·ªçc m·ªói ng√†y v√† t·ª± l√†m m·ªõi b·∫£n th√¢n t·ª´ng ch√∫t m·ªôt. Ti·∫øp ƒë·∫øn ch√≠nh l√† x√¢y d·ª±ng trang web n√†y v√† vi·∫øt c√°c b√†i blogs gi√∫p cho c√°c b·∫°n c√≥ th·ªÉ h·ªçc th√™m ki·∫øn th·ª©c ng√†nh, bi·∫øt th√™m ƒëi·ªÅu th√∫ v·ªã v√† th∆∞ gi√£n. Hi·ªán t·∫°i th√¨ OG v·∫´n c√≤n ng·ªìi tr√™n gh·∫ø gi·∫£ng ƒë∆∞·ªùng, v√† v·ª´a ƒë·∫∑t nh·ªØng vi√™n g·∫°ch ƒë·∫ßu ti√™n tr√™n con ƒë∆∞·ªùng t·ª± tr∆∞·ªüng th√†nh. H√£y lu√¥n theo d√µi OG nh√©! -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Hu·ª≥nh L∆∞u Vƒ©nh Phong Facebook: Phong Huynh Instagram: phong_huynh Ho·∫∑c b·∫°n c≈©ng c√≥ th·ªÉ gh√© thƒÉm Github c·ªßa m√¨nh: PhongHuynh0394 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#m√¨nh-c·ªßa-hi·ªán-t·∫°i"}]