[{"categories":[],"content":" Th·∫±ng nh√≥c th√≠ch code v√† data Ng√†nh Data c√≥ g√¨ hot m√† m√¨nh l·∫°i d√≠nh Read more... ","date":"24 Sep 2023","objectID":"/blogs/:0:0","series":[],"tags":[],"title":"Blogs","uri":"/blogs/#"},{"categories":["projects"],"content":"Stock Analysis using PCA and K-means","date":"31 Aug 2023","objectID":"/stock_analysis/","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/"},{"categories":["projects"],"content":"Warning ƒê√¢y l√† ki·∫øn th·ª©c t√≠ch g√≥p t·ª´ nhi·ªÅu ngu·ªìn v√† nghi√™n c·ª©u c·ªßa nh√≥m OG, t·∫•t nhi√™n kh√¥ng th·ªÉ tr√°nh kh·ªèi sai s√≥t. Hy v·ªçng b√†i vi·∫øt l·∫ßn n√†y th√∫ v·ªã v√† gi√∫p b·∫°n ƒë·ªçc th∆∞ gi√£n, tham kh·∫£o. Source github: Stock Analysis Hello! OG ƒë√¢y. ·ªû project l·∫ßn n√†y m√¨nh s·∫Ω ph√¢n t√≠ch gia tr·ªã c·ªï phi·∫øu ph√°i sinh VN30 Index b·∫±ng c√°ch s·ª≠ d·ª•ng PCA v√† K-means. Xin v√¥ c√πng c·∫£m ∆°n s·ª± ƒë√≥ng g√≥p c·ªßa 5 th√†nh vi√™n team OG v√† th·∫ßy Minh M·∫´n v√† th·∫ßy Ho√†ng ƒê·ª©c ƒë√£ t·∫≠n t√¨nh h∆∞·ªõng d·∫´n ƒë·ªÉ team c√≥ th·ªÉ ho√†n th√†nh ƒë·ªì √°n m·ªôt c√°ch t·ªët nh·∫•t. R·ªìi b√¢y gi·ªù g√©t g√¥ thooiii üòÑ ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#"},{"categories":["projects"],"content":"IntroStock Analysis hay c√≤n g·ªçi l√† Market Analysis ƒë·ªÅ c·∫≠p ƒë·∫øn ph∆∞∆°ng ph√°p m√† nh√† ƒë·∫ßu t∆∞ ho·∫∑c nh√† giao d·ªãch s·ª≠ d·ª•ng ƒë·ªÉ ƒë√°nh gi√° v√† ƒëi·ªÅu tra m·ªôt c√¥ng c·ª• giao d·ªãch c·ª• th·ªÉ, lƒ©nh v·ª±c ƒë·∫ßu t∆∞ ho·∫∑c to√†n b·ªô th·ªã tr∆∞·ªùng ch·ª©ng kho√°n. Kh√¥ng nh·ªØng th·∫ø, n√≥ li√™n quan ƒë·∫øn vi·ªác nghi√™n c·ª©u d·ªØ li·ªáu th·ªã tr∆∞·ªùng trong qu√° kh·ª© v√† hi·ªán t·∫°i v√† t·∫°o ra m·ªôt ph∆∞∆°ng ph√°p ƒë·ªÉ ch·ªçn c·ªï phi·∫øu ph√π h·ª£p ƒë·ªÉ giao d·ªãch. C√°c nh√† ƒë·∫ßu t∆∞ s·∫Ω ƒë∆∞a ra quy·∫øt ƒë·ªãnh mua ho·∫∑c b√°n d·ª±a tr√™n th√¥ng tin ph√¢n t√≠ch ch·ª©ng kho√°n. Trong project n√†y ta s·∫Ω ph√¢n t√≠ch, tr·ª±c quan h√≥a b·ªô d·ªØ li·ªáu gi·∫£ ƒë·ªãnh ƒë∆∞·ª£c cung c·∫•p b·ªüi kh√°ch h√†ng ƒë·ªÉ ƒë√°nh gi√° th·ªã tr∆∞·ªùng ch·ª©ng kho√°n trong kho·∫£ng th·ªùi gian 1 th√°ng c·ªßa 30 c√¥ng ty thu·ªôc VN30 D∆∞·ªõi ƒë√¢y l√† t√≥m t·∫Øt s∆° l∆∞·ª£c t·ª´ng b∆∞·ªõc ƒë·ªÉ x·ª≠ l√Ω v√† ph√¢n t√≠ch: EDA (Exploratory Data Analysis) Data Preprocessing PCA (Principle Component Analysis) K-Means Clustering Data Analysis References Raw Data Source: df_merged.pkl Raw data l√† d·ªØ li·ªáu b·∫£ng gi√° c·ªï phi·∫øu c·ªßa 30 c√¥ng ty thu·ªôc VN30 Index + 1 tr∆∞·ªùng gi√° ph√°i sinh trong 1 th√°ng ","date":"31 Aug 2023","objectID":"/stock_analysis/:1:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#intro"},{"categories":["projects"],"content":"Exploratory Data Analysisƒê√¢y l√† b∆∞·ªõc ƒë·∫ßu ti√™n, ch√∫ng ta s·∫Ω c√πng nhau t√¨m hi·ªÉu s∆° l∆∞·ª£c raw data c≈©ng nh∆∞ t√¨m hi·ªÉu c√°i nh√¨n t·ªïng qu√°t v·ªÅ d·ªØ li·ªáu ta s·∫Øp ph·∫£i ph√¢n t√≠ch ƒë·ªÉ t·ª´ ƒë√≥ c√≥ c√°ch ti·ªÅn x·ª≠ l√Ω ph√π h·ª£p. L√†m g√¨ th√¨ l√†m c·ª© ph·∫£i import packages ƒë·ªÉ ƒë·ªçc data c√°i ƒë√£ ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#exploratory-data-analysis"},{"categories":["projects"],"content":"Data AcquistionTa s·∫Ω import m·ªôt s·ªë packages quen thu·ªôc ƒë·ªÉ ƒë·ªçc file df_merged.pkl import pickle import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd.read_pickle('https://github.com/PhongHuynh0394/My-respository/blob/main/df_merged.pkl?raw=true') # Check the data type type(data) # --\u003e list Data nh·∫≠n ƒë∆∞·ª£c t·ª´ pickle file l√† m·ªôt list, b√¢y gi·ªù ta s·∫Ω t√¨m ki·∫øm c√°i nh√¨n t·ªïng quan v·ªÅ d·ªØ li·ªáu n√†y ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-acquistion"},{"categories":["projects"],"content":"A Brief View D·ªØ li·ªáu l∆∞u ·ªü pickle l√† m·ªôt list ch·ª©a 23 dataframe (df) M·ªói df c√≥ index theo datetime (nghƒ©a l√† ƒë√¢y l√† lo·∫°i d·ªØ li·ªáu thu·ªôc timeseries) C√°c columns l·∫ßn l∆∞·ª£t l√† t·ª´ng m√£ c·ªï phi·∫øu, ch·ª©a kh·ªëi l∆∞·ª£ng/ gi√° c·ªßa c√°c l·ªánh mua/b√°n s√°t v·ªõi l·ªánh kh·ªõp I v√† kh·ªëi l∆∞·ª£ng c·ªßa c√°c l·ªánh mua/b√°n s√°t v·ªõi gi√° kh·ªõp l·ªánh II print('So luong df:', len(data)) # --\u003e So luong df: 23 Raw data l√† gi√° l·ªánh mua/b√°n I II v√† kh·ªëi l∆∞·ª£ng giao d·ªãch c·ªßa c·ªï phi·∫øu 30 c√¥ng ty VN30raw data \" Raw data l√† gi√° l·ªánh mua/b√°n I II v√† kh·ªëi l∆∞·ª£ng giao d·ªãch c·ªßa c·ªï phi·∫øu 30 c√¥ng ty VN30 Th·ªùi gian thu th·∫≠p ƒë∆∞·ª£c c·∫≠p nh·∫≠t v·ªõi chu k√¨ l√† 10 gi√¢y b·∫Øt ƒë·∫ßu t·ª´ ng√†y 20 th√°ng 3 ƒë·∫øn ng√†y 19 th√°ng 4, t·ª´ 2 gi·ªù 15 ƒë·∫øn 7 gi·ªù 30 m·ªói ng√†y. Nh∆∞ng c√≥ m·ªôt s·ªë ng√†y b·ªã miss trong b·ªô d·ªØ li·ªáu n√†y (Chi ti·∫øt h∆°n trong notebook ·ªü source code) C√πng xem qua v·ªÅ s·ªë l∆∞·ª£ng observations c·ªßa m·ªói b·∫£ng T·ªïng c·ªông ta c√≥ 181 fields v√† m·ªói b·∫£ng kho·∫£ng 1345 observations (t·ªïng c·ªông 30538 quan s√°t). C≈©ng kh√° nhi·ªÅu ph·∫£i kh√¥ng n√†o. Ta s·∫Ω c√πng ti·ªÅn x·ª≠ l√Ω ch√∫ng n√†o ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#a-brief-view"},{"categories":["projects"],"content":"Data PreprocessingSau khi ƒë√£ bi·∫øt kh√°i qu√°t raw data, ta s·∫Ω c·∫ßn ph·∫£i ti·ªÅn x·ª≠ l√Ω nh·ªØng d·ªØ li·ªáu th√¥ n√†y tr∆∞·ªõc khi c√≥ th·ªÉ √°p d·ª•ng c√°c m√¥ h√¨nh m√°y h·ªçc ho·∫∑c gi·∫£m chi·ªÅu d·ªØ li·ªáu ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-preprocessing"},{"categories":["projects"],"content":"Data CleaningH√£y s·ª≠ d·ª•ng method describe() c·ªßa pandas ƒë·ªÉ c√≥ c√°i nh√¨n s∆° b·ªô nh·∫•t v·ªÅ df c·ªßa ch√∫ng ta data[0].describe() ƒê·∫ßu ti√™n, ta s·∫Ω drop duplicate v√† ƒë·ªãnh d·∫°ng l·∫°i index th·ªùi gian market = pd.DataFrame(columns=data[0].columns.to_list()) #create empty df # Data cleaning for _, df in enumerate(data): df.drop_duplicates() cols = df.columns.to_list() #convert/ replace 0 for col in cols: df[col] = pd.to_numeric(df[col], errors='coerce') # #missing handling df.fillna(0, inplace=True) market = pd.concat([market,df]).copy() #concat all clean df into market #datetime format market.reset_index(inplace=True) market = market.rename(columns={'index': 'datetime'}) market['datetime'] = market['datetime'].dt.strftime('%Y-%m-%d%H:%M:%S') market['datetime'] = pd.to_datetime(market['datetime']) market = market.sort_values(\"datetime\", ascending=True) market.set_index('datetime', inplace=True) K·∫ø ti·∫øp h√£y x·ª≠ l√Ω missing value b·∫±ng ph∆∞∆°ng ph√°p n·ªôi suy (interpolation) v·ªõi method padding, v√† sau ƒë√≥ s·∫Ω d√πng backfill ƒê√¢y l√† ph∆∞∆°ng ph√°p ∆∞·ªõc t√≠nh gi√° tr·ªã c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu ch∆∞a bi·∫øt trong ph·∫°m vi c·ªßa m·ªôt t·∫≠p h·ª£p r·ªùi r·∫°c ch·ª©a m·ªôt s·ªë ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ bi·∫øt. Nghe c√≥ v·∫ª l·∫±ng nh·∫±ng, ƒë∆°n gi·∫£n l√† th·∫ø n√†y: .interpolate(method=‚Äòpad‚Äô): fill null values b·∫±ng gi√° tr·ªã li·ªÅn k·ªÅ n√≥ l·∫ßn l∆∞·ª£t t·ª´ tr√™n xu·ªëng (n√≥ gi·ªëng nh∆∞ ffill()) .fillna(method=‚Äòbackfill‚Äô): ƒê√¢y l√† ph∆∞∆°ng ph√°p ng∆∞·ª£c l·∫°i b√™n tr√™n, fill null b·∫±ng gi√° tr·ªã li·ªÅn k·ªÅ t·ª´ d∆∞·ªõi l√™n Note C√≥ r·∫•t nhi·ªÅu ph∆∞∆°ng ph√°p n·ªôi suy nh∆∞ linear (default) hay polynomial,‚Ä¶ Nh∆∞ng OG ch·ªçn padding v√† backfill v√¨ 2 ph∆∞∆°ng ph√°p n√†y c√≥ th·ªÉ gi·ªØ cho data missing ·ªü gi√° tr·ªã s√°t nh·∫•t v·ªõi gi√° tr·ªã th·ª±c g·∫ßn nh·∫•t v√† gi√∫p cho k·∫øt qu·∫£ sau khi fill s√°t v·ªõi th·ª±c t·∫ø nh·∫•t. Ngo√†i ra 2 ph∆∞∆°ng ph√°p n√†y c√≥ th·ªÉ fill ƒë∆∞·ª£c v·ªã tr√≠ ƒë·∫ßu v√† cu·ªëi c√πng m·ªôt c√°ch hi·ªáu qu·∫£. def handle_null(X: pd.DataFrame) -\u003e pd.DataFrame: ''' handle missing value ''' for col in X.columns.to_list(): X[col].interpolate(method='pad', inplace=True) X[col].fillna(method='backfill', inplace=True) return X ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-cleaning"},{"categories":["projects"],"content":"Data transformingOG nh·∫≠n th·∫•y r·∫±ng v·ªõi c√°c tr∆∞·ªùng data hi·ªán t·∫°i ch∆∞a th·ª±c s·ª± gi√∫p √≠ch qu√° nhi·ªÅu trong vi·ªác ph√¢n t√≠ch sau n√†y (gi√° mua/b√°n v√† s·ªë l∆∞·ª£ng mua/b√°n + gi√° ph√°i sinh (label) ) Do ƒë√≥ OG c·∫ßn m·ªôt dataframe m·ªõi v·ªõi c√°c tr∆∞·ªùng m·ªõi c√≥ nhi·ªÅu gi√° tr·ªã ph√¢n t√≠ch h∆°n: gttb_ (Gi√° tr·ªã trung b√¨nh): l√† column m·ªõi ƒë∆∞·ª£c t√≠nh tr√™n b√¨nh qu√¢n gi√° c·∫£ mua v√†o, b√°n ra c·ªßa t·ª´ng c·ªï phi·∫øu ƒë∆∞·ª£c giao d·ªãch TH√ÄNH C√îNG tr√™n th·ªã tr∆∞·ªùng. total_ban \u0026 total_mua (T·ªïng b√°n/mua kh·ªëi l∆∞·ª£ng 1): l√† column m·ªõi ƒë·ªÉ t√≠nh t·ªïng gi√° b√°n kh·ªëi l∆∞·ª£ng 1 c≈©ng nh∆∞ mua kh·ªëi l∆∞·ª£ng 1 c·ªßa t·ª´ng c·ªë phi·∫øu ƒë∆∞·ª£c giao d·ªãch tr√™n th·ªã tr∆∞·ªùng. Gia_KL: sao ch√©p gi√° kh·ªëi l∆∞·ª£ng c·ªßa t·ª´ng m√£ c·ªï phi·∫øu t·ª´ b·ªô d·ªØ li·ªáu ban ƒë·∫ßu. (label) C√†i ƒë·∫∑t l·∫°i index th·ªùi gian: group by c√°c time-series theo ph√∫t. def transform_raw(market: pd.DataFrame) -\u003e pd.DataFrame: # split stock name name = [col.split('_1')[-1] for col in market.columns.to_list() if 'mua_gia_1' in col] new_df = pd.DataFrame() for i in name: # calculate gttb (mean) new_df[f'gttb_{i}'] = ((market[f'mua_gia_1{i}'] * market[f'mua_kl_1{i}'] + market[f'ban_gia_1{i}'] * market[f'ban_kl_1{i}']) /(market[f'mua_kl_1{i}'] + market[f'ban_kl_1{i}'])).copy() # get ban_kl and mua_kl new_df[f'total_ban_{i}'] = market[f'ban_kl_1{i}'].copy() new_df[f'total_mua_{i}'] = market[f'mua_kl_1{i}'].copy() # get Gia KL new_df['Gia KL'] = market['Gia KL'].copy() new_df.set_index(market.index, inplace=True) gttb = [col for col in new_df.columns.to_list() if 'gttb' in col] + ['Gia KL'] mua_ban = [col for col in new_df.columns.to_list() if col not in gttb] # Group by minute result = new_df[gttb].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute]).mean() result = pd.concat([result,new_df[mua_ban].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute ]).sum()],axis=1) #Set index in minute index = pd.to_datetime([f\"{d}{h}:{m}:00\" for (d, h, m) in result.index]) result.index = index #handle missing value result = handle_null(result) return result R·ªìi gi·ªù transform r·ªìi ki·ªÉm tra l·∫°i s·ªë l∆∞·ª£ng quan s√°t ·ªü b·∫£ng m·ªõi th√¥i # Check the length of new data len(new_market) # --\u003e 5154 V·ªõi k·∫øt qu·∫£ m·ªõi, ch·ªâ c√≤n l·∫°i 5154 quan s√°t m√† th√¥i, khi r√∫t l·∫°i m·ªôt s·ªë l∆∞·ª£ng quan s√°t l·ªõn nh∆∞ v·∫≠y, ta s·∫Ω ph·∫£i ch·∫•p nh·∫≠n r·ªßi ro m·∫•t ƒëi nhi·ªÅu th√¥ng tin v·ªÅ d·ªØ li·ªáu m√† c·ª• th·ªÉ l√† d·ªØ li·ªáu theo gi√¢y (c·ª© 10 gi√¢y c·∫≠p nh·∫≠t). Nh∆∞ng ƒë·ªïi l·∫°i, data s·∫Ω c√¥ ƒë·ªông h∆°n v√† b·ªõt nhi·ªÖu v√¨ v·ªõi s·ª± bi·∫øn ƒë·ªïi c·ªßa th·ªã tr∆∞·ªùng trong c·∫£ 1 th√°ng, s·ª± thay ƒë·ªïi c·ªßa c√°c tr∆∞·ªùng trong m·ªói 10 gi√¢y l√† qu√° nh·ªè v√† kh√¥ng ƒë√°ng k·ªÉ. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-transforming"},{"categories":["projects"],"content":"Data ScalingSau khi c√≥ b·ªô dataframe m·ªõi t·ªët h∆°n v√† s·∫°ch s·∫Ω, b∆∞·ªõc k·∫ø ti·∫øp s·∫Ω l√† scale l·∫°i d·ªØ li·ªáu v·ªÅ m·ªôt chu·∫©n ƒë·ªÉ tƒÉng hi·ªáu qu·∫£ c·ªßa c√°c thu·∫≠t to√°n h·ªçc m√°y C√≥ m·ªôt s·ªë ph∆∞∆°ng ph√°p scale data nh∆∞: Standardization, Normalization,‚Ä¶ ·ªû project n√†y, OG s·∫Ω d√πng ph∆∞∆°ng ph√°p Normalization ƒë·ªÉ scale data. Ph∆∞∆°ng ph√°p chu·∫©n h√≥a n√†y ƒë∆∞a t·ª∑ l·ªá d·ªØ li·ªáu t·ª´ ph·∫°m vi ban ƒë·∫ßu v·ªÅ chu·∫©n ph·∫°m vi t·ª´ 0 ƒë·∫øn 1, gi√° tr·ªã ƒë∆∞·ª£c normalize theo c√¥ng th·ª©c sau: $$ x' = \\frac{x - min}{max - min} $$ V·ªõi $x$ l√† gi√° tr·ªã c·∫ßn ƒë∆∞·ª£c chu·∫©n h√≥a, $max$ v√† $min$ l√† l·∫ßn l∆∞·ª£t l√† gi√° tr·ªã l·ªõn nh·∫•t v√† nh·ªè nh·∫•t trong t·∫•t c·∫£ c√°c observations c·ªßa feature trong t·∫≠p d·ªØ li·ªáu. Ta s·∫Ω d√πng MinMaxScaler c·ªßa scikit-learn trong t√°c v·ª• n√†y. from sklearn.preprocessing import MinMaxScaler # Normalization data using libraries min_max = MinMaxScaler() X = new_market.values X_std = min_max.fit_transform(X) print('Data after scaling: ') X_std # array([[9.10048201e-01, 9.43990665e-01, 9.59215952e-01, ..., # 1.31664615e-02, 1.45711006e-02, 2.90267046e-03], # [9.14492108e-01, 9.61493582e-01, 9.57989455e-01, ..., # 1.42007963e-02, 1.10109072e-04, 3.64335188e-03], # [9.12286536e-01, 9.57992999e-01, 9.56950233e-01, ..., # 3.58702686e-03, 1.43141794e-03, 4.40405173e-04], # ..., # [9.23665190e-01, 9.04317386e-01, 8.96622210e-01, ..., # 1.22024151e-01, 3.04635100e-03, 2.10293470e-02], # [9.24218272e-01, 8.89565349e-01, 8.97159958e-01, ..., # 1.05691866e-02, 1.13779375e-03, 2.88265204e-02], # [9.28532923e-01, 9.04317386e-01, 8.98196897e-01, ..., # 8.84087818e-03, 3.67030241e-04, 7.79383700e-03]] Nh∆∞ v·∫≠y l√† ƒë√£ chu·∫©n b·ªã ho√†n t·∫•t cho b∆∞·ªõc ti·∫øp theo r·ªìi. Ch√∫ng ta s·∫Ω b∆∞·ªõc v√†o thu·∫≠t to√°n ch√≠nh ƒë·∫ßu ti√™n trong project n√†y. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-scaling"},{"categories":["projects"],"content":"Principle Component Analysis (PCA)Ch√∫ng ta ƒë√£ ƒëi qua vi·ªác ti·ªÅn x·ª≠ l√Ω d√†i ngo·∫±n t·ª´ cleaning, transforming ƒë·∫øn scaling. V·∫≠y c√¢u h·ªèi l√†: d·ªØ li·ªáu ƒë√£ s·∫µn s√†ng ƒë·ªÉ √°p d·ª•ng cho c√°c m√¥ h√¨nh m√°y h·ªçc hay ch∆∞a ? C√¢u tr·∫£ l·ªùi cho tr∆∞·ªùng h·ª£p n√†y l√†: Ch∆∞a. T·∫°i sao v·∫≠y ? B·ªüi v√¨ t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng ta c√≥ qu√° nhi·ªÅu features Feature c·ªßa t·∫≠p data l√† g√¨ ? D√†nh cho b·∫°n ch∆∞a bi·∫øt, feature c·ªßa t·∫≠p data c√≤n ƒë∆∞·ª£c g·ªçi l√† c√°c tr∆∞·ªùng (hay field) c·ªßa t·∫≠p data ƒë√≥. ƒê√≥ l√† c√°c c·ªôt, m·ªói c·ªôt l√† m·ªôt ‚Äút√≠nh ch·∫•t‚Äù kh√°c nhau c·ªßa ƒë·ªëi t∆∞·ª£ng aka quan s√°t (observation) th∆∞·ªùng l√† c√°c h√†ng. Hi·ªán t·∫°i c√≥ th·ªÉ th·∫•y cleaning data c·ªßa ch√∫ng ta c√≥ 91 features: gttb_(c·ªï phi·∫øu): 30 c·ªôt gi√° tr·ªã trung b√¨nh giao d·ªãch c·ªßa 30 c·ªï phi·∫øu trong 1 ph√∫t total_ban_(c·ªï phi·∫øu): 30 c·ªôt t·ªïng kh·ªëi l∆∞·ª£ng b√°n c·ªßa 30 c·ªï phi·∫øu trong 1 ph√∫t total_mua_(c·ªï phi·∫øu): 30 c·ªôt t·ªïng kh·ªëi l∆∞·ª£ng mua c·ªßa 30 c·ªï phi·∫øu trong 1 ph√∫t Gia_KL: 1 c·ªôt gi√° ph√°i sinh VN30 Index (label) V·ªõi s·ªë l∆∞·ª£ng feature l·ªõn nh∆∞ v·∫≠y, s·∫Ω v√¥ c√πng k√©m hi·ªáu qu·∫£ n·∫øu ngay l·∫≠p t·ª©c s·ª≠ d·ª•ng train cho c√°c m√¥ h√¨nh machine learning. Gi·∫£i ph√°p ·ªü ƒë√¢y ch√≠nh l√† ta s·∫Ω gi·∫£m chi·ªÅu d·ªØ li·ªáu xu·ªëng m·ª©c v·ª´a ƒë·∫°t hi·ªáu nƒÉng t·ªët khi training m√† c≈©ng kh√¥ng l√†m m·∫•t qu√° nhi·ªÅu th√¥ng tin c·ªßa d·ªØ li·ªáu. V√¢ng ƒë√∫ng v·∫≠y, ph∆∞∆°ng ph√°p OG mu·ªën gi·ªõi thi·ªáu ch√≠nh l√† PCA hay c√≤n ƒë∆∞·ª£c bi·∫øt v·ªõi t√™n vi·ªát h√≥a l√† Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh. M·ª•c ti√™u c·ªßa ph∆∞∆°ng ph√°p n√†y l√† ƒë∆∞a b·ªô d·ªØ li·ªáu ban ƒë·∫ßu sang h·ªá t·ªça ƒë·ªô m·ªõi d·ª±a tr√™n c√°c th√†nh ph·∫ßn ch√≠nh. D·ªØ li·ªáu ·ªü h·ªá t·ªça ƒë·ªô m·ªõi c√≥ √≠t chi·ªÅu h∆°n nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c nhi·ªÅu nh·∫•t th√¥ng tin c√≥ th·ªÉ, t·ª´ ƒë√≥ gi√∫p tƒÉng t·ªëc ƒë·ªô t√≠nh to√°n v√† gi·∫£m ƒë·ªô ph·ª©c t·∫°p m√¥ h√¨nh h∆°n r·∫•t nhi·ªÅu. N√≥i t√≥m t·∫Øt cho d·ªÖ hi·ªÉu C∆° b·∫£n l√† ph∆∞∆°ng ph√°p n√†y ƒë∆∞a b·ªô data c·ªßa ta v√†o m·ªôt ‚Äúth·∫ø gi·ªõi song song‚Äù c√≥ s·ªë chi·ªÅu m·ªõi √≠t h∆°n (chi·ªÅu aka features). B·∫°n c√≥ th·ªÉ hi·ªÉu nh∆∞ l√† nh√¨n d·ªØ li·ªáu c·ªßa m√¨nh ·ªü m·ªôt g√≥c kh√°c v·∫≠y. ·ªû ph·∫ßn n√†y ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p n√†y th√¥ng qua s·ª± ph√¢n r√£ c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (Eigen decomposition of covariance matrix) ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#principle-component-analysis-pca"},{"categories":["projects"],"content":"EigenVector v√† EigenValueMa tr·∫≠n hi·ªáp ph∆∞∆°ng sai ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†: $$ S = \\frac{1}{N}\\hat{X}^T\\hat{X} $$ V·ªõi $\\hat{X} = X - \\hat{x}1^T$ l√† zero-corrected data hay d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n ho√°. Ta s·∫Ω vi·∫øt h√†m get_eigenpairs() ƒë·ªÉ t√¨m c√°c vector ri√™ng v√† gi√° tr·ªã ri√™ng c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai: $$ Su_i = \\lambda_iu_i $$ Trong ƒë√≥: c√°c $(\\lambda_i,u_i)$ l√† c√°c c·∫∑p tr·ªã ri√™ng (kh√¥ng √¢m) v√† vector ri√™ng c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai $S$ T·∫°i sao l·∫°i c·∫ßn t√¨m c√°c vector ri√™ng v√† gi√° tr·ªã ri√™ng c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai ? Vi·ªác s·ª≠ d·ª•ng c√°c gi√° tr·ªã ri√™ng ƒë·ªÉ ƒë√°nh gi√° s·ª± quan tr·ªçng c·ªßa m·ªói th√†nh ph·∫ßn ch√≠nh ƒë∆∞·ª£c t·∫°o ra t·ª´ vi·ªác gi·∫£m chi·ªÅu d·ªØ li·ªáu. C√°c gi√° tr·ªã ri√™ng c√†ng l·ªõn th√¨ th√†nh ph·∫ßn ch√≠nh t∆∞∆°ng ·ª©ng c√†ng quan tr·ªçng. C√°c vector ri√™ng t∆∞∆°ng ·ª©ng v·ªõi c√°c gi√° tr·ªã ri√™ng n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x√°c ƒë·ªãnh h∆∞·ªõng c·ªßa c√°c th√†nh ph·∫ßn ch√≠nh. Gi√° tr·ªã ri√™ng (Eigenvalues $\\lambda_i$): C√°c h·ªá s·ªë ƒë∆∞·ª£c g·∫Øn v·ªõi c√°c vector ri√™ng, cung c·∫•p cho ƒë·ªô l·ªõn c·ªßa tr·ª•c. Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng l√† th∆∞·ªõc ƒëo hi·ªáp ph∆∞∆°ng sai c·ªßa d·ªØ li·ªáu. Vector ri√™ng (EigenVector $u_i$):C√°c vector (kh√°c 0) kh√¥ng thay ƒë·ªïi h∆∞·ªõng khi √°p d·ª•ng b·∫•t k·ª≥ ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh (linear transformation) n√†o, n√≥ ch·ªâ thay ƒë·ªïi theo h·ªá s·ªë v√¥ h∆∞·ªõng. H√†m s·∫Øp x·∫øp c√°c vector ri√™ng (Sort eigenvalues): B·∫±ng c√°ch s·∫Øp x·∫øp c√°c vector ri√™ng theo th·ª© t·ª± c·ªßa gi√° tr·ªã ri√™ng, ta c√≥ th·ªÉ ch·ªçn ra c√°c vector ri√™ng c√≥ gi√° tr·ªã ri√™ng l·ªõn nh·∫•t ƒë·ªÉ x√¢y d·ª±ng c√°c th√†nh ph·∫ßn ch√≠nh c·ªßa d·ªØ li·ªáu (ƒë√≥ng g√≥p nhi·ªÅu nh·∫•t v√†o vi·ªác gi·∫£i th√≠ch s·ª± bi·∫øn thi√™n c·ªßa d·ªØ li·ªáu). C√°c th√†nh ph·∫ßn ch√≠nh n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√°i c·∫•u tr√∫c d·ªØ li·ªáu ban ƒë·∫ßu m√† v·∫´n gi·ªØ ƒë∆∞·ª£c ƒë·ªô gi·ªëng nhau c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu ban ƒë·∫ßu. def get_eigenpairs(X: np.array) -\u003e list: ''' Input: X: np.array (init matrix) return eigenpairs containing eigenvalues and eigenvectors of covariance matrix ''' # Covariance matrix cov_mat = np.cov(X.T) # Eigenvalues and Eigenvectors evals, evecs = np.linalg.eigh(cov_mat) # Sort eigenvalues epairs = [(abs(eval), evec) for (eval, evec) in zip(evals, evecs.T)] epairs = sorted(epairs, key = lambda pair: pair[0], reverse = True) return epairs ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#eigenvector-v√†-eigenvalue"},{"categories":["projects"],"content":"Cumulative Sum of ComponentsT√≠nh t·ªïng t√≠ch l≈©y c·ªßa c√°c th√†nh ph·∫ßn trong PCA (Cumulative Sum of Explained Variance) ƒë·ªÉ x√°c ƒë·ªãnh t·ªïng ph·∫ßn trƒÉm ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi c√°c th√†nh ph·∫ßn ƒë∆∞·ª£c gi·ªØ l·∫°i trong m√¥ h√¨nh PCA. $$ r_K = \\frac{\\sum^K_{i=1}\\lambda_i}{\\sum^D_{j=1}\\lambda_j} $$ l√† l∆∞·ª£ng th√¥ng tin ƒë∆∞·ª£c gi·ªØ l·∫°i khi s·ªë chi·ªÅu d·ªØ li·ªáu m·ªõi sau PCA l√† K. H√†m findNumVec() th·ª±c hi·ªán vi·ªác l·∫•y c√°c gi√° tr·ªã ri√™ng t·ª´ danh s√°ch c√°c eigenpairs v√† chuy·ªÉn ƒë·ªïi ch√∫ng th√†nh m·ªôt m·∫£ng numpy. Sau ƒë√≥, n√≥ t√≠nh t·ªïng t√≠ch l≈©y c·ªßa c√°c gi√° tr·ªã ri√™ng, s·ª≠ d·ª•ng h√†m np.cumsum () chu·∫©n h√≥a t·ªïng c·ªßa ch√∫ng =\u003e cho ra m·ªôt danh s√°ch c√°c gi√° tr·ªã (trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1) ƒë·∫°i di·ªán cho t·ª∑ l·ªá ph·∫ßn trƒÉm ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi m·ªói th√†nh ph·∫ßn ch√≠nh. Sau ƒë√≥, h√†m l·∫∑p qua danh s√°ch t·ªïng t√≠ch l≈©y v√† t√¨m ch·ªâ m·ª•c c·ªßa gi√° tr·ªã ƒë·∫ßu ti√™n l·ªõn h∆°n ho·∫∑c b·∫±ng t·ª∑ l·ªá ph·∫ßn trƒÉm ph∆∞∆°ng sai mong mu·ªën ƒë∆∞·ª£c gi·∫£i th√≠ch. Ch·ªâ s·ªë n√†y ƒë·∫°i di·ªán cho s·ªë l∆∞·ª£ng th√†nh ph·∫ßn ch√≠nh c·∫ßn thi·∫øt ƒë·ªÉ gi·∫£i th√≠ch t·ª∑ l·ªá ph·∫ßn trƒÉm ph∆∞∆°ng sai ƒë√≥, v√¨ v·∫≠y h√†m tr·∫£ v·ªÅ gi√° tr·ªã n√†y c·ªông v·ªõi 1 (v√¨ l·∫≠p ch·ªâ m·ª•c Python b·∫Øt ƒë·∫ßu t·ª´ 0). def findNumVec(eigenpairs: list, percent = 0.9): ''' Find number of principal components (eigenvectors) -\u003e return the number of principal components when total accumulate \u003e= percent ''' # Get eigenvalues eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) # Cumulative sum and calculate percent cumsum = np.cumsum(eigenvals) cumsum /= cumsum[-1] # Find number of principal components that accumulate \u003e= percent for i, val in enumerate(cumsum): if val \u003e= percent: return i + 1 Ta s·∫Ω th·ª≠ t√¨m xem s·ªë th√†nh ph·∫ßn ch√≠nh c·∫ßn ƒë·ªÉ gi·ªØ ƒë∆∞·ª£c 80% d·ªØ li·ªáu: print(findNumVec(epairs, 0.8)) # --\u003e 28 ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#cumulative-sum-of-components"},{"categories":["projects"],"content":"Scree ChartTa s·∫Ω v·∫Ω m·ªôt bi·ªÉu ƒë·ªì th·ªÉ hi·ªán quan h·ªá c·ªßa s·ªë l∆∞·ª£ng th√†nh ph·∫ßn ch√≠nh v√† ph·∫ßn trƒÉm ph∆∞∆°ng sai gi·∫£i th√≠ch t√≠ch l≈©y def screeplot(eigenpairs): ''' Scree plot ''' fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True) eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) cumsum = np.cumsum(eigenvals) # extracts the eigenvalues from the eigenpairs and calculates their cumulative sum cumsum /= cumsum[-1] name = [f'PCA {i}' for i in range(len(cumsum))] # line plot # the eigenvalues are plotted against the number of principal components axes[0].plot(range(len(eigenvals)), eigenvals, marker = '.', color = 'b', label = 'Eigenvalue') # the cumulative proportion of the variance explained by each component is plotted against the number of principal components axes[1].plot(range(len(cumsum)), cumsum, marker = '.', color = 'green', label = 'Cumulative propotion') # y axis label axes[0].set_ylabel('Eigen values') axes[1].set_ylabel('Cumulative explained variance') # item legend axes[0].legend() axes[1].legend() # grid axes[0].grid() axes[1].grid() # title fig.supxlabel('Number of components') plt.tight_layout() plt.show() #print the cumsum of eigenvalues print(pd.DataFrame(cumsum, columns = ['Cumulative total'], index = name)) result = { str(i): f\"PC {i+1}({var:.1f}%)\" for i, var in enumerate(cumsum*100) } return result pca_scree = screeplot(epairs) Scree plotScree plot \" Scree plot Gi·∫£i th√≠ch ƒê∆∞·ªùng c·ªßa gi√° tr·ªã ri√™ng m√†u xanh n∆∞·ªõc bi·ªÉn tr√™n bi·ªÉu ƒë·ªì cho ta bi·∫øt ƒë·ªô l·ªõn c·ªßa m·ªói th√†nh ph·∫ßn ch√≠nh v√† t·∫ßm quan tr·ªçng c·ªßa ch√∫ng trong gi·∫£i th√≠ch s·ª± bi·∫øn thi√™n c·ªßa d·ªØ li·ªáu. N·∫øu gi√° tr·ªã ri√™ng c·ªßa m·ªôt th√†nh ph·∫ßn ch√≠nh l√† l·ªõn, th√¨ th√†nh ph·∫ßn ƒë√≥ c√≥ t·∫ßm quan tr·ªçng cao trong vi·ªác gi·∫£i th√≠ch s·ª± bi·∫øn thi√™n c·ªßa d·ªØ li·ªáu. ƒê∆∞·ªùng m√†u xanh l√° th·ªÉ hi·ªán t·ªïng t√≠ch l≈©y cho ta bi·∫øt t·ªïng ph·∫ßn trƒÉm ƒë·ªô l·ªõn c·ªßa s·ª± bi·∫øn thi√™n c·ªßa d·ªØ li·ªáu m√† c√°c th√†nh ph·∫ßn ch√≠nh c√≥ th·ªÉ gi·∫£i th√≠ch. D·ª±a v√†o bi·ªÉu ƒë·ªì tr√™n c√≥ th·ªÉ nh·∫≠n th·∫•y n·∫øu ch·ªâ c√≥ 2 chi·ªÅu, ta ch·ªâ gi·ªØ ƒë∆∞·ª£c kho·∫£ng 37% d·ªØ li·ªáu ban ƒë·∫ßu. ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#scree-chart"},{"categories":["projects"],"content":"Visualize PCAB√¢y gi·ªù, ta s·∫Ω th·ª±c hi·ªán chi·∫øu d·ªØ li·ªáu ban ƒë·∫ßu ƒë√£ chu·∫©n h√≥a $\\hat{X}$ xu·ªëng kh√¥ng gian con t√¨m ƒë∆∞·ª£c v√† l·∫•y ra ma tr·∫≠n c√°c th√†nh ph·∫ßn ch√≠nh ƒë·ªÉ ti·∫øp t·ª•c c√¥ng vi·ªác ph√¢n t√≠ch v√† x√¢y d·ª±ng m√¥ h√¨nh. H√†m getPC() tr·∫£ v·ªÅ m·ªôt ma tr·∫≠n c√°c th√†nh ph·∫ßn ch√≠nh t·ª´ ma tr·∫≠n ban ƒë·∫ßu, d·ª±a tr√™n s·ªë l∆∞·ª£ng th√†nh ph·∫ßn ƒë√£ cho ho·∫∑c s·ªë l∆∞·ª£ng th√†nh ph·∫ßn gi·ªØ ƒë∆∞·ª£c 80% d·ªØ li·ªáu (n·∫øu num_components kh√¥ng ƒë∆∞·ª£c ƒë∆∞a ra). Ma tr·∫≠n tr·ªçng s·ªë $W$ l√† ma tr·∫≠n chuy·ªÉn ƒë·ªïi tuy·∫øn t√≠nh ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu g·ªëc v√†o kh√¥ng gian m·ªõi, trong ƒë√≥ m·ªói th√†nh ph·∫ßn ch√≠nh ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒë·ªô quan tr·ªçng gi·∫£m d·∫ßn. C·ª• th·ªÉ, m·ªói c·ªôt c·ªßa ma tr·∫≠n $W$ l√† m·ªôt vector ri√™ng chu·∫©n h√≥a t∆∞∆°ng ·ª©ng v·ªõi c√°c gi√° tr·ªã ri√™ng c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai $s$. Th·ª±c hi·ªán vi·ªác nh√¢n ma tr·∫≠n $W$ v·ªõi ho√°n v·ªã c·ªßa ma tr·∫≠n ƒë√£ chu·∫©n h√≥a $\\hat{X}$ (init_matrix). Ma tr·∫≠n k·∫øt qu·∫£ sau ƒë√≥ ti·∫øp t·ª•c ƒë∆∞·ª£c ho√°n v·ªã ƒë·ªÉ ph√π h·ª£p v·ªõi h√¨nh d·∫°ng ban ƒë·∫ßu c·ªßa init_matrix v√† tr·∫£ v·ªÅ k·∫øt qu·∫£. def getPC(eigenpairs, init_matrix, num_components = None): ''' Return matrix of principal components from init_matrix ''' # default num_components = number which to keep 80% data if num_components is None: num_components = findNumVec(eigenpairs, 0.8) # extracts the eigen vectors corresponding to the top num_components eigenvalues from the eigenpairs list eigenvecs = [eigenvec for (_, eigenvec) in eigenpairs[:num_components]] W = np.array([e.T for e in eigenvecs]) # stacks the eigen vectors into a weight matrix W return (W @ init_matrix.T).T X_pca = getPC(epairs, X_std) V·∫≠y l√† ta ƒë√£ gi·∫£m ƒë∆∞·ª£c ƒë·ªô ph·ª©c t·∫°p cho b·ªô d·ªØ li·ªáu kh√° ‚Äúnh·ªçc nh·∫±n‚Äù n√†y. H√£y tr·ª±c quan h√≥a l√™n bi·ªÉu ƒë·ªì ƒë·ªÉ c√≥ m·ªôt g√≥c nh√¨n c·ª• th·ªÉ v√† r√µ r√†ng h∆°n. Bi·ªÉu ƒë·ªì scatter plot sau khi PCA c√≥ th·ªÉ gi√∫p cho ch√∫ng ta nh√¨n th·∫•y c√°ch d·ªØ li·ªáu ƒë∆∞·ª£c ph√¢n b·ªë tr√™n c√°c th√†nh ph·∫ßn ch√≠nh (principal components) v√† ki·ªÉm tra xem li·ªáu ch√∫ng ta c√≥ th·ªÉ t√¨m th·∫•y c√°c cluster ho·∫∑c pattern n√†o trong d·ªØ li·ªáu. plt.scatter(X_pca[:,0], X_pca[:,1]) plt.xlabel('PC1') plt.ylabel('PC2') plt.title('Visualizing data through PCA', fontsize=18) plt.gca().set_aspect('equal', 'datalim') plt.grid() plt.show() Visualizing data via PCAVisualizing data via PCA \" Visualizing data via PCA Okayy d·ª±a v√†o bi·ªÉu ƒë·ªì tr√™n, c≈©ng c√≥ th·ªÉ th·∫•y l√† d·ªØ li·ªáu ·ªü kh√¥ng gian m·ªõi ƒë√£ ph√¢n t√°ch kh√° r√µ r√†ng r·ªìi. ƒêi·ªÅu n√†y nghƒ©a l√† ph∆∞∆°ng ph√°p PCA ƒë√£ gi·∫£m s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu m·ªôt c√°ch hi·ªáu qu·∫£. B∆∞·ªõc ti·∫øp theo ch√≠nh l√† √°p v√†o m√¥ h√¨nh K-Means ƒë·ªÉ ph√¢n c·ª•m v√† t√¨m pattern. Ch√∫ng ta s·∫Ω c√πng chi·∫øn ti·∫øp ·ªü ph·∫ßn 2 nh√© ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:4","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#visualize-pca"},{"categories":["projects"],"content":"To be ContinueCh√∫ng ta ƒë√£ th·ª±c hi·ªán c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† sau ƒë√≥ l√† th·ª±c hi·ªán PCA ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu m·ªôt c√°ch hi·ªáu qu·∫£. B√†i sau ph·∫ßn 2, OG s·∫Ω th·ª±c hi·ªán training m√¥ h√¨nh K-means clustering v√† cu·ªëi c√πng l√† ph√¢n t√≠ch d·ªØ li·ªáu ch·ª©ng kho√°ng. ƒê√¢y l√† ki·∫øn th·ª©c t√≠ch g√≥p t·ª´ nhi·ªÅu ngu·ªìn v√† nghi√™n c·ª©u c·ªßa nh√≥m OG, t·∫•t nhi√™n kh√¥ng th·ªÉ tr√°nh kh·ªèi sai s√≥t. Hy v·ªçng b√†i vi·∫øt l·∫ßn n√†y th√∫ v·ªã v√† gi√∫p b·∫°n ƒë·ªçc th∆∞ gi√£n, tham kh·∫£o. -Mew- ","date":"31 Aug 2023","objectID":"/stock_analysis/:5:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#to-be-continue"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis P2 Stock Analysis using PCA and K-means Read more... ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#related"},{"categories":[],"content":"Hellooo OG ƒë√¢yy ! ·ªû b√†i n√†y, m√¨nh s·∫Ω k·ªÉ c∆° duy√™n ƒë∆∞a m√¨nh ƒë·∫øn v·ªõi ng√†nh Data v√† quy·∫øt ƒë·ªãnh d·∫•n th√¢n v√†o con ƒë∆∞·ªùng tr·ªü th√†nh m·ªôt Data Engineer üòÑ G√©t Goo! ","date":"30 Aug 2023","objectID":"/start_journey/:0:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#"},{"categories":[],"content":"·ª¶a ng√†nh Data Science ?Khoan Khoan ‚Ä¶ B√™n tr√™n l√† Engineer, qua ƒë√¢y l√† Science l√† sao OG ? T·ª´ t·ª´ n√†o üòÑ M·ªçi chuy·ªán b·∫Øt ƒë·∫ßu khi m√¨nh ƒë·∫≠u v√†o m·ªôt ng√†nh ƒë∆∞·ª£c ca ng·ª£i l√† ng√†nh ‚Äúquy·∫øn r≈©‚Äù nh·∫•t th·∫ø k·ª∑ 21 theo Harvard Business Review , ƒë√≥ l√† Data Science. Kh√∫c n√†y m√¨nh nghe c≈©ng o√°ch o√°ch, nh∆∞ng ch√≠nh x√°c Data Science l√† g√¨ ? V√† c√°c nh√† khoa h·ªçc d·ªØ li·ªáu (data scientist) l√†m g√¨ ? ","date":"30 Aug 2023","objectID":"/start_journey/:1:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#·ªßa-ng√†nh-data-science-"},{"categories":[],"content":"Data Science l√† g√¨ nh·ªâ?Ng√†nh Khoa h·ªçc d·ªØ li·ªáu hay Data Science l√† m·ªôt lƒ©nh v·ª±c li√™n ng√†nh ·ª©ng d·ª•ng c√°c ph∆∞∆°ng ph√°p khoa h·ªçc, thu·∫≠t to√°n v√† c√°c ph√¢n t√≠ch th·ªëng k√™ ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a t·ª´ d·ªØ li·ªáu. Hay n√≥i b·∫±ng c√°ch d·ªÖ hi·ªÉu, Data Science l√† ng√†nh t√¨m ki·∫øm, ph√¢n t√≠ch d·ªØ li·ªáu ƒë·ªÉ khai th√°c t·∫•t c·∫£ nh·ªØng gi√° tr·ªã m√† d·ªØ li·ªáu mang l·∫°i ƒë·ªÉ ph·ª•c v·ª• nhi·ªÅu m·ª•c ƒë√≠ch kh√°c nhau. Data Science l√† ·ª©ng d·ª•ng khoa h·ªçc ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a c·ªßa d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n t∆∞∆°ng laiData Science l√† ·ª©ng d·ª•ng khoa h·ªçc ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a c·ªßa d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n \" Data Science l√† ·ª©ng d·ª•ng khoa h·ªçc ƒë·ªÉ t√¨m ki·∫øm √Ω nghƒ©a c·ªßa d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n t∆∞∆°ng lai M·ªôt nh√† khoa h·ªçc d·ªØ li·ªáu (Data Scientist) l√† ng∆∞·ªùi ch·ªãu tr√°ch nhi·ªám ƒë∆∞a ra c√°c d·∫´n ch·ª©ng t·ª´ d·ªØ li·ªáu, ƒë·ªÉ t·ª´ ƒë√≥ ƒë·ªÅ xu·∫•t c√°c gi·∫£i ph√°p, k·∫ø ho·∫°ch hay ƒë·ªãnh h∆∞·ªõng t·ª´ √Ω nghƒ©a t√¨m ƒë∆∞·ª£c t·ª´ d·ªØ li·ªáu ƒë·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n kinh doanh kh√°c nhau. M·ªôt data scientist c·∫ßn ph·∫£i bi·∫øt k·ªπ nƒÉng g√¨? L·∫≠p tr√¨nh: Python v√† R l√† 2 ng√¥n ng·ªØ ch√≠nh ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªëi v·ªõi ng√†nh n√†y. Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh linh ho·∫°t ph·ªï bi·∫øn v·ªõi r·∫•t nhi·ªÅu th∆∞ vi·ªán ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu nh∆∞ numpy, pandas, matplotlib,‚Ä¶ Trong khi ƒë√≥ R t·ªè l√† l√† m·ªôt ng√¥n ng·ªØ m·∫°nh m·∫Ω v·ªÅ ph√¢n t√≠ch v√† th·ªëng k√™, ngo√†i ra R c≈©ng th∆∞·ªùng ƒë∆∞·ª£c d√πng trong nghi√™n c·ª©u v√† h·ªçc thu·∫≠t. Th·ªëng k√™ v√† ·ª©ng d·ª•ng to√°n h·ªçc: N·∫øu b·∫°n kh√¥ng y√™u th√≠ch to√°n h·ªçc, ch·∫Øc h·∫≥n b·∫°n c≈©ng s·∫Ω kh√¥ng th·ªÉ l√†m ƒëi·ªÅu ƒë√≥ v·ªõi data science. H·∫≥n v·∫≠y, l√† m·ªôt nh√† khoa h·ªçc d·ªØ li·ªáu, b·∫°n c·∫ßn c√≥ m·ªôt n·ªÅn t·∫£ng ki·∫øn th·ª©c to√°n h·ªçc v·ªØng, ƒë·∫∑c bi·ªát l√† v·ªÅ x√°c su·∫•t th·ªëng k√™ v√† ƒë·∫°i s·ªë tuy·∫øn t√≠nh,‚Ä¶ SQL v√† DBMS: Ta ph·∫£i ti·∫øp x√∫c r·∫•t nhi·ªÅu v·ªõi h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu (Database Management System hay DBMS), ƒë√≥ c√≥ th·ªÉ l√† h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu Quan H·ªá (Relational Database Management System) nh∆∞ MySQL, Postgres, SQL server‚Ä¶ hay NoSQL database nh∆∞ MongoDB, Cassandra,‚Ä¶ V√† ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi database (RDBMS), ƒëi·ªÅu kh√¥ng th·ªÉ thi·∫øu ch√≠nh l√† SQL (Structured query language aka si c·ªì hay √©t qui eo üòÇ ). C∆° b·∫£n th√¨ ƒë√¢y l√† ng√¥n ng·ªØ d√πng ƒë·ªÉ truy su·∫•t d·ªØ li·ªáu, giao ti·∫øp v·ªõi database, ƒë·∫∑c bi·ªát l√† c√°c RDBMS. AI, Machine learning: Khi c√≥ m·ªôt l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì, m·ªôt data scientist c√≥ th·ªÉ s·∫Ω d√πng ch√∫ng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc m√°y ho·∫∑c m·∫°ng ƒë·ªÉ gi·∫£i c√°c b√†i to√°n h·ªìi quy v√† ƒë∆∞a ra ƒë∆∞·ª£c c√°c d·ª± ƒëo√°n v·ªÅ xu h∆∞·ªõng data hay gi·∫£i quy·∫øt c√°c b√†i to√°n ph√¢n lo·∫°i. C√≥ hi·ªÉu bi·∫øt v·ªÅ c√°c thu·∫≠t to√°n m√°y h·ªçc v√† ki·∫øn tr√∫c m·∫°ng noron c≈©ng l√† m·ªôt ƒëi·ªÅu c·∫ßn c√≥ ·ªü nh√† khoa h·ªçc d·ªØ li·ªáu. ƒê·ªçc ƒë·∫øn ƒë√¢y, c√≥ th·ªÉ b·∫°n s·∫Ω c√≥ c·∫£m gi√°c ‚ÄúD√®j√† vu‚Äù nh·∫π ‚Ä¶ Sao nhi·ªÅu ch·ªï gi·ªëng Data Analyst th·∫ø nh·ªâ ? M√† thi·ªát ra l√† kh√¥ng gi·ªëng ƒë√¢u nh√©, hai ng√†nh n√†y ch·ªâ l√† anh em x√£ h·ªôi v·ªõi nhau m√† th√¥i üòÇ ","date":"30 Aug 2023","objectID":"/start_journey/:1:1","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-science-l√†-g√¨-nh·ªâ"},{"categories":[],"content":"Data Scientist vs Data AnalystS·∫µn ti·ªán k·ªÉ m·ªôt ch√∫t v·ªÅ vai tr√≤ c·ªßa m·ªôt ng∆∞·ªùi Data Analyst. V·ªÅ c∆° b·∫£n, vai tr√≤ c·ªßa h·ªç c≈©ng gi·ªëng v·ªõi c√°c data scientist, h·ªç c≈©ng ph√¢n t√≠ch d·ªØ li·ªáu, c·ªë g·∫Øng t√¨m ki·∫øm v√† r√∫t ra gi√° tr·ªã t·ª´ ch√∫ng. Nh∆∞ng s·∫Ω c√≥ m·ªôt s·ªë ƒëi·ªÉm kh√°c bi·ªát: Data Analyst Data Science Chuy√™n vi√™n ph√¢n t√≠ch d·ªØ li·ªáu Nh√† khoa h·ªçc d·ªØ li·ªáu V·∫´n l√†m c√¥ng vi·ªác c·ªßa DS nh∆∞ng v·ªõi quy m√¥ nh·ªè T·ªèa s√°ng v·ªõi l∆∞·ª£ng data kh·ªïng l·ªì (BigData) Kh√¥ng c·∫ßn nhi·ªÅu ki·∫øn th·ª©c l·∫≠p tr√¨nh C·∫ßn ki·∫øn th·ª©c l·∫≠p tr√¨nh C·∫ßn c√≥ ki·∫øn th·ª©c v·ªÅ ho·∫°t ƒë·ªông kinh doanh nhi·ªÅu h∆°n v√† v·ªØng v·ªÅ ki·∫øn th·ª©c th·ªëng k√™ C·∫ßn c√≥ ki·∫øn th·ª©c kh√¥ng ch·ªâ to√°n th·ªëng k√™, ·ª©ng d·ª•ng m√† c√≤n ph·∫£i c√≥ ki·∫øn th·ª©c v·ªÅ computer science, AI/ML,‚Ä¶ D·ª±a v√†o d·ªØ li·ªáu ƒë∆∞a ra c√°c gi√° tr·ªã c√≥ √≠ch v√† c√°i nh√¨n tr·ª±c quan v·ªÅ d·ªØ li·ªáu ƒê∆∞·ª£c y√™u c·∫ßu ph√°t tri·ªÉn ‚Äúdata product‚Äù ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh c√≥ √≠ch t·ª´ t·∫≠p d·ªØ li·ªáu l·ªõn Data Science and Data AnalyticSource: https://www.datascience-pm.com/wp-content/uploads/2021/05/data-scientist-vs-analyst-venn-diagram.png \" Data Science and Data Analytic R·ªìi okay n√£y gi·ªù l√† c·∫£ data science (DS) v√† data analytic (DA) r·ªìi. Gi·ªù l√† m·ªõi ƒë·∫øn data engineer c·ªßa tui n√® hihi üòÑ ","date":"30 Aug 2023","objectID":"/start_journey/:1:2","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-scientist-vs-data-analyst"},{"categories":[],"content":"Data Engineer l√† g√¨ ?Tuy h·ªçc Data Science, nh∆∞ng th·ª±c ra ngay t·ª´ nh·ªØng l√∫c c√≤n m∆°n m·ªün c·∫•p 3, OG ƒë√£ t·ª´ng c√≥ ∆∞·ªõc mu·ªën tr·ªü th√†nh m·ªôt l·∫≠p tr√¨nh vi√™n m·ªôt tay cafe m·ªôt tay ch√©m code b√¨nh lo·∫°n thi√™n h·∫° üòÇ V√† th·∫ø l√† t√¨m ƒë∆∞·ª£c m·ªôt ng√†nh th√≠ch h·ª£p ƒë∆∞·ª£c coi l√† ‚ÄúSoftware engineer cho data‚Äù, ng√†nh n√†y l√† m·ªôt trong c√°c ng√†nh c√≥ xu h∆∞·ªõng ph√°t tri·ªÉn nhanh nh·∫•t trong nh√≥m ng√†nh c√¥ng ngh·ªá. V√¢ng ƒë√≥ ch√≠nh l√† Data Engineer Data EngineerData Engineer ƒë∆∞·ª£c coi l√† Software Engineer ·ªü Data field \" Data Engineer ƒê·∫ßu ti√™n, Data Engineer hay DE ƒë∆∞·ª£c g·ªçi l√† k·ªπ s∆∞ d·ªØ li·ªáu. ƒê√¢y l√† vai tr√≤ ƒë·∫£m nhi·ªám vi·ªác ph√¢n t√≠ch ngu·ªìn d·ªØ li·ªáu, x√¢y d·ª±ng v√† duy tr√¨ h·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu hi·ªáu qu·∫£. Ngo√†i ra c≈©ng l√† ng∆∞·ªùi ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho c√°c ph√≤ng ban kh√°c s·ª≠ d·ª•ng. C∆° b·∫£n ƒë·ªÉ l√† ƒë·ªÉ cho DS v√† DA l√†m vi·ªác m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t, h·ªç c·∫ßn c√≥ m·ªôt ngu·ªìn data ·ªïn ƒë·ªãnh v√† s·∫°ch s·∫Ω. V√† ng∆∞·ªùi ƒë·∫£m nhi·ªám vi·ªác lu√¢n chuy·ªÉn data ƒë√≥ t·ªõi cho h·ªç ch√≠nh l√† Data Engineer. Kh√¥ng ch·ªâ c√≥ DS v√† DA m√† data engineer ph·ª•c v·ª• cho t·∫•t c·∫£ c√°c ph√≤ng ban kh√°c Data Engineer N√≥i t√≥m l·∫°i, Data Engineer l√† ng∆∞·ªùi x√¢y d·ª±ng c√°c ƒë∆∞·ªùng ·ªëng d·ªØ li·ªáu (data pipeline) ƒë·ªÉ truy·ªÅn d·ªØ li·ªáu t·ª´ n∆°i n√†y sang n∆°i kh√°c m·ªôt c√°ch ch·∫•t l∆∞·ª£ng nh·∫•t :)) Kh√°i ni·ªám c∆° b·∫£n l√† th·∫ø th√¥i, nghe c√≥ v·∫ª ƒë∆°n gi·∫£n ph·∫£i kh√¥ng. H√£y ti·∫øp t·ª•c v·ªõi m·ª•c ti·∫øp theo ƒë·ªÉ xem li·ªáu ta c·∫ßn g√¨ ƒë·ªÉ tr·ªü th√†nh data engineer ","date":"30 Aug 2023","objectID":"/start_journey/:2:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-engineer-l√†-g√¨-"},{"categories":[],"content":"Data Engineer th√¨ c·∫ßn bi·∫øt g√¨ ?M·ªôt data Engineer v·ªÅ b·∫£n ch·∫•t l√† x√¢y d·ª±ng c√°c data pipeline ƒë·ªÉ lu√¢n chuy·ªÉn d·ªØ li·ªáu. ƒê·ªÉ l√†m t·ªët vi·ªác ƒë√≥, k·ªπ s∆∞ d·ªØ li·ªáu ph·∫£i bi·∫øt: K·ªπ nƒÉng l·∫≠p tr√¨nh: T·∫•t nhi√™n r·ªìi, b·∫°n l√† m·ªôt nh√¢n vi√™n IT th√¨ ƒëi·ªÅu n√†y l√† ph·∫£i c√≥. C√°c ng√¥n ng·ªØ m√† DE th∆∞·ªùng d√πng l√† SQL, Python v√† R. H·ªá c∆° s·ªü d·ªØ li·ªáu quan h·ªá v√† phi quan h·ªá: D·ªØ li·ªáu c√≥ r·∫•t nhi·ªÅu d·∫°ng: Structure/Semi/Unstructure data, do ƒë√≥ c≈©ng c·∫ßn c√≥ nhi·ªÅu lo·∫°i database qu·∫£n l√Ω ch√∫ng. V√† DE l√†m vi·ªác r·∫•t nhi·ªÅu v·ªõi database. H·ªç s·∫Ω l√† ng∆∞·ªùi tr·ª±c ti·∫øp t∆∞∆°ng t√°c k·ªÉ c·∫£ v·ªõi SQL v√† NoSQL database. ETL/ELT: ETL aka Extract Transform Load hay ELT aka Extract Load Transform l√† quy tr√¨nh x·ª≠ l√Ω v√† lu√¢n chuy·ªÉn d·ªØ li·ªáu t·ª´ ngu·ªìn ƒë·∫øn ƒë√≠ch. M·ªôt DE ph·∫£i n·∫Øm ƒë∆∞·ª£c ƒë·ªÉ thi·∫øt k·∫ø data pipeline m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t Data Warehouse: hay ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† kho ch·ª©a d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ s·∫Ω ph·∫£i x√¢y d·ª±ng, thi·∫øt k·∫ø c·∫•u tr√∫c data warehouse tr√™n cloud platform v√† x√¢y d·ª±ng c√°c k·∫øt n·ªëi d·ªØ li·ªáu ƒë·ªÉ t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô truy xu·∫•t v√† ƒë·∫£m b·∫£o vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu. Big Data: B·∫°n c≈©ng c·∫ßn ph·∫£i bi·∫øt c√°c ki·∫øn tr√∫c l∆∞u tr·ªØ v√† x·ª≠ l√Ω t·∫≠p d·ªØ li·ªáu l·ªõn nh∆∞ Hadoop, Spark,‚Ä¶ Cloud: T·∫•t nhi√™n l√† ph·∫£i c√≥ r·ªìi, c√°c cloud platform nh∆∞ Google Cloud Platform, AWS, Azure,‚Ä¶ ƒë√£ r·∫•t n·ªïi ti·∫øng trong vi·ªác h·ªó tr·ª£ x√¢y d·ª±ng v√† thi·∫øt k·∫ø h·ªá th·ªëng pipeline c≈©ng nh∆∞ h·ªó tr·ª£ t·ªëi ƒëa vi·ªác x·ª≠ l√Ω bigdata c≈©ng nh∆∞ deploy h·ªá th·ªëng h·∫° t·∫ßng m·ªôt c√°ch nhanh ch√≥ng. B·∫°n c√≥ th·ªÉ s·∫Ω ph·∫£i l√†m vi·ªác v·ªõi l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì v√† t·∫≠p d·ªØ li·ªáu l·ªõn. V√† ƒë·ªÉ x√¢y d·ª±ng h·ªá th·ªëng x·ª≠ l√Ω ƒë∆∞·ª£c l∆∞·ª£ng d·ªØ li·ªáu ƒë√≥, ch·∫Øc ch·∫Øn ph·∫£i c√≥ s·ª± g√≥p m·∫∑t c·ªßa c√°c n·ªÅn t·∫£ng ƒë√°m m√¢y. Well‚Ä¶ Nh√¨n chung c≈©ng nhi·ªÅu th·ª© c·∫ßn ph·∫£i bi·∫øt ƒë·∫•y nh·ªâ, t·∫•t nhi√™n ƒë√≥ ch·ªâ l√† m·ªôt s·ªë ƒëi·ªÅu quan tr·ªçng nh·∫•t. Ngo√†i ra b·∫°n c≈©ng c·∫ßn ph·∫£i bi·∫øt m·ªôt s·ªë ki·∫øn th·ª©c kh√°c v·ªÅ Unix v√† Linux, Docker, Git, Batch/Stream Processing,‚Ä¶ V√† c√≤n ti t·ªâ th·ª© kh√°c m√† OG c√≥ k·ªÉ ƒë·∫øn rƒÉng long ƒë·∫ßu b·∫°c c√≥ l·∫Ω c≈©ng ch∆∞a h·∫øt üòÇ ","date":"30 Aug 2023","objectID":"/start_journey/:2:1","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#data-engineer-th√¨-c·∫ßn-bi·∫øt-g√¨-"},{"categories":[],"content":"T·∫°m k·∫øtH√†nh tr√¨nh n√†o khi b·∫Øt ƒë·∫ßu c≈©ng gian nan, c·∫£ b·∫£n th√¢n OG khi b·∫Øt ƒë·∫ßu c≈©ng kh√¥ng bi·∫øt g√¨ c·∫£. Nh∆∞ng khi nh·∫•c ng√≥n ch√¢n l√™n v√† ƒëi th√¨ m·ªõi c·∫£m nh·∫≠n ƒë∆∞·ª£c th·∫ø gi·ªõi ch·ª© üòÑ Hy v·ªçng b√†i vi·∫øt n√†y gi√∫p b·∫°n th∆∞ gi√£n v√† c√≥ m·ªôt c√°i nh√¨n chung v·ªÅ ng√†nh data nh√©. H·∫πn g·∫∑p l·∫°i trong b√†i ti·∫øp theo hehe -Meww ","date":"30 Aug 2023","objectID":"/start_journey/:3:0","series":[],"tags":[],"title":"Th·∫±ng nh√≥c th√≠ch code v√† data","uri":"/start_journey/#t·∫°m-k·∫øt"},{"categories":[],"content":"Gi·ªõi thi·ªáu v·ªÅ trang web ƒë√°ng y√™u n√†y","date":"30 Aug 2023","objectID":"/intro_blog/","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/"},{"categories":[],"content":"Hello! Hello! m√¨nh l√† OG ƒë√¢y. ƒê√¢y l√† b√†i blog ƒë·∫ßu ti√™n c·ªßa m√¨nh ·ªü ƒë√¢y. C√≥ th·ªÉ b·∫°n ƒëang t·ª± h·ªèi r·∫±ng m√¨nh l√† ai v√† ƒë√¢y l√† n∆°i n√†o ƒë√∫ng kh√¥ng, v·∫≠y h√£y c√πng m√¨nh t√¨m hi·ªÉu th·ª≠ nh√© ","date":"30 Aug 2023","objectID":"/intro_blog/:0:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#"},{"categories":[],"content":"OG l√† ai ?M·ªôt l·∫ßn n·ªØa gi·ªõi thi·ªáu m√¨nh t√™n l√† Vƒ©nh Phong - m·ªôt anh ch√†ng th√≠ch t√¨m t√≤i ƒëi·ªÅu m·ªõi‚Ä¶ hmmm th·∫ø th√¥i :))) ƒê·ªÉ hi·ªÉu th√™m v·ªÅ m√¨nh: About C√≤n OG (nickname) l√† bi·ªát danh m√¨nh l·∫•y c·∫£m h·ª©ng t·ª´ m·ªôt nh√¢t v·∫≠t ho·∫°t h√¨nh r·∫•t h√≥m h·ªânh ƒë·∫•y nh√© hehe. ƒê√≥ l√† t√™n m·ªôt ch√∫ m√®o xanh d∆∞∆°ng hay b·ªã m·∫•y con gi√°n qu·∫≠y :)) b·∫°n th·ª≠ ƒëo√°n xem ƒê√°p √°n ·ª¶a l·ªôn n√†y l√† tui:))Tui \" ·ª¶a l·ªôn n√†y l√† tui:)) ƒê√¢y m·ªõi l√† ƒë√°p √°n N·∫øu b·∫°n hong bi·∫øt, th√¨ con m√®o xanh l√® n√†y t√™n l√† Oggy v√† nickname m√¨nh c≈©ng v·∫≠y :)) Bi·∫øt m√¨nh l√† ai r·ªìi, th·∫ø th√¨‚Ä¶ ","date":"30 Aug 2023","objectID":"/intro_blog/:1:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#OG-la-ai"},{"categories":[],"content":"ƒê√¢y l√† n∆°i n√†o ƒë√¢y ?ƒê√¢y trang m√† m√¨nh ƒëƒÉng l√™n c√°c b√†i Blogs v·ªÅ c√¥ng ngh·ªá, v·ªÅ ng√†nh Data n√≥i chung v√† v·ªÅ h√†nh tr√¨nh h·ªçc t·∫≠p c·ªßa OG ƒë·ªÉ tr·ªü th√†nh m·ªôt Data Engineer trong t∆∞∆°ng lai. T·∫•t nhi√™n kh√¥ng ch·ªâ nh∆∞ v·∫≠y M√¨nh c≈©ng vi·∫øt blogs v·ªÅ ƒë·ªùi s·ªëng, v·ªÅ nh·ªØng ƒëi·ªÅu th√∫ v·ªã c·ªßa cu·ªôc s·ªëng xung quanh V√† m√¨nh hy v·ªçng trang c≈©ng n√†y s·∫Ω l√† n∆°i mang l·∫°i s·ª± tho·∫£i m√°i v√† th∆∞ gi·∫£n cho m·ªçi ng∆∞·ªùi ","date":"30 Aug 2023","objectID":"/intro_blog/:2:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#day-la-dau"},{"categories":[],"content":"Th·∫ø ·ªü ƒë√¢y c√≥ g√¨ hay?T√≥m t·∫Øt c√°c trang: Blogs: b·∫°n c√≥ th·ªÉ t√¨m th·∫•y c√°c b√†i blogs m√¨nh vi·∫øt ·ªü ƒë√¢y Projects: Nh·ªØng d·ª± √°n m√¨nh ƒë√£ l√†m About: N·∫øu b·∫°n mu·ªën hi·ªÉu th√™m v·ªÅ m√¨nh ƒê√≥ l√† t·ªïng quan ‚Äúc√°c th·ª© c√≥ th·ªÉ s·∫Ω xu·∫•t hi·ªán‚Äù ·ªü trang web n√†y. ","date":"30 Aug 2023","objectID":"/intro_blog/:2:1","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#th·∫ø-·ªü-ƒë√¢y-c√≥-g√¨-hay"},{"categories":[],"content":"Cu·ªëi c√πngOG hy v·ªçng ƒë√¢y s·∫Ω l√† n∆°i gi√∫p b·∫°n th∆∞ gi√£n hay h·ªçc h·ªèi ƒë∆∞·ª£c nhi·ªÅu ƒëi·ªÅu th√∫ v·ªã nh√© üòÑ -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Hu·ª≥nh L∆∞u Vƒ©nh Phong Facebook: Phong Huynh Instagram: phong_huynh Ho·∫∑c b·∫°n c≈©ng c√≥ th·ªÉ gh√© thƒÉm Github c·ªßa m√¨nh: PhongHuynh0394 ","date":"30 Aug 2023","objectID":"/intro_blog/:3:0","series":[],"tags":[],"title":"Intro: Tui l√† ai ? ƒê√¢y l√† ƒë√¢u ?","uri":"/intro_blog/#cuoi-cung"},{"categories":null,"content":"Continuous of Football ETL series","date":"01 Aug 2023","objectID":"/football_etl_2/","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/"},{"categories":null,"content":"Hello! Hello! OG ƒë√¢y, sau ph·∫ßn 1 ch√∫ng ta ƒë√£ setup c√°c ki·ªÉu v√† ƒë·∫£m b·∫£o m·ªçi th·ª© tr∆°n tru r·ªìi, ·ªü ph·∫ßn n√†y ch√∫ng ta s·∫Ω chu·∫©n b·ªã Data Source, v√† kh·ªüi ch·∫°y pipeline ·ªü Implement sau ƒë√≥ s·∫Ω Visualize cleaned data c√≥ ƒë∆∞·ª£c t·ª´ data warehouse th√†nh Dashboard. B·∫Øt ƒë·∫ßu th√¥i n√†o ! ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#"},{"categories":null,"content":"Data Source","date":"01 Aug 2023","objectID":"/football_etl_2/:1:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#data-source"},{"categories":null,"content":"Chu·∫©n b·ªã file l√†m raw dataC√°c file csv s·ª≠ d·ª•ng l√†m d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i t·ª´ Football Database - Kaggle. ƒê√¢y l√† d·ªØ li·ªáu th·ªëng k√™ c·ªßa c·∫ßu th·ªß, ƒë·ªôi b√≥ng ƒë·∫øn t·ª´ 5 gi·∫£i b√≥ng h√†ng ƒë·∫ßu Ch√¢u √Çu (Premier League, Laliga, Seria A, Budesliga, League 1) Ta s·∫Ω c√≥ schema nh∆∞ sau: SchemaSchema \" Schema trong ƒë√≥: games: b·∫£ng ch·ª©a th√¥ng tin th·ªëng k√™ c·ªßa t·ª´ng tr·∫≠n ƒë·∫•u (gameID) teams: B·∫£ng ch·ª©a t√™n c√°c ƒë·ªôi b√≥ng (teamID) players: B·∫£ng ch·ª©a t√™n c√°c c·∫ßu th·ªß (playerID) leagues: B·∫£ng ch∆∞a t√™n c√°c gi·∫£i ƒë·∫•u (leagueID) appearances: B·∫£ng th·ªëng k√™ c·ªßa c·∫ßu th·ªß ·ªü c√°c game m√† h·ªç tham gia (gameID, playerID) teamstats: B·∫£ng th·ªëng k√™ c·ªßa ƒë·ªôi b√≥ng ·ªü t·ª´ng game (gameID, teamID) ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#chu·∫©n-b·ªã-file-l√†m-raw-data"},{"categories":null,"content":"Load data v√†o MySQLC√≥ nhi·ªÅu c√°ch ƒë·ªÉ load data v√†o MySQL, ·ªü ƒë√¢y m√¨nh s·∫Ω s·ª≠ d·ª•ng c√°ch LOAD LOCAL_INFILE c·ªßa MySQL lu√¥n. Tip H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ make up l·∫ßn ƒë·∫ßu r·ªìi nh√© ! H√£y copy folder ch·ª©a c√°c file csv v√†o de_mysql container: docker cp /football de_mysql:/tmp/dataset/ docker cp /load_data de_mysql:/tmp/dataset/ Sau ƒë√≥ t·∫°o b·∫£ng tr·ªëng s·∫µn trong MySQL: make mysql_create #Create table in mysql Ti·∫øp t·ª•c v·ªõi l·ªánh: make to_mysql_root # ----- You will access to MySQL container SET GLOBAL LOCAL_INFILE=TRUE; #Set local_infile variable to load data from local exit; # ----- Exit container make mysql_load #load data make mysql_create_relation #create table relation Th·∫ø l√† ƒë√£ chu·∫©n b·ªã xong d·ªØ li·ªáu cho MySQL. ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load-data-v√†o-mysql"},{"categories":null,"content":"Init PostgreSQL SchemaTa c≈©ng c·∫ßn ph·∫£i t·∫°o s·∫µn schema s·∫µn trong Posgres nh∆∞ sau: make to_psql CREATE SCHEMA IF NOT EXISTS analysis; exit; Th·∫ø l√† ƒë√£ ho√†n t·∫•t vi·ªác chu·∫©n b·ªã data, gi·ªù th√¨ ta b·∫Øt ƒë·∫ßu v√†o ph·∫ßn vi·ªác ch√≠nh th√¥i ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#init-postgresql-schema"},{"categories":null,"content":"ImplementC√¥ng vi·ªác ch√≠nh trong ph·∫ßn n√†y l√† x√¢y d·ª±ng c√°c data pipeline b·∫±ng dagster. C∆° b·∫£n c√≥ th·ªÉ hi·ªÉu l√† ta t·∫°o c√°c Asset v√† chuy·ªÉn ch√∫ng t·ª´ database n√†y sang database kh√°c. ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#implement"},{"categories":null,"content":"Extractionƒê·ªÉ c√≥ th·ªÉ qu·∫£n l√Ω vi·ªác truy xu·∫•t d·ªØ li·ªáu t·ª´ MySQL v√† load v√†o MinIO ƒë·ªÉ l∆∞u t·∫°m, ta s·∫Ω x√¢y d·ª±ng m·ªôt I/O Manager ph·ª•c v·ª• vi·ªác ƒë√≥. ƒê·∫ßu ti√™n, h√£y v√†o ƒë∆∞·ªùng d·∫´n: ./etl_pipeline/etl_pipeline/resources/ Ta s·∫Ω x√¢y d·ª±ng MySQL io manager b·∫±ng c√°ch t·∫°o file mysql_io_manager.py v·ªõi n·ªôi dung sau: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_mysql(config): conn_info = ( f\"mysql+pymysql://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class MySQLIOManager(IOManager): def __init__(self, config): self.config = config def handle_output(self, context: OutputContext, obj: pd.DataFrame): pass def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def extract_data(self, sql: str) -\u003e pd.DataFrame: with connect_mysql(self.config) as db_conn: pd_data = pd.read_sql_query(sql, db_conn) return pd_data Sau ƒë√≥, ti·∫øp t·ª•c ƒë·ªëi v·ªõi minio_io_manager.py: import os from contextlib import contextmanager from datetime import datetime from typing import Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from dagster import IOManager, InputContext, OutputContext from minio import Minio @contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\"endpoint_url\"), access_key=config.get(\"aws_access_key_id\"), secret_key=config.get(\"aws_secret_access_key\"), secure=False ) try: yield client except Exception: raise class MinIOIOManager(IOManager): def __init__(self, config): self._config= config def _get_path(self, context: Union[InputContext, OutputContext]): layer, schema, table = context.asset_key.path key = \"/\".join([layer, schema, table.replace(f\"{layer}_\", \"\")]) tmp_file_path = \"/tmp/file-{}-{}.parquet\".format( datetime.today().strftime(\"%Y%m%d%H%M%S\"), \"-\".join(context.asset_key.path) ) if context.has_asset_partitions: start, end = context.asset_partitions_time_window dt_format = \"%Y%m%d%H%M%S\" partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format) return os.path.join(key, f\"{partition_str}.pq\"), tmp_file_path else: return f\"{key}.pq\", tmp_file_path def handle_output(self, context: OutputContext, obj: pd.DataFrame): # convert to parquet format key_name, tmp_file_path = self._get_path(context) table = pa.Table.from_pandas(obj) pq.write_table(table, tmp_file_path) # upload to MinIO try: bucket_name = self._config.get(\"bucket\") with connect_minio(self._config) as client: # Make bucket if not exist. found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exists\") client.fput_object(bucket_name, key_name, tmp_file_path) row_count = len(obj) context.add_output_metadata({\"path\": key_name, \"tmp\": tmp_file_path}) # clean up tmp file os.remove(tmp_file_path) except Exception: raise def load_input(self, context: InputContext) -\u003e pd.DataFrame: bucket_name = self._config.get(\"bucket\") key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: #Make bucket if not exist found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exist\") client.fget_object(bucket_name, key_name, tmp_file_path) pd_data = pd.read_parquet(tmp_file_path) return pd_data except Exception: raise Sau khi ƒë√£ t·∫°o th√†nh c√¥ng c√°c io manager cho mysql v√† minio, ta s·∫Ω b·∫Øt ƒë·∫ßu x√¢y d·ª±ng bronze layer Note nho nh·ªè Trong project n√†y m√¨nh chia c√°c giai ƒëo·∫°n transformation th√†nh c√°c layer: bronze layer: Giai ƒëo·∫°n ch·ªè m·ªõi load raw data, c√≥ th·ªÉ hi·ªÉu ƒë√¢y l√† data ch∆∞a transform g√¨ c·∫£ siler layer: Transform m·ªôt ph·∫ßn t·ª´ bronze layer, ·ªü ƒëo·∫°n n√†y data ƒë√£ ƒë∆∞·ª£c cleaning s∆° gold layer: Sau khi transform m·ªôt l·∫ßn n·ªØa t·ª´ silver layer, giai ƒëo·∫°n n√†y s·∫Ω truy v·∫•n ra c√°c th√¥n","date":"01 Aug 2023","objectID":"/football_etl_2/:2:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#extraction"},{"categories":null,"content":"TransformationTi·∫øp t·ª•c t·∫°o file silver_layer.py c√πng folder v·ªõi bronze layer: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teamstats\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"leagues\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], description='Statistic of teams in games', group_name=\"Silver_layer\", compute_kind=\"Pandas\" ) def silver_statsTeamOnGames(teamstats: pd.DataFrame, games: pd.DataFrame, leagues: pd.DataFrame) -\u003e Output[pd.DataFrame]: ts = teamstats.copy() gs = games.copy() lgs = leagues.copy() #Drop unsusable columns in games gs.drop(columns=gs.columns.to_list()[13:], inplace=True) #create StatperLeagueSeason result = pd.merge(ts, gs, on=\"gameID\") result = result.merge(lgs, on=\"leagueID\", how=\"left\") result.drop(columns=['season_y', 'date_y'],inplace=True) result = result.rename(columns={'season_x': 'season', 'date_x': 'date'}) return Output( result, metadata={ \"table\": \"statsTeamOnGames\", \"records\": len(result) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"appearances\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"players\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='statistic of players in games', compute_kind=\"Pandas\" ) def silver_playerAppearances(appearances: pd.DataFrame, games: pd.DataFrame, players: pd.DataFrame) -\u003e Output[pd.DataFrame]: app = appearances.copy() ga = games.copy() pla = players.copy() #Drop unusable column ga.drop(columns=ga.columns.to_list()[13:], inplace=True) #Merge player_appearances = pd.merge(app, pla, on=\"playerID\", how=\"left\") player_appearances = pd.merge(player_appearances, ga, on=\"gameID\", how=\"left\") #drop unecessary columns and rename player_appearances.drop(columns=['leagueID_y'],inplace=True) player_appearances.rename(columns={'leagueID_x': 'leagueID'}, inplace=True) return Output( player_appearances, metadata={ \"table\": \"playerAppearances\", \"records\": len(player_appearances) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teams\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='Teams', compute_kind=\"Pandas\" ) def silver_teams(teams: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( teams, metadata={ \"table\": 'teams', 'records': len(teams) } ) L√∫c n√†y m√¨nh c√≥ 3 silver assets, ƒë∆∞·ª£c join t·ª´ c√°c b·∫£ng ·ªü bronze Ti·∫øp ƒë·∫øn l√† gold_layer, l√∫c n√†y ta s·∫Ω t√≠nh c√°c th√¥ng s·ªë th·ªëng k√™ c·ªßa t·ª´ng gi·∫£i ƒë√¢u t·ª´ng m√πa, c√°c th·ªëng k√™ c·ªßa c·∫ßu th·ªß trong 90 ph√∫t thi ƒë·∫•u, v√† c·∫£ th·ªëng k√™ c·ªßa t·ª´ng c·∫ßu th·ªß trong t·ª´ng m√πa gi·∫£i gold_layer.py s·∫Ω c√≥ n·ªôi dung: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_statsTeamOnGames\": AssetIn( key_prefix=[\"football\", \"silver\"] ) }, group_name=\"Gold_layer\", key_prefix=[\"football\", \"gold\"], description='Statistic of all league in each season', compute_kind=\"Pandas\" ) def gold_statsPerLeagueSeason(silver_statsTeamOnGames: pd.DataFrame) -\u003e Output[pd.DataFrame]: st = silver_statsTeamOnGames.copy() result = ( st.groupby(['name', 'season']) .agg({\"goals\": \"sum\", \"xGoals\": \"sum\", \"shots\": \"sum\", \"shotsOnTarget\": \"sum\", \"fouls\": \"sum\", \"yellowCards\": \"sum\", \"redCards\": \"sum\",'corners': 'sum', \"gameID\": 'count'}) .reset_index() ) result = result.rename(columns={'gameID':\"games\"}) result['goalPerGame']= result.goals/result.games result['season'] = result['season'].astype('string') return Output( result, metadata={ 'table': 'statPerLeagueSeason', 'records': len(result) } ) @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_playerAppearances\": AssetIn( key_prefix=[\"football\", \"silver\"] ) },","date":"01 Aug 2023","objectID":"/football_etl_2/:2:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#transformation"},{"categories":null,"content":"LoadTr∆∞·ªõc h·∫øt h√£y t·∫°o IO Manager cho Postgres ƒë·ªÉ qu·∫£n l√Ω vi·ªác load cleaned data. Ta t·∫°o file psql_io_manager.py ·ªü v·ªã tr√≠ m√† ta ƒë√£ t·∫°o 2 io manager tr∆∞·ªõc v·ªõi n·ªôi dung: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_psql(config): conn_info = ( f\"postgresql+psycopg2://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class PostgreSQLIOManager(IOManager): def __init__(self, config): self._config = config def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def handle_output(self, context: OutputContext, obj: pd.DataFrame): schema, table = context.asset_key.path[-2], context.asset_key.path[-1] with connect_psql(self._config) as conn: # insert new data ls_columns = (context.metadata or {}).get(\"columns\", []) obj[ls_columns].to_sql( name=f\"{table}\", con=conn, schema=schema, if_exists=\"replace\", index=False, chunksize=10000, method=\"multi\" ) Sau ƒë√≥, t·∫°o m·ªôt asset warehouse_layer.py: from dagster import multi_asset, Output, AssetIn, AssetOut, asset import pandas as pd @multi_asset( ins={ \"gold_statsPerLeagueSeason\": AssetIn( key_prefix=[\"football\", \"gold\"] ) }, outs={ \"statsperleagueseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerLeagueSeason\", 'analysis'], metadata={ \"columns\": [ \"name\", \"season\", \"goals\", \"xGoals\", \"shots\", \"shotsOnTarget\", \"fouls\", \"yellowCards\", \"redCards\", \"corners\", \"games\", \"goalPerGame\" ] } ), }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerLeagueSeason(gold_statsPerLeagueSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerLeagueSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerLeagueSeason\", \"records\": len(gold_statsPerLeagueSeason) } ) @multi_asset( ins={ \"gold_statsPerPlayerSeason\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsperplayerseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerPlayerSeason\", 'analysis'], metadata={ \"columns\": [ \"playerID\", \"name\", \"season\", \"goals\", \"shots\", \"xGoals\", \"xGoalsChain\", \"xGoalsBuildup\", \"assists\", \"keyPasses\", \"xAssists\", \"gDiff\", \"gDiffRatio\" ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerPlayerSeason(gold_statsPerPlayerSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerPlayerSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerPlayerSeason\", \"records\": len(gold_statsPerPlayerSeason) } ) @multi_asset( ins={ \"gold_statsPlayerPer90\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsplayerper90\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPlayerPer90\", 'analysis'], metadata={ \"columns\": [ 'playerID', 'name', 'total_goals', 'total_assists', 'total_time', 'goalsPer90', 'assistsPer90', 'scorers' ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPlayerPer90(gold_statsPlayerPer90: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPlayerPer90, metadata={ \"schema\": \"analysis\", \"table\": \"statsPlayerPer90\", \"records\": len(gold_statsPlayerPer90) } ) ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load"},{"categories":null,"content":"Run systemCu·ªëi c√πng, ta s·∫Ω k·∫øt h·ª£p t·∫•t c·∫£ c√°c asset l·∫°i gi√∫p dagster nh·∫≠n di·ªán v√† qu·∫£n l√Ω v·ªõi file __init__.py ·ªü etl_pipeline/etl_pipeline/__init__.py import os from dagster import Definitions from .assets.silver_layer import * from .assets.gold_layer import * from .assets.bronze_layer import * from .assets.warehouse_layer import * from .resources.minio_io_manager import MinIOIOManager from .resources.mysql_io_manager import MySQLIOManager from .resources.psql_io_manager import PostgreSQLIOManager MYSQL_CONFIG = { \"host\": os.getenv(\"MYSQL_HOST\"), \"port\": os.getenv(\"MYSQL_PORT\"), \"database\": os.getenv(\"MYSQL_DATABASE\"), \"user\": os.getenv(\"MYSQL_USER\"), \"password\": os.getenv(\"MYSQL_PASSWORD\") } MINIO_CONFIG = { \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\"), \"bucket\": os.getenv(\"DATALAKE_BUCKET\"), \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"), \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\") } PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } ls_asset=[asset_factory(table) for table in tables] + [silver_statsTeamOnGames, silver_teams , silver_playerAppearances, gold_statsPerLeagueSeason, gold_statsPerPlayerSeason, gold_statsPlayerPer90, statsPerLeagueSeason, statsPerPlayerSeason, statsPlayerPer90] defs = Definitions( assets=ls_asset, resources={ \"mysql_io_manager\": MySQLIOManager(MYSQL_CONFIG), \"minio_io_manager\": MinIOIOManager(MINIO_CONFIG), \"psql_io_manager\": PostgreSQLIOManager(PSQL_CONFIG), } ) sau ƒë√≥ h√£y d√πng l·ªánh sau ƒë·ªÉ c·∫≠p nh·∫≠t c√°c assets docker restart etl_pipeline ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:4","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#run-system"},{"categories":null,"content":"Check UIH√£y ki·ªÉm tra Dagit UI ·ªü localhost:3001 ƒë·ªÉ ch·∫Øc ch·∫Øn r·∫±ng m·ªçi th·ª© v·∫´n ·ªïn Ngo√†i ra c≈©ng c√≥ th·ªÉ check MinIO: localhost:9000 ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:5","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#check-ui"},{"categories":null,"content":"VisualizationCu·ªëi c√πng l√† v·∫Ω dashboard, ƒë·∫ßu ti√™n ta c·∫ßn ph·∫£i l·∫•y ƒë∆∞·ª£c data t·ª´ psql, h√£y v√†o t·∫°o file: ./streamlit/src/psql_connect.py: import os import psycopg2 from dotenv import load_dotenv import pandas as pd #load environment load_dotenv() #list table in database table = ['statsperleagueseason','statsperplayerseason', 'statsplayerper90'] PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } #create connection def init_connection(config): return psycopg2.connect( database=config['database'], user=config['user'], password=config['password'], host=config['host'], port=config['port'] ) def extract_data(): conn = init_connection(PSQL_CONFIG) return [pd.read_sql(f'SELECT * FROM analysis.{tab}', conn) for tab in table] Cu·ªëi c√πng l√† t·∫°o main.py ngay trong th∆∞ m·ª•c scr: import streamlit as st import pandas as pd import plotly.express as px import plotly.graph_objects as go from psql_connect import extract_data import numpy as np # #extract data from PostgreSQL ls_df = extract_data() l_season = ls_df[0] p_season = ls_df[1] p_match = ls_df[2] st.set_page_config(page_title = 'Dashboard Football', layout='wide', page_icon='chart_with_upwards_trend') #Overview def overview(table: pd.DataFrame, detail: str): if (st.checkbox('Do you want to see Data ?')): table col1, col2 = st.columns(2) co_df = table.columns.to_list() with col1: st.bar_chart(table.describe()) if (st.checkbox('Do you want to see describe each column ?')): for col in co_df: if table[col].dtypes not in ['int64', 'float64']: continue st.bar_chart(table[col].describe()) with col2: st.caption(f':red[Columns]: {len(co_df)}') st.caption(f':red[Records]: {len(table)}') st.caption(f':red[Description]: {detail}') st.caption(f':red[Columns name]:{co_df}') #league statistic def statleague(): Cards = l_season[['name','season','yellowCards', 'redCards', 'fouls']] #Card_fouls col1, col2 = st.columns(2) with col1: #Goals per games fig = px.bar(l_season, x=\"name\", y=\"goalPerGame\", color=\"name\", barmode=\"stack\", facet_col=\"season\", labels={\"name\": \"League\", \"goals/games\": \"GPG\"}) fig.update_layout(showlegend=False, title='Goals per Game') st.plotly_chart(fig) #fouls fig = px.line(Cards, x='season', y='fouls', color='name') fig.update_layout(title='Fouls of leagues', xaxis_title='Season', yaxis_title='Fouls', legend_title='League') st.plotly_chart(fig) with col2: #Red card fig = px.line(Cards, x='season', y='redCards', color='name') fig.update_layout(title='Red Cards of leagues', xaxis_title='Season', yaxis_title='RedCards', legend_title='League') st.plotly_chart(fig) #yellow card fig = px.line(Cards, x='season', y='yellowCards', color='name') fig.update_layout(title='Yellow Cards of leagues', xaxis_title='Season', yaxis_title='YellowCards', legend_title='League') st.plotly_chart(fig) #Player statistic def statplayer(): col1, col2 = st.columns(2) with col1: #Best offensive player top_player90= p_match[(p_match['goalsPer90'] \u003e 0.8) | (p_match['assistsPer90'] \u003e 0.4)] fig = px.scatter(p_match[['name','goalsPer90', 'assistsPer90']], x='goalsPer90', y='assistsPer90', hover_name='name') fig.add_trace( go.Scatter(x=top_player90['goalsPer90'], y=top_player90['assistsPer90'], mode='markers+text', marker_size=5, text=top_player90['name'], textposition='bottom center', textfont=dict(size=15)) ) fig.update_layout(title='Best offensive Players (2018-2020)', xaxis_title='Goals Per 90min', yaxis_title='Assists Per 90min') st.plotly_chart(fig) #goals-xgoal fig = px.scatter(p_season, x=\"xGoals\", y=\"goals\", color=(p_season['xGoals'] - p_season['goals'] \u003c 10), color_discrete_sequence=[\"red\", \"green\"], opacity=0.5) fig.update_layout(title=\"Goals (G) and Expected Goals (xG)\", xaxis_title=\"xG\", yaxis_title=\"G\", ) st.plotly_chart(fig) with col2: #Top score player topPlayer = p_season.groupby(['name']).agg({'goals': 'sum'}).sort_values('goals', ascending=False).res","date":"01 Aug 2023","objectID":"/football_etl_2/:3:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#visualization"},{"categories":null,"content":"ConclusionCu·ªëi c√πng c≈©ng ƒë√£ xong m·ªôt project x√¢y d·ª±ng data pipeline c∆° b·∫£n, trong l√∫c th·ª±c hi·ªán ch·∫Øc ch·∫Øn s·∫Ω c√≥ c·∫£ t·∫•n l·ªói x·∫£y ra, nh∆∞ng OG tin l√† m·ªçi gian kh√≥ ƒë·ªÅu s·∫Ω v∆∞·ª£t quan ƒë∆∞·ª£c, th·ª© ƒë·ªçng l·∫°i ch√≠nh l√† ki·∫øn th·ª©c v√† k·ªπ nƒÉng c·ªßa ch√∫ng ta. Ch√∫c b·∫°n th√†nh c√¥ng v√† ƒë√≥n xem ti·∫øp c√°c d·ª± √°n ti·∫øp theo c·ªßa OG nh√© ! -Mew- ","date":"01 Aug 2023","objectID":"/football_etl_2/:4:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#conclusion"},{"categories":null,"content":"Related Content Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#related-content"},{"categories":["projects"],"content":"A Data Engineer project building pipeline to analyze football data","date":"31 Jul 2023","objectID":"/football_etl/","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/"},{"categories":["projects"],"content":"Source code: Football_ETL_Analysis ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#"},{"categories":["projects"],"content":"IntroduceTrong project n√†y, OG s·∫Ω build end-to-end ETL data pipeline ho√†n ch·ªânh ƒë·ªÉ ph√¢n t√≠ch football data t·ª´ Kaggle v·ªõi data pipeline nh∆∞ sau: Data pipelinedata pipeline \" Data pipeline C√°c b∆∞·ªõc c·ª• th·ªÉ: Set up: D√πng Docker t·∫°o container v√† c√°c images c·∫ßn thi·∫øt cho pipeline, trong ƒë√≥ c√≥ c·∫£ Dagster d√πng x√¢y d·ª±ng pipeline. Chu·∫©n b·ªã data source: load c√°c file csv (c√≥ ƒë∆∞·ª£c t·ª´ Kaggle) v√†o MySQL nh·∫±m m·ª•c ƒë√≠ch l∆∞u tr·ªØ raw data (m√¥ ph·ªèng source data) Extract: L·∫•y data t·ª´ MySQL v√† load v√†o MinIO chu·∫©n b·ªã cho b∆∞·ªõc transform Transform: S·ª≠ d·ª•ng Pandas ƒë·ªÉ truy v·∫•n c√°c file t·ª´ MinIO Load: Cleaned v√† transformed data ƒë∆∞·ª£c load v√†o warehouse PostgreSQL Visualization: S·ª≠ d·ª•ng Streamlit ƒë·ªÉ l√†m Dashboard ","date":"31 Jul 2023","objectID":"/football_etl/:1:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#introduce"},{"categories":["projects"],"content":"Set upB·∫Øt ƒë·∫ßu v·ªõi Docker, ta s·∫Ω x√¢y d·ª±ng l·∫ßn l∆∞·ª£t t·ª´ng image b·∫±ng c√°ch vi·∫øt docker-compose.yml Tip nho nh·ªè H√£y pull/build l·∫ßn l∆∞·ª£t t·ª´ng lo·∫°i framework l·∫ßn l∆∞·ª£t ƒë·ªÉ ch·∫Øc ch·∫Øn r·∫±ng ch√∫ng ho·∫°t ƒë·ªông tr∆°n tru nh·∫•t tr∆∞·ªõc Ho·∫∑c b·∫°n c≈©ng c√≥ th·ªÉ xem lu√¥n ph·∫ßn ho√†n ch·ªânh Ho√†n ch·ªânh set up ","date":"31 Jul 2023","objectID":"/football_etl/:2:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#set-up"},{"categories":["projects"],"content":"MinIOMinIO l√† m·ªôt server l∆∞u tr·ªØ ƒë·ªëi t∆∞·ª£ng d·∫°ng ph√¢n t√°n v·ªõi hi·ªáu nƒÉng cao v√† cung c·∫•p c√°c api gi·ªëng v·ªõi Amazon S3, ta c√≥ th·ªÉ upload, download file,‚Ä¶ m·ªôt c√°ch ƒë∆°n gi·∫£n. minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_network Note V·ªÅ .env file, ƒë√¢y l√† file ch·ª©a th√¥ng tin c√°c bi·∫øn m√¥i tr∆∞·ªùng thi·∫øt l·∫≠p cho t·ª´ng image, m√¨nh s·∫Ω n√≥i sau ","date":"31 Jul 2023","objectID":"/football_etl/:2:1","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#minio"},{"categories":["projects"],"content":"MySQLMySQL l√† m·ªôt trong s·ªë c√°c ph·∫ßn m·ªÅm RDBMS (Relational DataBase Management Systems) ph·ªï bi·∫øn nh·∫•t, ta s·∫Ω s·ª≠ d·ª•ng database n√†y ƒë·ªÉ l∆∞u raw data m√¥ ph·ªèng cho source data c·∫ßn ingest de_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:2","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#mysql"},{"categories":["projects"],"content":"PostgeSQLPostgreSQL l√† m·ªôt h·ªá th·ªëng qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu quan h·ªá-ƒë·ªëi t∆∞·ª£ng (object-relational database management system), v√† ta s·∫Ω dung n√≥ l√†m data warehouse cho project l·∫ßn n√†y. de_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:3","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#postgesql"},{"categories":["projects"],"content":"DagsterDagster l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü h·ªó tr·ª£ Orchestrate Task (qu·∫£n l√Ω, t·ªï ch·ª©c, ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• v√† c√¥ng vi·ªác), h·ªó tr·ª£ gi√∫p x√¢y d·ª±ng data pipeline kh√° t·ªët. M√¨nh th·∫•y c√¥ng c·ª• n√†y g·ªçn nh·∫π h∆°n Apache Airflow v√† c≈©ng ƒë∆°n gi·∫£n h∆°n. ƒê·∫ßu ti√™n h√£y vi·∫øt trong docker-compose.yml v·ªÅ dagster nh∆∞ sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster ƒê·ªëi v·ªõi Dagster, ta s·∫Ω t·ª± config b·∫±ng Dockerfile, h√£y t·∫°o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau ƒë√≥ t·∫°o Dockerfile c√≥ n·ªôi dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt v√† requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit l√† giao di·ªán UI c·ªßa Dagster, v√† ta s·∫Ω thao t√°c tr√™n Dagit ƒë·ªÉ qu·∫£n l√Ω c√°c assets, jobs,‚Ä¶ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau ƒë√≥ h√£y t·∫°o 1 folder ./dagster_home, trong ƒë√≥ t·∫°o 2 file config cho workspace c·ªßa dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage v√† workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster Deamonƒê·ªÉ qu·∫£n l√Ω c√°c Schedules, sensors,‚Ä¶ ta c·∫ßn ph·∫£i c√≥ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster"},{"categories":["projects"],"content":"DagsterDagster l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü h·ªó tr·ª£ Orchestrate Task (qu·∫£n l√Ω, t·ªï ch·ª©c, ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• v√† c√¥ng vi·ªác), h·ªó tr·ª£ gi√∫p x√¢y d·ª±ng data pipeline kh√° t·ªët. M√¨nh th·∫•y c√¥ng c·ª• n√†y g·ªçn nh·∫π h∆°n Apache Airflow v√† c≈©ng ƒë∆°n gi·∫£n h∆°n. ƒê·∫ßu ti√™n h√£y vi·∫øt trong docker-compose.yml v·ªÅ dagster nh∆∞ sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster ƒê·ªëi v·ªõi Dagster, ta s·∫Ω t·ª± config b·∫±ng Dockerfile, h√£y t·∫°o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau ƒë√≥ t·∫°o Dockerfile c√≥ n·ªôi dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt v√† requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit l√† giao di·ªán UI c·ªßa Dagster, v√† ta s·∫Ω thao t√°c tr√™n Dagit ƒë·ªÉ qu·∫£n l√Ω c√°c assets, jobs,‚Ä¶ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau ƒë√≥ h√£y t·∫°o 1 folder ./dagster_home, trong ƒë√≥ t·∫°o 2 file config cho workspace c·ªßa dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage v√† workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster Deamonƒê·ªÉ qu·∫£n l√Ω c√°c Schedules, sensors,‚Ä¶ ta c·∫ßn ph·∫£i c√≥ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagit"},{"categories":["projects"],"content":"DagsterDagster l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü h·ªó tr·ª£ Orchestrate Task (qu·∫£n l√Ω, t·ªï ch·ª©c, ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• v√† c√¥ng vi·ªác), h·ªó tr·ª£ gi√∫p x√¢y d·ª±ng data pipeline kh√° t·ªët. M√¨nh th·∫•y c√¥ng c·ª• n√†y g·ªçn nh·∫π h∆°n Apache Airflow v√† c≈©ng ƒë∆°n gi·∫£n h∆°n. ƒê·∫ßu ti√™n h√£y vi·∫øt trong docker-compose.yml v·ªÅ dagster nh∆∞ sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster ƒê·ªëi v·ªõi Dagster, ta s·∫Ω t·ª± config b·∫±ng Dockerfile, h√£y t·∫°o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau ƒë√≥ t·∫°o Dockerfile c√≥ n·ªôi dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt v√† requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit l√† giao di·ªán UI c·ªßa Dagster, v√† ta s·∫Ω thao t√°c tr√™n Dagit ƒë·ªÉ qu·∫£n l√Ω c√°c assets, jobs,‚Ä¶ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau ƒë√≥ h√£y t·∫°o 1 folder ./dagster_home, trong ƒë√≥ t·∫°o 2 file config cho workspace c·ªßa dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage v√† workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster Deamonƒê·ªÉ qu·∫£n l√Ω c√°c Schedules, sensors,‚Ä¶ ta c·∫ßn ph·∫£i c√≥ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster-deamon"},{"categories":["projects"],"content":"PipelineT·∫•t c·∫£ m·ªçi vi·ªác x√¢y d·ª±ng pipeline ta s·∫Ω ho·∫°t ƒë·ªông ·ªü ƒë√¢y ƒê·∫ßu ti√™n ta c·∫ßn init m·ªôt dagster project dagster project scaffold --name etl_pipeline v√† th∆∞ m·ª•c m·ªõi t·∫°o s·∫Ω tr√¥ng nh∆∞ th·∫ø n√†y: Tip ƒê·ªÉ ch·∫°y ƒë∆∞·ª£c l·ªánh dagster ·ªü b∆∞·ªõc t·∫°o dagster project, ta c·∫ßn ph·∫£i c√≥ dagster package, n·∫øu ch∆∞a c√≥ h√£y c√†i ƒë·∫∑t b·∫±ng: pip install dagster ‚Äì\u003e N√™n c√†i ƒë·∫∑t trong m√¥i tr∆∞·ªùng ·∫£o Sau ƒë√≥ v√†o th∆∞ m·ª•c v·ª´a t·∫°o v√†o vi·∫øt Dockerfile th√¥i: FROMpython:3.10-slim# Add repository codeWORKDIR/opt/dagster/appCOPY requirements.txt /opt/dagster/appRUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txtWORKDIR/opt/dagster/appCOPY . /opt/dagster/app/etl_pipeline# CMD allows this to be overridden from run launchers or executors that want to run other commands against your repositoryCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"etl_pipeline\"] c√πng v·ªõi requirements.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-aws==0.17.20 dagster-dbt==0.17.20 pandas==1.5.3 SQLAlchemy==1.4.46 pymysql==1.0.2 cryptography==39.0.0 pyarrow==10.0.1 boto3==1.26.57 fsspec==2023.1.0 s3fs==0.4.2 minio==7.1.13 Cu·ªëi c√πng l√† vi·∫øt v√†o docker-compose.yml: etl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:5","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#pipeline"},{"categories":["projects"],"content":"StreamlitCu·ªëi c√πng l√† vi·ªác l√† Dashboard, Streamlit ch·∫Øc ch·∫Øc l√† c√¥ng c·ª• si√™u ph√π h·ª£p l√†m vi·ªác n√†y. ƒê√¢y l√† framework h·ªó tr·ª£ vi·ªác x√¢y d·ª±ng giao di·ªán ∆∞u nh√¨n ch·ªâ b·∫±ng Python, qu√° ƒë√£ ph·∫£i kh√¥ng n√†o :)) H√£y t·∫°o folder ./streamlit/scr/ c√πng v·ªõi ./streamlit/Dockerfile: FROMpython:3.10EXPOSE8501WORKDIR/usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . . v√† streamlit/requirements.txt: pandas plotly matplotlib numpy streamlit psycopg2-binary sqlalchemy python-dotenv Cu·ªëi c√πng l√† ghi trong yml streamlit:build:./streamymlcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:6","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#streamlit"},{"categories":["projects"],"content":"Ho√†n ch·ªânh setupCu·ªëi c√πng, file yaml s·∫Ω tr√¥ng nh∆∞ th·∫ø n√†y: # version: '3.9'services:minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_networkde_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_networkde_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_networkstreamlit:build:./streamlitcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_networkde_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagsterde_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_networkde_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network# Pipelinesetl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_networknetworks:de_network:driver:bridgename:de_network V√† .env file: # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_DB=football POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_HOST_AUTH_METHOD=trust # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=football # MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=football MYSQL_ROOT_PASSWORD=admin123 MYSQL_USER=admin MYSQL_PASSWORD=admin123 # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=warehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 Warning N·∫øu b·∫°n ch·ªâ ƒë·ªçc ph·∫ßn Ho√†n ch·ªânh setup th√¨ c√≥ th·ªÉ h·ªá th·ªëng v·ªÖ s·∫Ω g·∫∑p l·ªói v√¨ c√≥ th·ªÉ thi·∫øu c√°c configuration c·∫ßn thi·∫øt cho dagster, dagit hay pipeline. B·∫°n c·∫ßn ƒë·ªçc qua ph·∫ßn Dagster, Dagit, Pipeline Ch·∫°y th·ª≠: sau khi ho√†n t·∫•t to√†n b·ªô set up d√†i ngo·∫±n, c≈©ng ƒë√£ ƒë·∫øn l√∫c ch·∫°y ch∆∞∆°ng tr√¨nh th√¥i. Note nho nh·ªè N·∫øu b·∫°n ƒë√£ build l·∫ßn l∆∞·ª£t c√°c images r·ªìi, th√¨ ch·ªâ c·∫ßn compose up th√¥i, n·∫øu kh√¥ng, h√£y build b·∫±ng l·ªánh docker compose build tr∆∞·ªõc khi ch·∫°y compose up. docker compose --env-file .env up -d L·∫°i l√† m·ªôt tip c√≥ th·ªÉ h·ªØu √≠ch ƒê·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác ghi l·ªánh d√†i d√≤ng, h√£y t·∫°o m·ªôt make file t√™n Makefile v·ªõi n·ªôi dung sau: include .env build: docker compose build up: docker compose --env-file .env up -d down: docker compose --env-file .env down restart: make down \u0026\u0026 make up to_psql: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} psql_create: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} -f /tmp/psql_schema.sql to_mysql: docker exec -it de_mysql mysql --local-infile=1 -u\"${MYSQL_U","date":"31 Jul 2023","objectID":"/football_etl/:2:7","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#ho√†n-ch·ªânh-setup"},{"categories":["projects"],"content":"To be ContinueB√†i ƒë·∫øn ƒë√¢y c≈©ng qu√° l√† d√†i r·ªìi, m√¨nh s·∫Ω vi·∫øt ti·∫øp ·ªü ph·∫ßn 2 :))) Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh -Mew- ","date":"31 Jul 2023","objectID":"/football_etl/:3:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#to-be-continue"},{"categories":["projects"],"content":"Related Content Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... Football ETL Analysis P2 Continuous of Football ETL series Read more... ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#related-content"},{"categories":[],"content":"Data Engineering Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#data-engineering"},{"categories":[],"content":"Machine learning Basic Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#machine-learning-basic"},{"categories":null,"content":" Hello! M√¨nh t√™n l√† Vƒ©nh Phong hay ch√≠nh l√† OG (t√°c gi·∫£ c·ªßa c√°c blogs ·ªü trang n√†y) Hi·ªán t·∫°i m√¨nh l√† sinh vi√™n ng√†nh Khoa h·ªçc D·ªØ li·ªáu (Data Science) t·∫°i ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n, ƒêHQG-HCM (HCMUS). Tuy nhi√™n m√¨nh c≈©ng y√™u th√≠ch c√¥ng ngh·ªá, code v√† m√¨nh ƒëang tr√™n con ƒë∆∞·ªùng h·ªçc t·∫≠p m·ªói ng√†y ƒë·ªÉ tr·ªü th√†nh m·ªôt Data Engineer. OG h·ªìi cu·ªëi l·ªõp 12OG h·ªìi c√∫i l·ªõp 12 :)) \" OG h·ªìi cu·ªëi l·ªõp 12 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#"},{"categories":null,"content":"M√¨nh c·ªßa hi·ªán t·∫°iƒê·∫ßu ti√™n quan tr·ªçng nh·∫•t ch√≠nh l√† vi·ªác h·ªçc t·∫°i HCMUS. Ngo√†i ra, m√¨nh c√≤n t·ª± h·ªçc v·ªÅ c√°c ch·ªß ƒë·ªÅ li√™n quan nh∆∞ Data pipeline, Data Streaming,‚Ä¶ Vi·ªác luy·ªán t·∫≠p, h·ªçc h·ªèi c≈©ng ƒÉn s√¢u d√¥ m√°u m√¨nh l√∫c n√†o kh√¥ng hay :)) M√¨nh ƒë√£ t·ª´ng ƒë·ªçc th·∫•y ƒë√¢u ƒë√≥ r·∫±ng: The most beautiful thing about learning is that no one take that away from you V√† ƒëi·ªÅu ƒë√≥ ƒë√£ truy·ªÅn c·∫£m h·ª©ng m√¨nh r·∫•t nhi·ªÅu, th√∫c ƒë·∫©y b·∫£n th√¢n t·ª± h·ªçc m·ªói ng√†y v√† t·ª± l√†m m·ªõi b·∫£n th√¢n t·ª´ng ch√∫t m·ªôt. Ti·∫øp ƒë·∫øn ch√≠nh l√† x√¢y d·ª±ng trang web n√†y v√† vi·∫øt c√°c b√†i blogs gi√∫p cho c√°c b·∫°n c√≥ th·ªÉ h·ªçc th√™m ki·∫øn th·ª©c ng√†nh, bi·∫øt th√™m ƒëi·ªÅu th√∫ v·ªã v√† th∆∞ gi√£n. Hi·ªán t·∫°i th√¨ OG v·∫´n c√≤n ng·ªìi tr√™n gh·∫ø gi·∫£ng ƒë∆∞·ªùng, v√† v·ª´a ƒë·∫∑t nh·ªØng vi√™n g·∫°ch ƒë·∫ßu ti√™n tr√™n con ƒë∆∞·ªùng t·ª± tr∆∞·ªüng th√†nh. H√£y lu√¥n theo d√µi OG nh√©! -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Hu·ª≥nh L∆∞u Vƒ©nh Phong Facebook: Phong Huynh Instagram: phong_huynh Ho·∫∑c b·∫°n c≈©ng c√≥ th·ªÉ gh√© thƒÉm Github c·ªßa m√¨nh: PhongHuynh0394 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#m√¨nh-c·ªßa-hi·ªán-t·∫°i"}]