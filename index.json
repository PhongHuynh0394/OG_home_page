[{"categories":[],"content":"Trải nghiệm làm Data Engineer tại Bosch","date":"26 Jul 2024","objectID":"/workatbosch/","series":[],"tags":[],"title":"Bosch và cuộc hành trình mới","uri":"/workatbosch/"},{"categories":[],"content":"Lời cảm ơn ❤️ Lời đầu tiên, mình muốn gửi lời cảm ơn đến các anh chị tại Bosch R\u0026D Center đã luôn giúp đỡ, hỗ trợ OG mọi lúc trong suốt quá trình OG học tập và làm việc tại đây. OG đã biết và đã có nhiều trải nghiệm tuyệt vời phần lớn là có sự giúp đỡ tận tình của mọi người. Thật tốt khi luôn có mọi người ở bên và động viên OG trong cả chuyến đi. Thực sự rất yêu quý tất cả các anh chị ❤️ ","date":"26 Jul 2024","objectID":"/workatbosch/:0:0","series":[],"tags":[],"title":"Bosch và cuộc hành trình mới","uri":"/workatbosch/#"},{"categories":[],"content":"Cơ hội mớiThành thực mà nói mình không nghĩ là sẽ có một ngày, mình có cơ hội được thực tập tại một tập đoàn lớn như Bosch. Là một đứa chỉ “chân ướt chân ráo” vào ngành, cơ hội này giống như một kho báu giữa đại dương rộng lớn vậy. Ban đầu, mình thấy được những tiềm năng về chuyên môn mình có thể học hỏi và những mối quan hệ mới mình có thể kết nối. Nhưng rồi sau đó, mình nhận ra rằng Bosch cho mình hơn thế rất nhiều. Không chỉ là kiến thức, kinh nghiệm hay trải nghiệm trong chuyên môn, mà còn đó là những kỷ niệm và kỹ năng trong đời sống. Quan trọng nhất là cho mình một cơ hội để mình gặp gỡ được những con người vô cùng ưu tú và tuyệt vời. ","date":"26 Jul 2024","objectID":"/workatbosch/:1:0","series":[],"tags":[],"title":"Bosch và cuộc hành trình mới","uri":"/workatbosch/#cơ-hội-mới"},{"categories":[],"content":"Rất nhiều cái “lần đầu”Đến với Bosch, mình được unlock rất nhiều trải nghiệm đầu tiên. Đó là lần đầu tiên được đặt chân đến làm việc tại một văn phòng xìn xò đầy đủ trang thiết bị và không gian làm việc rộng rãi. Là lần đầu tiên được trải nghiệm “cơm trưa văn phòng” với các anh chị đồng nghiệp. Hay là trải nghiệm cái view triệu đô của Dinh Độc Lập từ trên cao, tha hồ sống ảo 😎 Đến làm tại đây thì không thể không nhắc đến mỳ ly cuối buổi, phải nói là cứu cánh cho mọi nhân viên văn phòng sau một ngày mệt mỏi để…lấy sức làm tiếp 😂 Mình đã bất ngờ khi có hẳn cả một thống kê về số lượng mì ly tiêu thụ trong một tháng 😋 Tất nhiên là có những ngày ăn những món khác nhưng mà mì ly vẫn là cái gì đó quá huyền thoại tại đây mà bất cứ Engineer nào cũng ăn sau mỗi buổi làm. ","date":"26 Jul 2024","objectID":"/workatbosch/:2:0","series":[],"tags":[],"title":"Bosch và cuộc hành trình mới","uri":"/workatbosch/#rất-nhiều-cái-lần-đầu"},{"categories":[],"content":"Là một đại gia đìnhThực sự, mình rất thích cách mà mọi người, các anh chị tại Bosch R\u0026D gắn bó với nhau. Ngay từ những ngày đầu tiên lập bập đi làm, mình cứ nghĩ sẽ mất rất nhiều thời gian để làm quen. Tuy nhiên các anh chị, đồng nghiệp đều cởi mở và chào đón mình như thể một thành viên mới của đại gia đình, và điều đó giúp cho bản thân OG cảm thấy tự tin hơn để hòa nhập với môi trường mới. Ngoài ra cũng có những buổi đi team building hay hẹn nhau đi chơi siêu vui. Cuối cùng với OG, đây chính là gia đình thứ 2 của mình ❤️ Dù cuộc hành trình nào cũng đến hồi kết, cuộc vui nào cũng đến lúc tàn. Nhưng những ký ức về kỳ trải nghiệm tuyệt vời ở Bosch R\u0026D sẽ mãi là cột mốc đáng nhớ trong sự nghiệp của OG - luv all of you ❤️ –Meww– ","date":"26 Jul 2024","objectID":"/workatbosch/:3:0","series":[],"tags":[],"title":"Bosch và cuộc hành trình mới","uri":"/workatbosch/#là-một-đại-gia-đình"},{"categories":[],"content":"Nhìn lại chặng đường dài năm 2023 của bản thân mình","date":"31 Dec 2023","objectID":"/recap2023/","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/"},{"categories":[],"content":"Năm 2023 vừa qua thật sự là một cột mốc đáng nhớ đối với bản thân OG. Đó là một chặng đường rất dài thăng trầm buồn vui lẫn lộn. Vào những giây phút cuối cùng, OG sẽ dùng bài viết này để nhìn lại cuộc hành trình đã qua để rồi sau cuối chính là hướng đến một tương lai với nhiều kế hoạch mới phía trước ở năm mới 2024 ❤️ ","date":"31 Dec 2023","objectID":"/recap2023/:0:0","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/#"},{"categories":[],"content":"Là một năm đầy niềm vuiNếu được tóm gọn 2 từ để mô tả năm vừa qua, thì OG sẽ chọn 2 từ “Mãn nguyện”. Vì điều lớn lao mà năm qua OG đã đạt được chính là đã được cười thật nhiều, đã được sống thật hết mình và trải nghiệm nhiều điều mới lạ. Như thế với mình đã là “Mãn nguyện” rồi 😄 Khi nhìn lại quá khứ, điều gợi lên đầu tiên chắc hẳn là những điều khiến ta vui vẻ, hạnh phúc. Đến đây, OG muốn nói lời cảm ơn đến những người bạn học, những người mà OG đã gặp, những “anh em” đã luôn sát cánh với OG trong cả chuyến hành trình dài vừa qua. Họ không chỉ là những người bạn đồng hành, mà còn là những người giúp “nêm nếm” cho cuộc sống OG thiệt mặn mà và thú vị 😂 Xin vô cùng cảm ơn các bạn. ","date":"31 Dec 2023","objectID":"/recap2023/:1:0","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/#là-một-năm-đầy-niềm-vui"},{"categories":[],"content":"Cuộc đời là những chuyến điĐầu tiên nói về chuyện đi chơi trước, năm vừa qua thật sự là một năm đặc biệt, vì OG được đi du lịch nhiều nơi và trải nghiệm nhiều cảm giác mới lạ. Info Thật sự thì mình rất thích đi du lịch và khám phá thế giới xung quanh để sau này khi nhìn lại thì mình có thể cho con cháu “Wow hồi đó ông bodoi dữ” 😂 Mở đầu cho năm 2023, mình và những người bạn đã cùng dắt nhau đi hưởng cái lạnh Đà Lạt và sau đó là ngắm biển Nha Trang quá xá đã Đà Lạt lúc đó lạnh teo\" Đà Lạt lúc đó lạnh teo Nhờ những chuyến đi như vậy, mình đã làm quen nhiều những người bạn mới. Sau đó (hẳn nửa năm sau 😂) mình lại có dịp quay lại Nha Trang để xả hơi trong kỳ hè. -- Chuẩn bị ra biển bắt ốcnha trang \" Chuẩn bị ra biển bắt ốc Tham quan nhà thờ đá mệt luônnha trang \" Tham quan nhà thờ đá mệt luôn Đã có những lần đi thật xa, và tất cả những chuyến đi đó, đều đọng lại trong OG những điều đáng quý như là một phần của năm 2023 quá đáng nhớ. ","date":"31 Dec 2023","objectID":"/recap2023/:2:0","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/#cuộc-đời-là-những-chuyến-đi"},{"categories":[],"content":"Là năm của sự nỗ lựcMình đi chơi nhiều, thì đổi lại mình học cũng nhiều. Năm vừa qua chính là năm mà mình có rất nhiều cơ hội được học hỏi, dấn thân trải nghiệm nhiều thứ. Thật sự, OG muốn cảm ơn bản thân mình vì đã không bao giờ bỏ cuộc và luôn dấn thân làm những điều mới mẻ và học tập không ngừng. Bước tiến đầu tiên mình muốn kể đó chính là lần đầu tiên bản thân mình tự tay hoàn thiện được một dự án Data Engineer end-to-end. Có thể nó không to tát nhưng với bản thân OG, đó lại chính là động lực và là viên gạch thành quả đầu tiên cho hành trình học tập dài phía trước. OG muốn gửi lời cảm ơn sâu sắc đến các thầy của khóa học FDE của AIDE và các bạn đã hỗ trợ và góp ý cho mình để hoàn thiện dự án đầu tiên ❤️ Bạn có thể ghé qua xem thử dự án đó của mình: Football ETL Analysis Và năm qua cũng là năm mà OG tham gia một cuộc thi về dữ liệu là Datathon VietNam. Tuy không may mắn đạt được thành tích tốt, nhưng lại là minh chứng cho sự dũng cảm của bản thân vì đã bước khỏi vùng an toàn và thử thách chính mình. Và hơn thế nữa bản thân mình cũng có cơ hội làm việc với những “làn gió mới” vô cùng ăn ý và góp nhặt được nhiều kinh nghiệm từ họ và từ cuộc thi. Một thành quả nho nhỏ không thể không kể đến trong năm qua chính là sự ra đời của trang web này :))) bravo Tất nhiên là ở trường mình cũng đã luôn cố gắng để không bị “chìm nghỉm” và cũng thật tuyệt vời vì luôn có những đồng chí luôn giúp đỡ OG hết mình trong trường lớp. ","date":"31 Dec 2023","objectID":"/recap2023/:3:0","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/#là-năm-của-sự-nỗ-lực"},{"categories":[],"content":"Những khoảng trầmTrong suốt một chặng đường dài, không phải lúc nào cũng luôn là những điều vui vẻ. Bản thân mình đã từng rất stress, áp lực về cuộc sống. Đôi lúc mình thấy mình thật “vô tri”… Thật đấy, một cảm giác không hề dễ chịu gì nhìn thấy sự tiến bộ của mọi người xung quanh còn mình thì vẫn loay hoay với những dự định, kế hoạch vẫn còn dang dở. Và đôi khi nhìn thấy bản thân nỗ lực mà chẳng có kết quả gì, điều đó rất nhiều, đã rất nhiều lần làm mình nản chí và tự trách. Nhưng sau cuối, sau mọi cảm giác tiêu cực, cảm ơn bản thân vì đã chưa bao giờ đầu hàng và luôn tiến về phía trước. Điều quan trọng là ta chỉ cần giỏi hơn bản thân của ngày hôm qua. Như thế đã thành công rồi. ","date":"31 Dec 2023","objectID":"/recap2023/:4:0","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/#những-khoảng-trầm"},{"categories":[],"content":"Một chương mớiĐi qua một năm 2023 nhiều sự kiện như vậy đã giúp bản thân mình trưởng thành hơn rất nhiều. Khép lại một chương cũ với nhiều bài học quý giá và những kỷ niệm đáng nhớ để làm bước đà cho chương mới 2024. OG chúc các bạn đọc một năm mới bình an và có nhiều thành công đột phá với những dự định của mình trong năm 2024 🎋 Hẹn gặp lại các bạn vào “NĂM SAU” 😂 -Mew New Year- ","date":"31 Dec 2023","objectID":"/recap2023/:5:0","series":[],"tags":["lifestory"],"title":"Nhìn lại 2023","uri":"/recap2023/#một-chương-mới"},{"categories":["projects"],"content":"A End to End ELT data pipeline to Analyze Spotify Data and build recommendation system","date":"10 Dec 2023","objectID":"/spotify_analysis/","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/"},{"categories":["projects"],"content":"Shout out cho những member tâm huyết Project này có thể hoàn thiện cũng là sự đóng góp của các thành viên trong team: Ngọc Tuấn (Data Engineer), Duy Sơn (Data Scientist), Vĩ Thiên (Data Analyst) PhongHuynh0394 Spotify Analysis with PySpark English Version Available Hello ! Hello ! Lại là OG đây. Lần này mình sẽ cùng nhau xây dựng End-to-End ELT data pipeline và một Recommender System dựa trên dữ liệu phân tích từ Spotify API nhé. Không dài dòng nữa, bắt đầu thôi nào. ","date":"10 Dec 2023","objectID":"/spotify_analysis/:0:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#"},{"categories":["projects"],"content":"IntroductionProject lần này là xây dựng hệ thống phân tích nhạc từ Spotify như diagram sau Trong diagram bao gồm những thành phần sau: Infrastructure: Ta sẽ sử dụng Docker để set up cho hầu hết các frameworks sử dụng trong hệ thống và Terraform (Optional) để set up cho MongoDB. Pipeline 1: Đây là pipeline sẽ thực hiện incremental load vào MongoDB mỗi kéo data từ Spotify API về. Pipeline 2: Data Pipeline này thực hiện việc chính là ELT (Extract, Load, Transform) để xử lý và chuẩn hóa dữ liệu JSON từ MongoDB. Việc xử lý cấu trúc dữ liệu phức tạp và lồng nhau của JSON chủ yếu sẽ được thực hiện bằng pySpark (Python API của Apache Spark) ngay trên HDFS (Hadoop Distributed File System) Analytic Layer: Để có thể sử dụng phân tích được, ta sẽ apply một lớp phân tích lên trên Hadoop. Ở project này ta sẽ sử dụng Dremio để cho việc đó. Machine learning and Dashboard: Ở phần này chủ yếu là việc training mô hình machine learning model cho hệ thống gợi ý âm nhạc spoitfy. Ngoài ra cũng có thể PowerBI để tạo các Dashboard để phân tích từ dữ liệu ở Dremio. Application: Cuối cùng ta sẽ dùng Streamlit để viết một web đơn giản để làm hệ thống gợi ý và tìm kiếm nhạc. Ngoài ra người dùng cũng có thể tương tác với BI Dashboard ở layer này ","date":"10 Dec 2023","objectID":"/spotify_analysis/:1:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#introduction"},{"categories":["projects"],"content":"Data SchemaNguồn data của chúng ta đến từ Spotify API. Ngoài ra ta cũng sẽ cào thêm một danh sách nghệ sĩ trên Spotify từ trang Sportify Artists. Data sau khi đã được xử lý cuối cùng sẽ có schema như sau: ","date":"10 Dec 2023","objectID":"/spotify_analysis/:2:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#data-schema"},{"categories":["projects"],"content":"InfrastructureTa sẽ sử dụng chủ yếu là Docker để cấu hình cho hầu hết các framework trong hệ thống thông qua docker-compose.yml file. Chỉ riêng có MongoDB Atlas thì có thể set up bằng nhiều cách khác nhau. ","date":"10 Dec 2023","objectID":"/spotify_analysis/:3:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#infrastructure"},{"categories":["projects"],"content":"Apache HadoopĐầu tiên ta sẽ set up Apache Hadoop bằng Docker Compose. Như đã biết Hadoop là một framework được viết bằng Java được giới thiệu bởi Google vào năm 2006 và ứng dụng công nghệ hệ thống phân tán để lưu trữ và xử lý Big Data. Hadoop được xây dựng đựa trên ba thành phần chính: HDFS (Hadoop Distributed File System): là hệ thống file phân tán được sử dụng để lưu trữ dữ liệu YARN (Yet Another Resouce Negotiator): là một framework quản lý và phân bổ tài nguyên để vận hành các ứng dụng trên Hadoop MapReduce: là một framework lập trình xử lý và tính toán dữ liệu song song trong môi trường hệ thống phân tán. HDFS và YARN trong Hadoop Trong project này, ta sẽ lượt bớt phần setup MapReduce trong Hadoop Cluster và thay vào đó chỉ là HDFS và YARN thôi (vì phần xử lý tính toán ta sẽ thay thế bằng Apache Spark). Ngoài ra vì là project PoC (Proof of Concept), ta sẽ đơn giản hóa Hadoop cluster với 1 Master Node, 1 Worker Node mà thôi. volumes:hadoop_datanode:hadoop_namenode:services:namenode:container_name:namenodeimage:apache/hadoop:3hostname:namenodecommand:bash -c \"if [ ! -f /tmp/hadoop-root/dfs/name/.formatted ]; then hdfs namenode -format \u0026\u0026 touch /tmp/hadoop-root/dfs/name/.formatted; fi \u0026\u0026 hdfs namenode\"ports:- 9870:9870- 8020:8020- 9000:9000user:rootenv_file:- .envvolumes:- hadoop_namenode:/tmp/hadoop-root/dfs/namenetworks:- docker-netdatanode:image:apache/hadoop:3container_name:datanodehostname:datanode command:[\"hdfs\",\"datanode\"]ports:- 9864:9864- 9866:9866expose:- 50010env_file:- .envuser:rootvolumes:- hadoop_datanode:/tmp/hadoop-root/dfs/datanetworks:- docker-netresourcemanager:image:apache/hadoop:3hostname:resourcemanagercommand:[\"yarn\",\"resourcemanager\"]ports:- 8088:8088env_file:- .envvolumes:- ./test.sh:/opt/test.shnetworks:- docker-netnodemanager:image:apache/hadoop:3command:[\"yarn\",\"nodemanager\"]env_file:- .envports:- 8188:8188networks:- docker-net Ta sẽ chủ yếu sử dụng HDFS để làm Datalake cho project này. Ngoài ra có thể truy cập vào các services của Hadoop thông qua: Namenode: localhost:9870 Datanode: localhost:9864 Resouces Manager (YARN): localhost:8088 ","date":"10 Dec 2023","objectID":"/spotify_analysis/:3:1","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#apache-hadoop"},{"categories":["projects"],"content":"PrefectPrefect là một orchestration tool khá dễ dùng và dễ bắt đầu. Với giao diện bắt mắt và tính dễ tiếp cận, framework này giúp cho việc lên lịch, sắp xếp và chạy các task hay thậm chí là chạy song song (concurrent task). Ta sẽ set up một prefect server và prefect API bằng docker như sau: prefect-server:build:context:./prefectimage:prefecthostname:prefect-servercontainer_name:prefect-servervolumes:- prefect:/root/.prefectcommand:prefect server startenvironment:- PREFECT_UI_URL=http://127.0.0.1:4200/api- PREFECT_API_URL=http://127.0.0.1:4200/api- PREFECT_SERVER_API_HOST=0.0.0.0ports:- 4200:4200networks:- docker-netprefect:image:prefect:latestcontainer_name:prefectrestart:alwaysvolumes:- \"./prefect/flows:/opt/prefect/flows\"- \"/etc/timezone:/etc/timezone:ro\"- \"/etc/localtime:/etc/localtime:ro\"env_file:- .envnetworks:- docker-netdepends_on:- prefect-server Tại sao lại set up Prefect Server và Prefect riêng với nhau ? Có nhiều lý do để chọn set up riêng Prefect Server và Prefect riêng biệt, nhưng thường lý do quan trọng nhất là ta muốn tùy chỉnh môi trường chạy flow mà không ảnh hưởng đến server. Prefect Server sẽ đóng vai trò Backend xử lý các tín hiệu từ các container khác thông qua prefect API. Điều này giúp tăng tính ổn định cho hệ thống và dễ để bảo trì, debug. Và trong lúc setup, ta sẽ không sử dụng built-in image PrefectHQ mà thay vào đó sẽ tự build một custom image để có thể dễ tùy biến. Ngoài các module cho python trong requirements.txt ta sẽ cần phải config thêm Java 11 để tương thích với pyspark ARG IMAGE_VARIANT=slim-busterARG OPENJDK_VERSION=11 ARG PYTHON_VERSION=3.11.0FROMpython:${PYTHON_VERSION}-${IMAGE_VARIANT} AS py3FROMopenjdk:${OPENJDK_VERSION}-${IMAGE_VARIANT}COPY --from=py3 / /WORKDIR/opt/prefectCOPY requirements.txt .RUN pip install -r requirements.txt --trusted-host pypi.python.org --no-cache-dirCOPY flows /opt/prefect/flows# Run our flow script when the container startsCMD [\"python\", \"flows/main_flow.py\"] Ta có thể truy cập UI Prefect thông qua localhost:4042 để trigger pipeline. ","date":"10 Dec 2023","objectID":"/spotify_analysis/:3:2","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#prefect"},{"categories":["projects"],"content":"DremioBây giờ ta đã có một datalake là hadoop HDFS, tuy nhiên Object Storange như HDFS không cung cấp khả năng phân tích hay truy vấn, do đó ta sẽ cần xây dựng một Analytic Layer trên HDFS. Ở vị trí này có khá nhiều lựa chọn như Trino, Hive,… Tuy nhiên ở project này ta sẽ dùng một Lakehouse platform là Dremio. Dremio là một mã nguồn mở data-as-a-service hỗ trợ phân tích và dễ dàng kết nối với đa dạng nguồn data khác nhau và tất nhiên trong đó có Hadoop. Dremio cung cấp khả truy vấn mạnh mẽ với SQL engine và một giao diện thân thiện Dremio Cluster Ta sẽ config một dremio cluster với dremio-oss image. Image này bao gồm: Embedded Zookeeper Master Coordinator Execuor dremio:image:dremio/dremio-osshostname:dremiocontainer_name:dremiorestart:alwaysuser:rootvolumes:- dremio_data:/var/lib/dremio- dremio_data:/localFiles- dremio_data:/opt/dremioports:- \"9047:9047\"# Web UI (HTTP)- \"31010:31010\"# ODBC/JDBC client- \"32010:32010\"# Apache Arrow Flight clientsnetworks:- docker-net Sau khi deploy thành công, ta có thể đăng nhập vào dremio ở localhost:9047 với username=dremio và password=dremio123 ","date":"10 Dec 2023","objectID":"/spotify_analysis/:3:3","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#dremio"},{"categories":["projects"],"content":"MongoDB with TerraformWarning Ở bước set up này cần phải đăng ký tài khoản MongoDB Atlas và cài đặt Terraform Về Terraform: HashiCorp Terraform là một công cụ “insfrastructure as code” giúp ta setup và config những tài nguyên cả on-prem hay cloud chỉ với việc viết configuration file. MongoDB Atlas: có thể không cần config trong docker compose và hoàn toàn có thể set up thủ công ở Đây. Tuy nhiên trong phần này thì ta sẽ làm điều đó tự động bằng Terraform. Đầu tiên ta càn phải có tài khoản MongoDB Atlas, sau đó bước đầu tiên là sẽ set up vài variables trong file variable.tf. File này sẽ lưu đa số các biến môi trường, API key, token,…. Note Chi tiết cách set up file variable.tf có thể đọc ở Đây Với terraform, ta sẽ chủ yếu chạy file main.tf vì file này sẽ lưu mọi config về cluster muốn deploy. Ta sẽ sử dụng MongoDB Atlas Provider và gọi lại các variables (được set trong file file variable.tf trước đó) terraform { required_providers { mongodbatlas = { source = \"mongodb/mongodbatlas\", version = \"1.8.0\" } } } provider \"mongodbatlas\" { public_key = var.public_key private_key = var.private_ke } Tip Chi tiết các resources có thể tham khảo tại đây mongodbatlas Provider terraform docs Ngoài ra ta cũng sẽ tạo một vài resources khác để hoàn thiện phần setting up: mongdodbatlas_project: Config project khỏi tạo resource \"mongodbatlas_project\" \"spotify_project\" { name = \"spotify_project\" org_id = var.org_id is_collect_database_specifics_statistics_enabled = true is_data_explorer_enabled = true is_performance_advisor_enabled = true is_realtime_performance_panel_enabled = true is_schema_advisor_enabled = true } mongodbatlas_cluster: setup cluster cho atlas, ta sẽ host trên gcp (có thể chọn azure hay aws đều được) resource \"mongodbatlas_cluster\" \"spotify\" { name = var.cluster_name project_id = mongodbatlas_project.spotify_project.id backing_provider_name = \"GCP\" provider_name = \"TENANT\" provider_instance_size_name = var.cluster_size provider_region_name = var.region } Chạy lệnh deploy cluster Sau khi đã chuẩn bị hoàn tất các configuration, việc còn lại là chạy các lệnh tf cli để set up plan terraform init # set up provider for atlas terraform plan Sau đó, nhập yes để tiếp tục quá trình set up. cuối dùng để deploy cluster thì ta sẽ chỉ cần nhập terraform apply. sau đó ta sẽ nhận được kết quả giống thế này Apply complete! Resources: 4 added, 0 changed, 0 destroyed. Outputs: password = \"123\" srv_address = \"mongodb+srv://spotify-cluster.kdglyul.mongodb.net\" user = \"root\" Hãy chú ý các thông tin ở ba dòng cuối là password, srv_address và user. ta sẽ cần những thông tin này cho file .env để set up connection với Atlas sau này. Đó sẽ là các biến MONGODB_PASS, MONGODB_SRV và MONGODB_USER Tip Để xóa mọi thứ trên mongodb atlas kể cả cluster thì chỉ cần chạy lệnh terraform destroy. hãy cẩn thận vì lệnh này có thể xóa hết cluster và cả data ","date":"10 Dec 2023","objectID":"/spotify_analysis/:3:4","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#mongodb-with-terraform"},{"categories":["projects"],"content":"Pipeline 1Data pipeline này thực hiện việc chính là pooling api để crawl data từ spotify api và incremental load data vào mongodb atlas. để giải quyết vấn đề về rate limit, ta sẽ config cho flow này được tự động trigger mỗi 2 phút 5 giây để crawl một batch gồm lần lượt thông tin 5 artists từ file danh sách artists.txt Incremental load Khác với full load khi ta load data và overwrite tất cả trong database , incremental load hay được gọi là delta load thực hiện việc chỉ load phần data update hoặc data mới mà không load lại tất cả các historical data trước đó. pipeline này sẽ luôn cào và ingest thông tin nghệ sĩ mới nhất trong file artists.txt để gọi spotify api qua mỗi batch và dùng một file log.txt để đánh dấu vị trí index và số lượng nghệ sĩ đã cào thành công ","date":"10 Dec 2023","objectID":"/spotify_analysis/:4:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#pipeline-1"},{"categories":["projects"],"content":"Pipeline 2Đây là phần chính của project này và hầu hết các tác vụ xử lý sẽ thực hiện trong pipeline này. Pipeline này chính là một ELT (Extract - Load - Transform) data pipeline với nguồn raw là các collections từ MongoDB, sau đó sẽ được xử lý bằng spark và lưu trong hdfs. Ta đồng thời sẽ xây dựng một Medallion architecture ngay trong hdfs để phân vùng cho các loại data. Medallion architecture kiến trúc này được Databricks giới thiệu là một “data design pattern” dùng để tổ chức data trong lakehouse với mục tiêu là để cải thiện chất lượng data qua từng layer của kiến trúc này (bronze -\u003e silver -\u003e gold) ","date":"10 Dec 2023","objectID":"/spotify_analysis/:5:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#pipeline-2"},{"categories":["projects"],"content":"PySparkApache Spark được set up ở đâu ? Có lẽ bạn sẽ thắc mắc tại sao không thấy Apache Spark được set up. có 2 lý do: Tài nguyên hạn chế: đối với project hiện tại khi đã có quá nhiều services được chạy cùng lúc bởi docker containers, set up thêm spark cluster có thể sẽ quá tải và không đủ tài nguyên để deploy tất cả. Data nhỏ: thành thực thì số lượng data phải process không thực sự quá lớn (khoảng hơn 200k quan sát) thì việc sử dụng spark có thể sẽ lãng phí tài nguyên. tuy nhiên vì mục đích học tập và thực hiện project poc, ta sẽ sử dụng spark với local mode thay vì set up standardlone mode với cluster Apache Spark có nhiều chế độ chạy mà ta có thể tùy vào quy mô project mà sử dụng: local[*]: là chế độ local, chế độ này cho phép chạy mà không cần phải setup spark cluster spark://{master-node-name}:7077: chế độ standalone và ta sẽ kết nối với một spark cluster yarn-client: chế độ yarn-client yarn-cluster: chế độ yarn-cluster mesos://host:5050: mesos cluster Và ở project này ta sẽ sử dụng local mode để tối ưu tài nguyên cũng như phù hợp với kích thước dataset. để tiện lợi cho việc tạo SparkSession, ta sẽ viết một sparkIO bằng contextlib from pyspark.sql import SparkSession from pyspark import SparkConf from contextlib import contextmanager @contextmanager def SparkIO(conf: SparkConf = SparkConf()): app_name = conf.get(\"spark.app.name\") master = conf.get(\"spark.master\") print(f'Create SparkSession app {app_name}with {master}mode') spark = SparkSession.builder.config(conf=conf).getOrCreate() try: yield spark except Exception: raise Exception finally: print(f'Stop SparkSession app {app_name}') spark.stop() điều này giúp đơn giản hóa việc tạo sparksession và tắt spark trở nên đơn giản và tránh thiếu sót Tip Ta cũng có thể làm tương tự cho việc kết nối với mongodb bằng cách viết một mongodb_io from pymongo import MongoClient from pymongo.errors import ConnectionFailure from contextlib import contextmanager import os @contextmanager def MongodbIO(): user = os.getenv(\"MONGODB_USER\") password = os.getenv(\"MONGODB_PASSWORD\") cluster = os.getenv(\"MONGODB_SRV\") cluster = cluster.split(\"//\")[-1] uri = f\"mongodb+srv://{user}:{password}@{cluster}/?retryWrites=true\u0026w=majority\" try: client = MongoClient(uri) print(f\"MongoDB Connected\") yield client except ConnectionFailure: print(f\"Failed to connect with MongoDB\") raise ConnectionFailure finally: print(\"Close connection to MongoDB\") client.close() ","date":"10 Dec 2023","objectID":"/spotify_analysis/:5:1","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#pyspark"},{"categories":["projects"],"content":"Data Processingta sẽ xây dựng hdfs theo kiến trúc meddalion bằng cách chia data quality thành cách vùng (hay directory): Bronze layer: dùng để chỉ lưu raw data Silver layer: lưu data đã xử lý một phần (xử lý kiểu dữ liệu, xử lý kiểu array và các cấu trúc lồng phức tạp) từ Bronze Layer Gold layer: lưu data đã được làm sạch sau khi chuẩn hóa dữ liệu và làm sạch các bảng mới từ Silver Layer ","date":"10 Dec 2023","objectID":"/spotify_analysis/:5:2","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#data-processing"},{"categories":["projects"],"content":"Analytic layerNhư đã setup từ trước, ta sẽ sử dụng dremio như một analytic layer để có thể phân tích trên data sạch ở gold layer trong HDFS. Việc đơn giản là ta chỉ cần kết nối dremio đến HDFS qua dremio UI ở localhost:9047 và sau đó format file .parquet ở thư mục gold_layer và ta có thể viết các câu lệnh sql query để bắt đầu phân tích được rồi ","date":"10 Dec 2023","objectID":"/spotify_analysis/:6:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#analytic-layer"},{"categories":["projects"],"content":"Machine learning and DashboardĐến đây có thể coi như là kết thúc phần việc của một data engineer, ta sẽ bắt đầu các tasks của một data scientist và data analyst trong việc xây dựng mô hình máy học và xây dựng report. ","date":"10 Dec 2023","objectID":"/spotify_analysis/:7:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#machine-learning-and-dashboard"},{"categories":["projects"],"content":"K-Nearest Neighbors (KNN)Ta sẽ xây dựng một mô hình có thể gợi ý nhạc dựa trên độ giống nhau giữa các features của những bản nhạc như: độ lớn, giai điệu, thể loại, tempo, mức độ sôi động,… Để làm được việc đó ta sẽ sử dụng một mô hình có thể đưa ra được top k các record khác nhau từ danh sách hàng trăm nghìn bài hát. Đó là KNN KNN là một mô hình học máy đơn giản giúp ta tìm được top những điểm data gần nhất với vị trí data point đầu vào dựa trên khoảng cách của chúng trong không gian vector. Similarity Metric lần này ta sử dụng sẽ là độ tương tự consine hay consine similarity $$ S_C(A,B) = cos(\\theta) = \\frac{A \\cdot B}{\\Vert A\\Vert \\Vert B \\Vert} = \\frac{\\sum^n_{i=1}A_iB_i}{\\sqrt{\\sum^n_{i=1}A^2} \\cdot \\sqrt{\\sum^n_{i=1}B^2} } $$ Ta sẽ xây dựng một mô hình gợi ý top k các bài hát giống nhất với bài hát lựa chọn như sau: class SongRecommendationSystem: def __init__(self, client, options): self.client = client self.options = options self.knn_model = NearestNeighbors(metric='cosine') def fit(self, table_name): matrix_table = self._get_table(table_name).values self.knn_model.fit(matrix_table) def _get_table(self, table_name): sql = f\"select * from {table_name}\" return self.client.query(sql, self.options) def recommend_songs(self, track_name: str, table_name='home.searchs', num_recommendations=5): song_library = self._get_table(table_name).reset_index(drop=True) if track_name not in song_library['track_name'].values: print(f'Track \"{track_name}\" not found in the dataset.') return None features_matrix = self._get_table('home.model') track_index = song_library.index[song_library['track_name'] == track_name].tolist()[0] _, indices = self.knn_model.kneighbors([features_matrix.iloc[track_index]], n_neighbors=num_recommendations + 1) return song_library.loc[indices[0][1:], :] Khi thử tìm các bài hát giống với “Something Just Like This” (có vẻ cũng chưa tốt lắm 😂) ","date":"10 Dec 2023","objectID":"/spotify_analysis/:7:1","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#k-nearest-neighbors-knn"},{"categories":["projects"],"content":"PowerBI DashboardKhi có rất nhiều data sạch, điều tiếp theo ta có thể nghĩ đến chính là làm sao thấy được nhiều insight giá trị nhất từ data đó. Đây là lúc ta có thể dùng PowerBI để xây dựng Dashboard từ Dremio ","date":"10 Dec 2023","objectID":"/spotify_analysis/:7:2","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#powerbi-dashboard"},{"categories":["projects"],"content":"ApplicationCuối cùng, ta có thể xây dựng một ứng dụng đơn giản để ứng dụng mô hình machine learning cũng như như tích hợp Dashboard vào trong ứng dụng này như một portal toàn diện. Ta sẽ sử dụng Streamlit để xây dựng một web app đơn giản ","date":"10 Dec 2023","objectID":"/spotify_analysis/:8:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#application"},{"categories":["projects"],"content":"Final ThoughtsProject chỉ mang tính chất học tập, do đó độ lớn của data hay một số kiến trúc set up có thể chưa hoàn chỉnh. Tuy nhiên đây vẫn là một cột mốc đáng nhớ trong hành trình học tập của OG. Hy vọng nó sẽ giúp ích cho các bạn tham khảo 😄 -Meww- ","date":"10 Dec 2023","objectID":"/spotify_analysis/:9:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#final-thoughts"},{"categories":["projects"],"content":"Related Stock Analysis Basic analyzing vn30 stock using pca and k-means Read more... Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"10 Dec 2023","objectID":"/spotify_analysis/:0:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis/#related"},{"categories":null,"content":"A End to End ELT data pipeline to Analyze Spotify Data and build recommendation system","date":"10 Dec 2023","objectID":"/spotify_analysis_en/","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/"},{"categories":null,"content":"Shout out for our dedicated members This project is contributed by: Ngọc Tuấn (Data Engineer), Duy Sơn (Data Scientist), Vĩ Thiên (Data Analyst) PhongHuynh0394 Spotify Analysis with PySpark Vietnamese Version Available Hello! Hello! It’s OG again. This time we’ll be building an End-to-End ELT data pipeline and a Recommender System based on analytics from the Spotify API. Without further ado, let’s get started. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:0:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#"},{"categories":null,"content":"IntroductionThis project involves building a music analytics system from Spotify, as shown in the diagram below: The diagram includes the following components: Infrastructure: We will use Docker to set up most of the frameworks used in the system and optionally use Terraform to set up MongoDB. Pipeline 1: This pipeline performs incremental loading into MongoDB each time data is pulled from the Spotify API. Pipeline 2: This data pipeline performs ELT (Extract, Load, Transform) to process and normalize JSON data from MongoDB. Most of the work dealing with the complex, nested JSON structure will be handled by PySpark (Python API for Apache Spark) on HDFS (Hadoop Distributed File System). Analytic Layer: To analyze the data, we will apply an analytics layer on top of Hadoop. In this project, we will use Dremio for that. Machine learning and Dashboard: This part involves training a machine learning model for the Spotify music recommendation system. Additionally, we can use PowerBI to create dashboards to analyze data from Dremio. Application: Finally, we will use Streamlit to build a simple web application for the music recommendation and search system. Users can also interact with the BI Dashboard at this layer. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:1:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#introduction"},{"categories":null,"content":"Data SchemaOur data source comes from the Spotify API. We will also scrape a list of Spotify artists from Sportify Artists. The final processed data will have the following schema: ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:2:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#data-schema"},{"categories":null,"content":"InfrastructureWe will primarily use Docker to configure most of the frameworks in the system via a docker-compose.yml file. MongoDB Atlas, however, can be set up in multiple ways. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:3:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#infrastructure"},{"categories":null,"content":"Apache HadoopFirst, we will set up Apache Hadoop using Docker Compose. As we know, Hadoop is a Java-based framework introduced by Google in 2006 that leverages distributed system technology to store and process Big Data. Hadoop is built on three main components: HDFS (Hadoop Distributed File System): A distributed file system used for data storage. YARN (Yet Another Resource Negotiator): A resource management framework for running applications on Hadoop. MapReduce: A parallel data processing framework in a distributed system environment. HDFS and YARN in Hadoop In this project, we will simplify the setup by omitting the MapReduce component in the Hadoop Cluster and only using HDFS and YARN (since we will use Apache Spark for processing). Additionally, as this is a PoC (Proof of Concept) project, we will simplify the Hadoop cluster to just one Master Node and one Worker Node. volumes:hadoop_datanode:hadoop_namenode:services:namenode:container_name:namenodeimage:apache/hadoop:3hostname:namenodecommand:bash -c \"if [ ! -f /tmp/hadoop-root/dfs/name/.formatted ]; then hdfs namenode -format \u0026\u0026 touch /tmp/hadoop-root/dfs/name/.formatted; fi \u0026\u0026 hdfs namenode\"ports:- 9870:9870- 8020:8020- 9000:9000user:rootenv_file:- .envvolumes:- hadoop_namenode:/tmp/hadoop-root/dfs/namenetworks:- docker-netdatanode:image:apache/hadoop:3container_name:datanodehostname:datanode command:[\"hdfs\",\"datanode\"]ports:- 9864:9864- 9866:9866expose:- 50010env_file:- .envuser:rootvolumes:- hadoop_datanode:/tmp/hadoop-root/dfs/datanetworks:- docker-netresourcemanager:image:apache/hadoop:3hostname:resourcemanagercommand:[\"yarn\",\"resourcemanager\"]ports:- 8088:8088env_file:- .envvolumes:- ./test.sh:/opt/test.shnetworks:- docker-netnodemanager:image:apache/hadoop:3command:[\"yarn\",\"nodemanager\"]env_file:- .envports:- 8188:8188networks:- docker-net We will primarily use HDFS as the Data Lake for this project. Additionally, you can access Hadoop services through: Namenode: localhost:9870 Datanode: localhost:9864 Resource Manager (YARN): localhost:8088 ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:3:1","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#apache-hadoop"},{"categories":null,"content":"PrefectPrefect is an easy-to-use orchestration tool with an attractive interface that simplifies task scheduling, organization, and parallel (concurrent) task execution. We will set up a Prefect server and API using Docker: prefect-server:build:context:./prefectimage:prefecthostname:prefect-servercontainer_name:prefect-servervolumes:- prefect:/root/.prefectcommand:prefect server startenvironment:- PREFECT_UI_URL=http://127.0.0.1:4200/api- PREFECT_API_URL=http://127.0.0.1:4200/api- PREFECT_SERVER_API_HOST=0.0.0.0ports:- 4200:4200networks:- docker-netprefect:image:prefect:latestcontainer_name:prefectrestart:alwaysvolumes:- \"./prefect/flows:/opt/prefect/flows\"- \"/etc/timezone:/etc/timezone:ro\"- \"/etc/localtime:/etc/localtime:ro\"env_file:- .envnetworks:- docker-netdepends_on:- prefect-server Why separate Prefect Server and Prefect? There are several reasons for separating Prefect Server from Prefect, but the most important is to allow customization of the flow environment without affecting the server. The Prefect Server acts as the backend, processing signals from other containers via the Prefect API. This improves system stability and makes it easier to maintain and debug. Instead of using the built-in image from PrefectHQ, we will build a custom image to allow easier customization. In addition to the Python modules in requirements.txt, we will configure Java 11 for PySpark compatibility: ARG IMAGE_VARIANT=slim-busterARG OPENJDK_VERSION=11 ARG PYTHON_VERSION=3.11.0FROMpython:${PYTHON_VERSION}-${IMAGE_VARIANT} AS py3FROMopenjdk:${OPENJDK_VERSION}-${IMAGE_VARIANT}COPY --from=py3 / /WORKDIR/opt/prefectCOPY requirements.txt .RUN pip install -r requirements.txt --trusted-host pypi.python.org --no-cache-dirCOPY flows /opt/prefect/flows# Run our flow script when the container startsCMD [\"python\", \"flows/main_flow.py\"] We can access the Prefect UI at localhost:4042 to trigger the pipeline. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:3:2","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#prefect"},{"categories":null,"content":"DremioNow that we have a data lake in Hadoop HDFS, we need an Analytic Layer to analyze or query the data, as HDFS does not provide these capabilities. We have various options such as Trino or Hive, but in this project, we will use Dremio, a Lakehouse platform. Dremio is an open-source data-as-a-service platform that supports analytics and can easily connect with various data sources, including Hadoop. Dremio provides powerful query capabilities with its SQL engine and a user-friendly interface: Dremio Cluster We will configure a Dremio cluster using the dremio-oss image, which includes: Embedded Zookeeper Master Coordinator Executor dremio:image:dremio/dremio-osshostname:dremiocontainer_name:dremiorestart:alwaysuser:rootvolumes:- dremio_data:/var/lib/dremio- dremio_data:/localFiles- dremio_data:/opt/dremioports:- \"9047:9047\"# Web UI (HTTP)- \"31010:31010\"# ODBC/JDBC client- \"32010:32010\"# Apache Arrow Flight clientsnetworks:- docker-net After successfully deployed, we can access Dremio UI at localhost:9047 with username=dremio and password=dremio123 ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:3:3","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#dremio"},{"categories":null,"content":"MongoDB with TerraformWarning In this setup step, you need to register for a MongoDB Atlas account and install Terraform. About Terraform: HashiCorp Terraform is an “infrastructure as code” tool that allows us to set up and configure resources, whether on-prem or in the cloud, simply by writing configuration files. MongoDB Atlas: You can manually configure MongoDB without Docker Compose via this link. However, in this section, we will automate the setup using Terraform. First, you need a MongoDB Atlas account. The next step is to set up some variables in the variable.tf file. This file will store most of the environment variables, API keys, tokens, etc. Note You can find details on how to set up the variable.tf file here With Terraform, we’ll primarily run the main.tf file because it contains all the configuration to deploy the cluster. We’ll use the MongoDB Atlas Provider and refer to the variables defined in variable.tf as follows: terraform { required_providers { mongodbatlas = { source = \"mongodb/mongodbatlas\", version = \"1.8.0\" } } } provider \"mongodbatlas\" { public_key = var.public_key private_key = var.private_ke } Tip You can refer to detailed resources for the MongoDB Atlas provider here. Next, we’ll create a few resources to complete the setup: mongodbatlas_project: This resource sets up the project. resource \"mongodbatlas_project\" \"spotify_project\" { name = \"spotify_project\" org_id = var.org_id is_collect_database_specifics_statistics_enabled = true is_data_explorer_enabled = true is_performance_advisor_enabled = true is_realtime_performance_panel_enabled = true is_schema_advisor_enabled = true } mongodbatlas_cluster: This resource sets up the MongoDB Atlas cluster. We will host it on GCP, but you can also choose Azure or AWS. resource \"mongodbatlas_cluster\" \"spotify\" { name = var.cluster_name project_id = mongodbatlas_project.spotify_project.id backing_provider_name = \"GCP\" provider_name = \"TENANT\" provider_instance_size_name = var.cluster_size provider_region_name = var.region } Deploy the Cluster Once the configuration is ready, you can run Terraform CLI commands to set up the plan: terraform init # set up provider for atlas terraform plan Then, type yes to proceed with the setup. To deploy the cluster, just enter terraform apply. The result will look like this: Apply complete! Resources: 4 added, 0 changed, 0 destroyed. Outputs: password = \"123\" srv_address = \"mongodb+srv://spotify-cluster.kdglyul.mongodb.net\" user = \"root\" Pay attention to the information in the last three lines: password, srv_address, and user. These details will be needed for the .env file to set up the connection with MongoDB Atlas later. These will be the MONGODB_PASS, MONGODB_SRV and MONGODB_USER. Tip To delete everything from MongoDB Atlas, including the cluster, you only need to run terraform destroy. Be cautious, as this command will remove the entire cluster and all data. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:3:4","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#mongodb-with-terraform"},{"categories":null,"content":"Pipeline 1This data pipeline performs API pooling to crawl data from the Spotify API and incrementally loads it into MongoDB Atlas. To handle the rate limit issue, we configure this flow to automatically trigger every 2 minutes and 5 seconds, crawling a batch of information for 5 artists from the artists.txt file. Incremental Load Unlike full load, which overwrites all data in the database, incremental load (or delta load) only loads updated or new data without reloading all historical data. This pipeline will always fetch and ingest the latest artist information from the artists.txt file by calling the Spotify API in batches and using a log.txt file to track the index and number of artists successfully crawled. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:4:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#pipeline-1"},{"categories":null,"content":"Pipeline 2This is the main part of the project where most of the processing takes place. It follows an ELT (Extract - Load - Transform) pipeline model with raw data from MongoDB collections, which is then processed with Spark and stored in HDFS. We will also build a Medallion Architecture within HDFS to partition the data. Medallion Architecture This architecture, introduced by Databricks, is a “data design pattern” used to organize data in a Lakehouse to improve data quality through its layers (Bronze -\u003e Silver -\u003e Gold). ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:5:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#pipeline-2"},{"categories":null,"content":"PySparkWhere is Apache Spark set up? You might wonder why Apache Spark is not set up. There are two reasons: Limited resources: With many services running simultaneously via Docker containers, setting up an additional Spark Cluster might exceed resource limits. Small dataset: The dataset size is relatively small (around 200K observations), so using Spark could be overkill. However, for learning purposes, we will use Spark in local mode rather than setting up a standalone cluster. Apache Spark offers several running modes depending on project scale: local[*]: Local mode, no need for a Spark Cluster setup. spark://{master-node-name}:7077: Standalone mode, connecting to a Spark Cluster yarn-client: Yarn-client mode yarn-cluster: Yarn-cluster mode mesos://host:5050: Mesos cluster For this project, we will use local mode to optimize resources. To simplify creating and stopping SparkSessions, we write a SparkIO using contextlib from pyspark.sql import SparkSession from pyspark import SparkConf from contextlib import contextmanager @contextmanager def SparkIO(conf: SparkConf = SparkConf()): app_name = conf.get(\"spark.app.name\") master = conf.get(\"spark.master\") print(f'Create SparkSession app {app_name}with {master}mode') spark = SparkSession.builder.config(conf=conf).getOrCreate() try: yield spark except Exception: raise Exception finally: print(f'Stop SparkSession app {app_name}') spark.stop() This approach simplifies the creation and termination of SparkSessions, ensuring we don’t miss any steps. Tip We can do something similar for MongoDB connections by writing a MongoDB_io from pymongo import MongoClient from pymongo.errors import ConnectionFailure from contextlib import contextmanager import os @contextmanager def MongodbIO(): user = os.getenv(\"MONGODB_USER\") password = os.getenv(\"MONGODB_PASSWORD\") cluster = os.getenv(\"MONGODB_SRV\") cluster = cluster.split(\"//\")[-1] uri = f\"mongodb+srv://{user}:{password}@{cluster}/?retryWrites=true\u0026w=majority\" try: client = MongoClient(uri) print(f\"MongoDB Connected\") yield client except ConnectionFailure: print(f\"Failed to connect with MongoDB\") raise ConnectionFailure finally: print(\"Close connection to MongoDB\") client.close() ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:5:1","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#pyspark"},{"categories":null,"content":"Data ProcessingWe will organize HDFS using the Medallion architecture, dividing data quality into different zones (or directories): Bronze Layer: Stores raw data. Silver Layer: Stores partially processed data (handling data types, arrays, and complex nested structures) from the Bronze Layer. Gold Layer: Stores cleaned data after standardizing and cleaning the tables from the Silver Layer. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:5:2","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#data-processing"},{"categories":null,"content":"Analytic LayerAs previously set up, we will use Dremio as an analytic layer to analyze the clean data stored in the Gold Layer within HDFS. The process is straightforward: connect Dremio to HDFS through the Dremio UI at localhost:9047. Then, format the .parquet files in the golde_layer directory, and you’ll be able to write SQL queries to start analyzing the data. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:6:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#analytic-layer"},{"categories":null,"content":"Machine Learning and DashboardAt this point, the responsibilities of a Data Engineer are largely complete. We will now move on to the tasks of a Data Scientist and Data Analyst, focusing on building machine learning models and generating reports. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:7:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#machine-learning-and-dashboard"},{"categories":null,"content":"K-Nearest Neighbors (KNN)We will develop a model to recommend music based on the similarity of features between songs, such as loudness, melody, genre, tempo, energy level, and more. To achieve this, we will use a model that can return the top K most similar records from a list of hundreds of thousands of songs. This model is KNN KNN is a simple machine learning model that helps us find the closest data points to the input data point based on their distance in vector space. The similarity metric we will use is cosine similarity: $$ S_C(A,B) = cos(\\theta) = \\frac{A \\cdot B}{\\Vert A\\Vert \\Vert B \\Vert} = \\frac{\\sum^n_{i=1}A_iB_i}{\\sqrt{\\sum^n_{i=1}A^2} \\cdot \\sqrt{\\sum^n_{i=1}B^2} } $$ We will build a model to recommend the top K songs most similar to a selected song as follows: class SongRecommendationSystem: def __init__(self, client, options): self.client = client self.options = options self.knn_model = NearestNeighbors(metric='cosine') def fit(self, table_name): matrix_table = self._get_table(table_name).values self.knn_model.fit(matrix_table) def _get_table(self, table_name): sql = f\"select * from {table_name}\" return self.client.query(sql, self.options) def recommend_songs(self, track_name: str, table_name='home.searchs', num_recommendations=5): song_library = self._get_table(table_name).reset_index(drop=True) if track_name not in song_library['track_name'].values: print(f'Track \"{track_name}\" not found in the dataset.') return None features_matrix = self._get_table('home.model') track_index = song_library.index[song_library['track_name'] == track_name].tolist()[0] _, indices = self.knn_model.kneighbors([features_matrix.iloc[track_index]], n_neighbors=num_recommendations + 1) return song_library.loc[indices[0][1:], :] When testing the system to find songs similar to “Something Just Like This” (it’s not perfect yet! 😅): ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:8:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#k-nearest-neighbors-knn"},{"categories":null,"content":"PowerBI DashboardWith a large amount of clean data, the next logical step is to derive valuable insights from that data. This is where PowerBI comes into play, allowing us to create a dashboard using data from Dremio. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:8:1","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#powerbi-dashboard"},{"categories":null,"content":"ApplicationFinally, we can build a simple application to implement the machine learning model and integrate the dashboard into this application, creating a comprehensive portal. We will use Streamlit to build a simple web app. ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:9:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#application"},{"categories":null,"content":"Final ThoughtsThis project is primarily for learning purposes, so the dataset size and some architectural setups may not be fully complete. However, this marks an important milestone in OG’s learning journey. Hopefully, it will be helpful as a reference for others! 😄 -Meww- ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:10:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#final-thoughts"},{"categories":null,"content":"Related Stock Analysis Basic analyzing VN30 stock using PCA and K-Means Read more... Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"10 Dec 2023","objectID":"/spotify_analysis_en/:0:0","series":[],"tags":["streamlit","API","ELT","Data Engineer","Machine Learning","Distributed System","Hadoop","Spark","Docker","Terraform","Visualization","PowerBI"],"title":"Spotify Analysis","uri":"/spotify_analysis_en/#related"},{"categories":[],"content":"Khi biết đến Linux, tôi đã yêu em lúc nào không hay","date":"27 Sep 2023","objectID":"/linux/","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/"},{"categories":[],"content":"Xin chào xin chào là OG đâyy ! Ở bài trước OG đã kể cho các bạn nghe về các khái niệm cơ bản trong ngành Data rồi. Thế thì ở số này mình sẽ nói nhiều hơn về viên gạch đầu tiên khi mình đến với Data Engineer. Đó chính là việc mình biết đến hệ điều hành Linux, kể từ đó cuộc đời mình đổi thay. Lets goo! ","date":"27 Sep 2023","objectID":"/linux/:0:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#"},{"categories":[],"content":"Quá khứ bi đátTrước khi đến với người anh bạn cánh cụt cư tê kia, mình đã từng làm việc với chiếc laptop cùi bắp (intel pentium) win 8.1. Khi đó OG vừa bước chân vào giảng đường đại học, và cứ nghĩ rằng thế này đã đủ xài, có thể hơi chậm chạp nhưng có lẽ sẽ sống ổn. Ôi thật ngây thơ 😢 ngay nửa học kì đầu tiên của năm học mới, cái laptop đó đã dốc từng nhịp thở và chạy chương trình bằng cả tính mạng. Đó là trải nghiệm học tập không mấy dễ chịu khi phải đối mặt với con lap ì ạch như vậy trong suốt một khoảng thời gian dài. Và một combo hủy diệt đi kèm với sự ì ạch đó: hệ điều hành WINDOW, đặc biệt là phiên bản 8 và 8.1 . OG trước đây là một fan của window, phải thú thực là vậy. Giao diện dễ dùng, thân thiện. Nhưng có 1 vấn đề, nó quá nặng và chậm chạp (ít nhất là đối với laptop của OG). Còn cả về lỗi và các bản update mà chỉ khi dùng window, OG mới cảm nhận được sự đau khổ. Thế là có 1 ngày nọ, một người bạn đã chỉ mình chuyển sang Linux và cuộc sống lập trình của mình bước sang một trang mới toanh. ","date":"27 Sep 2023","objectID":"/linux/:1:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#quá-khứ-bi-đát"},{"categories":[],"content":"Linux - Tiếng sét ái tìnhTrước khi được giới thiệu về Linux, OG hoàn toàn không biết gì về nó trước đó. Từ Terminal, Distro,… Hoàn toàn là tờ giấy trắng tinh. Tuy nhiên OG vẫn chọn hệ điều hành này vì nó NHANH, rất nhanh là đằng khác. Ngay khoảnh khắc đó, OG đã không biết mình đã mê đắm hđh này mất rồi 😘 Và sau một thời gian học tập và làm việc với anh bạn 🐧 cư tê thì OG đã không còn muốn quay lại cửa sổ (window) lần nào nữa. Vậy Linux có gì hay hãy đọc tiếp nhé ","date":"27 Sep 2023","objectID":"/linux/:2:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#linux---tiếng-sét-ái-tình"},{"categories":[],"content":"Hệ điều hành chất chơi người dơiLinux là một hệ điều hành được phát triển từ năm 1991 bởi Linus Torvalds dựa trên hệ điều hành Unix. Chú cánh cụt này được viết bằng C và đảm nhận nhiệm vụ nhận các request từ chương trình trên máy tính và chuyển chúng đến phần cứng. Điều đặc biệt nhất có lẽ là nó được phát hành miễn phí (open source). Nghĩa là bạn có thể dễ dàng cài Linux một cách quang minh chính đại mà không cần phải crack hay “Đi cửa sau” với nhiều rủi ro lỗi như khi sử dụng hđh khác. Thậm chí bạn có thể chỉnh sửa, đóng góp xây dựng thêm cho hệ điều hành này nếu muốn. Không những vậy, cộng đồng hỗ trợ của hệ điều hành này cũng rộng lớn và vô cùng “bá đạo” về nhiều khía cạnh 😂 Bạn sẽ có thể gần như ngay lập tức được giải đáp hầu hết các thắc mắc hay trục trặc khi sử dụng linux. Cộng đồng này đa số là các lập trình viên, nhà phát triển,… Vì chú cánh cụt này hệ điều hành ưu thích và thích hợp để phát triển sản phẩm hoặc xây dựng hệ thống nhờ là ứng dụng mã nguồn mở và tính bảo mật cao. Khi chuyển sang dùng Linux, bạn cũng không cần phải quá lo lắng về việc thay đổi môi trường và thích nghi với hệ điều hành này. Vì cơ bản Linux vẫn giống với những hệ điều hành trước đây bạn dùng thôi. Bạn vẫn có thể thao tác với giao diện hệ điều hành, vẫn sử dụng được các phần mềm như xử lý văn bản, edit video hay ảnh,… Nghĩa là các thao tác sử dụng thông thường, Linux vẫn đáp ứng tốt và còn rất nhanh. ","date":"27 Sep 2023","objectID":"/linux/:2:1","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#hệ-điều-hành-chất-chơi-người-dơi"},{"categories":[],"content":"ShellHẳn là có thể đâu đó, bạn thấy một anh chàng ngầu lòi nào đó bấm vài dòng lệnh, Bụp enter. Rồi tự nhiên một đống chữ xuất hiện dồn dập trên cái nền terminal tím. Ồ có thể anh ta đang sử dụng shell đấy. Hoặc là một lập trình viên, bạn đã quá chán với giao diện Command Prompt và PowerShell ? Hãy chuyển sang Linux và bạn sẽ bước vào thế giới huyền diệu khi sử dụng terminal. Shell là một chương trình phát triển dành cho các máy tính chạy trên hệ điều hành Unix và Linux. Phần mềm này cũng cấp giao diện người dùng nhập và giao tiếp với máy tính dưới dạng văn bản. Và trên các máy tính Ubuntu thì Shell cũng có thể gọi là Terminal. Đối với người dùng Linux, việc thao tác với terminal là điều gần như hiển nhiên. Đặc biệt thích hợp cho các “cót đơ” hay nhà phát triển. Việc chạy lệnh trên Terminal ngoài việc trông có vẻ “nguy hiểm” hơn, trải nghiệm sử dụng shell cũng được cho là thoải mái hơn trên window rất nhiều. Không những vậy, với một cộng đồng cực lớn, việc làm quen với shell dường như đã trở nên dễ dàng hơn bao giờ hết. Việc sử dụng shell như thế nào có lẽ OG sẽ để dành lại vào 1 bài khác nhé hihi ","date":"27 Sep 2023","objectID":"/linux/:2:2","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#shell"},{"categories":[],"content":"Đa dạng phiên bảnVì là sản phẩm mã nguồn mở, Linux có rất nhiều phiên bản khác nhau. Các Linux Distro là phần đã góp phần vào sự thú vị nói chung của hệ điều hành này và giúp cho chúng ta có đa dạng sự chọn lựa cho phiên bản mình yêu thích. Hiện có khoảng 600 bản Distro và hơn nửa trong số đó được liên tục phát triển và cải thiện. Có đa dạng các distro và phù hợp với đa dạng các nền tảng khác nhau từ desktop, laptop, di động,… Và cũng nhắm tới phục vụ cho đa dạng các đối tượng khác nhau. Hãy cùng xem qua một số distro phổ biến nhất nào Ubuntu: đây là phiên bản đông đảo người dùng sủ dụng nhất. Đây chính là một nhánh của Debian Linux. Ubuntu có giao diện dễ nhìn và dễ tiếp cận, do đó người dùng mới không khó để làm quen và sử dụng. Hơn nữa là cộng đồng sử dụng lớn nên bạn sẽ dễ dàng tìm được giải đáp cho các thắc mắc mình trong khi sử dụng. Và OG cũng đang sử dụng Distro này 😄 Linux Mint: Là một trong những distro tốt nhất dành do người dùng Linux mới. Giao diện Desktop của Mint rất là Window, tạo cảm giác vô cùng quen thuộc và dễ thích nghi đối với người dùng mới chuyển hoặc ưu thích Window. Đây cũng là một trong những điểm đặc sắc của distro này. Ngoài ra Linux Mint dựa trên Ubuntu, do đó cũng có thể chạy ứng dụng dành cho Ubuntu. Fedora: Trước đây gọi là Fedora core, được phát triển dựa trên cộng đồng theo Fedora Project và được bảo trợ bởi Red Hat (Công ty con của IBM). Đây là một trong những bản phân phối có tốc độ nhanh và ổn định nhất. Được ưu chuộng bởi các nhà phát triển. CentOS: Một trong những phân phối Linux dành cho môi trường server, CentOS dựa trên mã nguồn mở của Red Hat Enterprise Linux (RHEL) và miễn phí. CentOS có độ ổn định cao và được nhiều công ty ưu chuộng. Arch Linux: Bản phân phối này là một “Rolling Release”, đại loại như là nó sẽ luôn được cung cấp bản cập nhật phần mềm sớm nhất có thể và cải thiện tốt nhất. Điều đặc biệt của distro này là khả năng cá nhân hóa rất cao, bạn thậm chí sẽ không nhận được giao diện đồ họa khi mới bắt đầu. Đổi lại bạn sẽ có thể thiết kế, xây dựng theo sở thích của chính mình. Và còn rất rất nhiều phân phối khác thú vị nữa mà kể ra chắc phải thành sách mất 😂 ","date":"27 Sep 2023","objectID":"/linux/:2:3","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#đa-dạng-phiên-bản"},{"categories":[],"content":"Linux vs WindowHiện nay có 3 hệ điều hành phổ biến là Window, Linux, MacOS. Đối với Mac thì OG chưa trải nghiệm (vì mình nghèo 😢). Do đó chúng ta sẽ so sánh trải nghiệm giữa 2 hệ điều hành này theo cảm nhận của OG sau một thời gian sử dụng Linux và cả Window nhé Đối tượng sử dụng: Đối với Window, đã quá phổ biến rồi, do đó đối tượng hướng đến cũng rất rộng rãi già trẻ lớn bé đều có thể sử dụng dễ dàng sử dụng. Ngược lại, những người sử dụng Linux thường là các lập trình viên và các nhà phát triển. Họ là người hiểu về hệ thống và có thể kiểm soát hệ thống. Bash Shell: Đầu tiên, đối với việc lập trình cần phải thao tác nhiều với hệ thống thì việc sử dụng cmd hay PowerShell gần như là 1 trải nghiệm “đồ đá” đối với OG. Window 10 có hỗ trợ cmd và PowerShell nhưng cả 2 loại này đều có giao diện cũ kĩ và cú pháp dài dòng, khó nhớ. Trong khi đó, sử dụng Bash Shell ở Linux đưa lại trải nghiệm mượt mà hơn với câu lệnh dễ nhớ, màu sắc cũng đa dạng hơn và tất nhiên là cũng dễ dùng hơn rất nhiều. Xử lý, can thiệp mã nguồn: Trong khi sử dụng Window, việc truy cập mã nguồn gần như là bất khả thi. Thì đối với người dùng Linux lại ngược lại, bạn hoàn toàn có khả năng truy cập hay chỉnh sửa mã nguồn đến tận nhân. Việc cấu hình hệ thống theo sở thích cá nhân hoàn toàn là điều khả thi và tạo tính linh hoạt khi sử dụng. Xử lý văn bản, hình ảnh: Về phương diện này, phải nói là không thể qua được Microsoft Office 365 của Window, tất nhiên Linux cũng có LibreOffice, tuy nhiên phần mềm này vẫn còn khá hạn chế và kém. Ngoài ra việc sử dụng các công cụ chỉnh sửa hình ảnh, video như Adobe Photoshops hay Adobe Premiere trên linux cũng là không thể. Gaming: Thành thật mà nói, nếu bạn sử dụng linux, thì hãy tập bỏ dần thói quen chơi game đi 😂 vì hầu hết các game hỗ trợ tốt nhất trên hệ điều hành Window Độ bảo mật: Tất nhiên, không thể phủ nhận Window là một hệ điều hành cực kỳ bảo mật. Nhưng nó luôn là mục tiêu của các cuộc tấn công vì độ phổ biến quá lớn của nó. Trong khi người dùng Linux hầu hết là các lập trình viên và tất nhiên là cả Hacker. Okay nói quá trời nói thì tóm gọn qua bảng sau đây: Tiêu chí Window Linux Đối tượng Người dùng phổ thông với nhu cầu cơ bản: lướt web, công việc văn phòng,… Lập trình viên, nhà phát triển, hacker,… Giao diện Command Line Thô sơ, khó dùng với Command Prompt, PowerShell Mạnh mẽ với nhiều loại shells (Bash, Zsh,…) Khả năng tùy chỉnh mã nguồn Không thể Hoàn toàn có thể, giúp cá nhân hóa và tạo tính linh hoạt Xử lý văn bản, phim ảnh Rất tốt với bộ Office 365, Adobe Photoshops,… Có thể sử dụng LibreOffice chỉnh sửa cơ bản nhưng chức năng ít Game Hỗ trợ tốt Khá khó khăn Độ bảo mật Tốt, nhưng dễ bị nhắm tới Tốt và có tính bảo mật cao Việc học lập trình hay quản lý hệ thông, có lẽ Linux sẽ là lựa chọn hoàn hảo hơn (hoặc nếu có điều kiện, hãy mua Mac 😄 thật đấy !). Còn nếu bạn là người dùng phổ thông, không có nhu cầu phải biết về lập trình, quản lý hạ tầng,… thì cứ cửa sổ mà dùng thôi hihi ","date":"27 Sep 2023","objectID":"/linux/:2:4","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#linux-vs-window"},{"categories":[],"content":"Muốn thì tìm cáchNếu bạn là một người dùng Mac, chúc mừng bạn đã quay vào ô an toàn 😄 sướng nhất bạn rồi hi. Còn nếu sử dụng Window mà bất giác có một tình yêu với Linux thì phải làm sao đây ? Không lẽ phải bỏ Win cài lại Linux sao ? Câu trả lời là Không. Hiện nay có 3 cách chính để bạn có thể sử dụng Linux mà không cần phải từ bỏ chiếc cửa sổ của mình: Dùng máy ảo: Việc đầu tiên mà hầu hết mọi người nghĩ tới khi dùng một hệ điều hành khác với hệ điều hành chính của máy chính là dùng một con máy ảo. Bạn có thể sử dụng VirtualBox hoặc VMware PLayer một cách miễn phí để cài đặt. Nhưng với OG, sử dụng máy ảo chả khác nào một cực hình, vì nó chậm kinh khủng khiếp. Vì loại máy ảo này chạy trên hệ điều hành máy và sử dụng công nghệ ảo hóa của CPU để tạo ra các máy ảo khác. Cơ bản là phải thông qua một OS trung gian (Window), do đó tốc độ của nó thực sự như rùa. Dùng dual boot: Cơ bản cách này là cài đặt cho máy tính bạn chạy được cả 2 hệ điều hành cùng một lúc, cách này được khá nhiều người sử dụng và tất nhiên là nhanh hơn việc sử dụng máy ảo. Dùng WSL: hay còn được gọi là Window Subsystem for Linux, về bản chất mà nói thì đây cũng là một loại máy ảo. Tuy nhiên nó là loại máy ảo chạy trên một nền tảng ảo hóa của CPU là Hyper-V, và không hề thông qua hệ điều hành trung gian nào. Có thể hiểu là công nghệ Hyper-V là công nghệ ảo hóa của CPU cung cấp các tầng ảo hóa, kể cả cho chính Window của bạn. Việc sử dụng máy ảo loại này thực sự nhanh hơn rất nhiều và cũng không cần phải restart máy để chuyển sang OS khác như cách dual boot mà đơn giản chỉ là tắt wsl trên PowerShell đi là xong. Quá đã phải không nào. Đối với OG thì mình đang dùng cách 3 để có thể sử dụng Linux trên Window một cách hoàn toàn miễn phí và trải nghiệm vô cùng ổn định. Microsoft đã tạo ra WSL để giúp giải tỏa cơn khát Linux của người dùng Window và giúp cho cuộc sống các lập trình viên thêm ngọt ngào dễ sống hơn 😂 Hoặc bạn có thể ghé thăm page Ubunchuu trường ú để tìm hiểu tất tần tật mọi thứ về Linux và Ubuntu nhé. Đây là kênh do một nhóm các sinh viên đam mê với linux của HCMUS thành lập nhằm tạo cộng đồng linux giúp đỡ lẫn nhau. Và tất nhiên là hướng đến các bạn người mới, newbie với Ubuntu rồi. ","date":"27 Sep 2023","objectID":"/linux/:3:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#muốn-thì-tìm-cách"},{"categories":[],"content":"Cúng cùiVà đó là tất cả cảm nhận của OG về việc sử dụng Linux, cũng như là giới thiệu một chút về chú cánh cụt 🐧 cư tê này. Tất cả chỉ là cảm nhận cá nhân của OG trong quá trình sử dụng Linux, hy vọng rằng sẽ giúp bạn cảm thấy thú vị. -Mew- ","date":"27 Sep 2023","objectID":"/linux/:4:0","series":["Data lú"],"tags":[],"title":"Linux - tiếng sét ái tình","uri":"/linux/#cúng-cùi"},{"categories":[],"content":"Data lú #1 Thằng nhóc thích code và data Ngành Data có gì hot mà mình lại dính Read more... #2 Linux - Tiếng sét ái tình Khi biết đến Linux, tôi đã yêu em lúc nào không hay Read more... ","date":"24 Sep 2023","objectID":"/blogs/:0:0","series":[""],"tags":[],"title":"Blogs","uri":"/blogs/#data-lú"},{"categories":[],"content":"Câu chuyện đời sống Nhìn lại 2023 Quả là một năm gian nan nhưng thật mừng vì mình đã không đầu hàng Read more... Bosch và cuộc hành trình mới Đây là câu chuyện về cuộc hành trình làm một Data Engineer của mình tại Bosch R\u0026D Center Read more... ","date":"24 Sep 2023","objectID":"/blogs/:0:0","series":[""],"tags":[],"title":"Blogs","uri":"/blogs/#câu-chuyện-đời-sống"},{"categories":["projects"],"content":"Warning Đây là kiến thức tích góp từ nhiều nguồn và tìm hiểu của nhóm OG, tất nhiên không thể tránh khỏi sai sót. Hy vọng bài viết lần này thú vị và giúp bạn đọc thư giãn, tham khảo. PhongHuynh0394 Stock-Analysis Hé lô hé lô là OG đâyy, ở phần 1, ta đã giảm chiều dữ liệu bằng PCA rồi, tiếp đến phần này, chúng ta sẽ dùng K-means để phân cụm rồi tìm ra điểm chung của dữ liệu nhé, cuối cùng là phân tích các “sự kiện” đã diễn ra trong từng cụm. Lét gô ! Một lần nữa xin cảm ơn thầy Nguyễn Hoàng Đức và thầy Ngô Minh Mẫn đã hỗ trợ để đồ án được hoàn thiện. Đồng thời cảm ơn các thành viên nhóm 7 đã cùng làm việc hết mình. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:0:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#"},{"categories":["projects"],"content":"K-Means ClusteringNhắc lại chút xíu, ở Phần 1 chúng ta đã dùng PCA làm giảm chiều dữ liệu nhưng xét về mặt ý nghĩa, dữ liệu sau PCA chưa thể phân tích được mà ta sẽ chỉ dùng nó để áp vào một kỹ thuật tiếp theo. Kỹ thuật này sẽ “gộp nhóm” dữ liệu và trả lời câu hỏi “làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau, sao cho dữ liệu trong cùng một cụm có tính chất giống nhau?” Bản chất của việc phân nhóm này dựa trên những biến động, sự kiện nào đó trên thị trường mà chúng ta chưa biết nhưng nó chi phối trực tiếp hay gián tiếp, nhiều hay ít đến với các cổ phiếu có liên quan. Chính vì lý do đó, chúng ta tiến hành sử dụng thuật toán K-Means Clustering để phân cụm dữ liệu phục vụ mục tiêu đề ra. Trong bài này chúng ta sẽ thực hiện thuật K-means step by step, nào bắt đầu thoaii 😄 ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#k-means-clustering"},{"categories":["projects"],"content":"K-Means step by stepStep 1: Lựa chọn số clusters $K$ # initialize labels N, d = X_pca.shape pre_labels = np.zeros((N, 1)) # Hyper-parameters K_CLUSTERS = 3 # \u003c\u003c N Step 2: Chọn K điểm ngẫu nhiên từ dữ liệu làm trọng tâm import random # random K-samples to be centroids k_indices = random.sample(range(0, N), K_CLUSTERS) centroids = X_pca[k_indices] # shape: (K, d) Step 3: Gán tất cả các điểm cho tâm cụm gần nhất Hàm assign_cluster() giúp ta phân mỗi điểm dữ liệu vào cluster có center gần nó nhất với K điểm bất kỳ được chọn làm các center ban đầu def assign_cluster(distances: np.ndarray) -\u003e np.ndarray: # distances: (N, K) return np.argmin(distances, axis=1) # return min value from distances array Cụ thể: Mảng distances chứa khoảng cách từ điểm $i$ đến cụm $k$, đây là khoảng cách Euclid với công thức distance$(x,y)=||x-y||$ Hàm np.argmin() trả về giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất. Nói cách khác, nó giúp chúng ta tìm ra được vị trí mà điểm dữ liệu phải thuộc về dựa trên quy tắc gần cụm nào nhất thì chọn cụm đó. Ví dụ: # Example for np.argmin() # k = 0 1 2 D = np.array([[1, 0, 3], # min = 0 and it locate in 1 -\u003e x_1 y_1 = 1 [-1, 2, 1]]) # min =-1 and it locate in 0 -\u003e x_2 y_2 = 0 np.argmin(D, axis=1) # --\u003e array([1,0]) Step 4: Tính toán lại các trọng tâm của các cụm mới được hình thành Hàm update_centroids() dùng cập nhật lại tâm cụm và trả về 1 bộ các tâm cụm mới bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó. def update_centroids(X, labels): new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K_CLUSTERS)]) return new_centroids # (3, 13) Cuối cùng: lặp lại step 3 và step 4 Question Khi nào thì bài toán hội tụ vậy OG ? Khi nào thì chúng ta mới dừng lại thuật toán ? –\u003e Câu trả lời là khi việc phân cụm không còn sự thay đổi nào nữa hoặc giá trị hàm mất mát không thay đổi nhiều sau mỗi lần update tâm cụm. Chúng ta sẽ viết một hàm kiểm tra tính hội tụ của bài toán là has_convert(). Hàm này kiểm tra cụm trước và sau có giống nhau không. # Check convergence def has_convert(pre_labels: np.ndarray, cur_labels: np.ndarray) -\u003e bool: return (pre_labels == cur_labels).all() Hàm get_total_wcv() là một hàm mất mát, tính tổng các phương sai bên trong của 1 cluster Nếu ta coi $m_k$ là center (representation) của mỗi cluster và ước lượng tất cả các điểm được phân vào cluster này bởi $m_k$, thì một điểm dữ liệu $x_i$ được phân vào cluster $k$ sẽ bị sai số là $(x_i-m_k)$. Chúng ta mong muốn sai số này có trị tuyệt đối bé nhất nên ta sẽ tìm cách để đại lượng sau đây đạt min: $||x_i - m_k||^2_2$ def get_total_wcv(X, labels, centroids): # Total within cluster variance WCVs = [ np.sum(np.linalg.norm(X[labels == k] - centroids[k], axis=1) ** 2) \\ for k in range(K_CLUSTERS) ] return np.sum(WCVs) Nhìn chung về điều kiện hội tụ có thể thấy mối liên hệ giữa các điều kiện là gần tương đồng như nhau. Khi có ít điểm dữ liệu được gán sang cluster khác có thể khiến điểm trung tâm không thay đổi nhiều và từ đó hàm mất mát cũng sẽ ít bị ảnh hưởng. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:1","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#k-means-step-by-step"},{"categories":["projects"],"content":"The K-means algorithm is written in object-oriented formTa sẽ kết hợp tất cả các bước thuật toán vào một đối tượng KMeansClustering import numpy as np import pandas as pd import random import matplotlib.pyplot as plt from scipy.spatial import distance class KMeansClustering: \"\"\" An instance of K-Means Clustering algorithm \"\"\" def __init__(self, n_clusters=29): \"\"\" n_clusters: number of clusters _centroids: center/ centroid of clusters inertia_: sum of squared distances of samples to their closest cluster center labels_: labels of input samples X: input data \"\"\" self.n_clusters = n_clusters def fit(self, X: np.ndarray) -\u003e None: \"\"\" K-means execution \"\"\" N, p = X.shape self.X = X # random K-samples to be centroids k_indices = random.sample(range(0, N), self.n_clusters) self._centroids = self.X[k_indices] # initialize labels pre_labels = np.zeros((N, 1)) # training it = 0 while True: # compute distances from X_i to distances = self._calc_dists(self.X, self._centroids) # assign new labels self.labels_ = self._assign_cluster(distances) # assign new labels # check convergence if self._has_convert(pre_labels, self.labels_): break # update centroids self._update_centroids(self.X, self.labels_) pre_labels = self.labels_ it += 1 # compute total Within Cluster Variance (WCV) self.inertia_ = self._calc_total_WCV(self.X, self.labels_, self._centroids) def _calc_dists(self, X, centroids): return distance.cdist(X, centroids, \"euclidean\") def _assign_cluster(self, distances): return np.argmin(distances, axis=1) def _update_centroids(self, X, labels): self._centroids = np.array( [X[labels == k].mean(axis=0) for k in range(self.n_clusters)] ) def _calc_total_WCV(self, X, labels, centroids): WCVs = [ np.sum(np.linalg.norm(X[labels == k] - centroids[k], axis=1) ** 2) for k in range(self.n_clusters) ] return np.sum(WCVs) def _has_convert(self, pre_labels, cur_labels): return (pre_labels == cur_labels).all() def _predict(self, X_test): dist_test = self._calc_dists(X_test, self._centroids) test_labels = self._assign_cluster(dist_test) return test_labels def predict(self, X_test): return self._predict(X_test) Trước khi thật sự áp dụng thuật toán này cho dataset của chúng ta, có một câu hỏi đặt ra là: Thuật toán này thực hiện với đầu vào là số $K$ tức là số lượng cluster, thế thì $K$ bằng bao nhiêu là tốt nhất? ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#the-k-means-algorithm-is-written-in-object-oriented-form"},{"categories":["projects"],"content":"Finding the optimal ‘K’ in a K-Means clusteringCó một phương pháp tên là Elbow aka cái khuỷa tay 😄 Phương pháp Elbow là một cách giúp ta lựa chọn được số lượng các cụm phù hợp dựa vào đồ thị trực quan hoá bằng cách nhìn vào sự suy giảm của hàm biến dạng và lựa chọn ra điểm khuỷu tay (elbow point). Đối với mỗi giá trị của $K$, ta tính toán WCSS (Tổng bình phương trong cụm). WCSS là tổng bình phương khoảng cách giữa mỗi điểm và tâm trong một cụm. def elbow_method(X, k_clusters = list(range(1,9))): total_wcss = [] for k in k_clusters: # Train with k cluster kmeans_model = KMeansClustering(n_clusters=k) kmeans_model.fit(X) # calculate WCSS total_wcss.append(kmeans_model.inertia_) plt.figure() plt.plot(k_clusters, total_wcss, marker='o', color='r') plt.ylabel('WCSS') plt.xlabel('Number of clusters K') plt.grid() plt.show() elbow_method(X_pca) Khi ta vẽ đồ thị WCSS với giá trị K, đồ thị trông giống như một khuỷu tay. Khi số cụm tăng lên, giá trị WCSS sẽ bắt đầu giảm. Giá trị WCSS lớn nhất khi $K=1$ elbow_method Chúng ta có thể thấy rằng biểu đồ sẽ thay đổi nhanh chóng tại $K=2$ và do đó tạo ra hình dạng khuỷu tay. Từ thời điểm này, đồ thị di chuyển gần như song song với trục $X$ Giá trị $K$ tương ứng với điểm này là giá trị tối ưu của $K$ hoặc cụm tối ưu. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#finding-the-optimal-k-in-a-k-means-clustering"},{"categories":["projects"],"content":"Evaluation Metrics of K-Means ClusteringSilhouette Score: cho chúng ta biết những điểm dữ liệu hay những quan sát nào nằm gọn bên trong cụm (tốt) hay nằm gần ngoài rìa cụm (không tốt) để đánh giá hiệu quả phân cụm. Giả sử có 2 cluster A, B thì Silhouette score là: $$ s_i = \\frac{(b_i - a_i)}{max(b_i - a_i)} $$ Với $a_i, b_i$ lần lượt là khoảng cách của điể $i$ đến tâm cụm A và B Giá trị này nằm trong khoảng [-1, 1]: Điểm dữ liệu có Silhouette cao, gần bằng 1: nằm đúng trong cluster. Điểm dữ liệu có Silhouette gần bằng 0: nằm giữa 2 cluster Điểm dữ liệu có Silhouette thấp, có giá trị âm: thì khả năng đã nằm sai cluster. from sklearn import metrics kmeans_train = KMeansClustering(2) kmeans_train.fit(X_pca) labels = kmeans_train.labels_ print(f'Silhouette Score (n = 2): {metrics.silhouette_score(X_pca,labels)}') #--\u003e Silhouette Score (n = 2): 0.265365408366845 kmeans_train = KMeansClustering(3) kmeans_train.fit(X_pca) labels = kmeans_train.labels_ print(f'Silhouette Score (n = 3): {metrics.silhouette_score(X_pca,labels)}') # --\u003e Silhouette Score (n = 3): 0.22528216696570613 Ở Phương pháp Elbow, chúng ta cũng đã thấy được $K$ tối ưu cho bài toán chính là khi chọn $K=2$. Nhưng ở bước này, ta cũng thấy được rõ ràng hơn giữa 2 sự lựa chọn $K=2$ và $K=3$ bằng cách so sánh chỉ số Silhouette giữa bọn chúng. So sánh: $0.265(K=2) \u003e 0.225(K=3)$ Từ đó, một lần nữa ta thấy được $K$ tối ưu nhất cho bài toán này là $K=2$ ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:4","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#evaluation-metrics-of-k-means-clustering"},{"categories":["projects"],"content":"VisualizeSau tất cả các bước trên, dữ liệu đã được chia thành 2 label chính tương ứng với từng màu sắc được biểu thị dưới hình vẽ sau đây: # K means import plotly.express as px K_CLUSTERS = 2 kmeans_train = KMeansClustering(K_CLUSTERS) kmeans_train.fit(X_pca) fig = px.scatter_matrix( X_pca, labels=pca_scree, dimensions=range(4), color=kmeans_train.labels_ ) fig.update_traces(diagonal_visible=False) fig.show() Kmeans Clustering Sau khi chia cụm, chúng ta đến với bước cuối cùng thôi ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:1:5","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#visualize"},{"categories":["projects"],"content":"Data AnalysisSau khi K-means, ta có được một tập nhãn (labels), ta sẽ gán các nhãn này vào bộ data có giá trị và tiến hành phân tích từng cụm. # Data #set label value_data = new_market.copy() value_data['label'] = kmeans_train.labels_ value_data['label'].replace({0: 'A', 1: 'B'}, inplace=True) # get specific columns gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] mua = [col for col in value_data.columns.to_list() if 'total_mua' in col] ban = [col for col in value_data.columns.to_list() if 'total_ban' in col] # create new valuabel column value_data['total_ban_30'] = value_data[ban].sum(axis=1) value_data['total_mua_30'] = value_data[mua].sum(axis=1) value_data['total_volume_30'] = value_data['total_ban_30'] + value_data['total_mua_30'] value_data['mua_ban_ratio'] = value_data['total_mua_30'] / value_data['total_ban_30'] value_data bao gồm các cột: label: nhãn nhận được từ K means (gồm ‘A’ và ‘B’) gttb_{mã}: Trung bình giá của một {mã} cổ phiếu total_mua_{mã}: Tổng khối lượng mua của {mã} cổ phiếu total_ban_{mã}: Tổng khối lượng bán của {mã} cổ phiếu total_mua_30: Tổng khối lượng mua của tất cả 30 cổ phiếu total_ban_30: Tổng khối lượng bán của tất cả 30 cổ phiếu total_volume_30: Tổng khối lượng giao dịch của 30 cổ phiếu (total_mua_30 + total_ban_30) mua_ban_ratio: tỉ số total_mua_30 / total_ban_30 Gia KL: Giá phái sinh gần với VN30 Time-series Data theo phút (5154 dòng) ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#data-analysis"},{"categories":["projects"],"content":"Analytical Overview import plotly.graph_objects as go import plotly.express as px from plotly.subplots import make_subplots fig = make_subplots(rows=2, cols=2, vertical_spacing=0.03) # plot A fig.add_trace(go.Bar(name='',x=value_data.index[value_data['label'] == 'A'], y=value_data['total_volume_30'][value_data['label'] == 'A']), row=1, col=1) fig.add_trace(go.Scatter(x=value_data.index[value_data['label'] == 'A'], y=value_data['Gia KL'][value_data['label'] == 'A'], mode='lines'), row=2, col=1) # plot B fig.add_trace(go.Bar(name='',x=value_data.index[value_data['label'] == 'B'], y=value_data['total_volume_30'][value_data['label'] == 'B']), row=1, col=2) fig.add_trace(go.Scatter(x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines'), row=2, col=2) # update layout fig.update_layout(title_text=\"Stock Data Visualization\", showlegend=False, barmode='stack') fig.update_yaxes(title_text=\"Total Volume 30\", row=1, col=1) fig.update_yaxes(title_text=\"Gia KL\", row=2, col=1) fig.update_yaxes(title_text=\"Total Volume 30\", row=1, col=2) fig.update_yaxes(title_text=\"Gia KL\", row=2, col=2) fig.show() Stock Data Visualization Cluster B chứa các dữ liệu trong giai đoạn từ 20/3 đến 6/4/2023 Nhìn nhận một cách tổng quát: Đối với cluster B, thị trường luôn trong thế “giằng co” với khoảng 6 đợt giảm mạnh và chừng ấy đợt phục hồi liên tục trong suốt 18 ngày. Mặc dù GiaKL duy trì được đà tăng (Giá KL từ 1036 tăng đến 1080) tạo tích cực cho thị trường song thế giằng co vẫn có chiều hướng kéo dài. Cluster A chứa các dữ liệu trong giai đoạn từ 1/4 đến 19/4/2023: Nhìn nhận một cách tổng quát: Đối với cluster A các khoảng giảm kéo dài trong nhiều ngày dẫn đến việc phục hồi gặp khó khăn đáng kể. Cụ thể, GiaKL có mức “vùng dậy” đến ngưỡng 1086 điểm tại 6/4 nhưng lại bị đẩy về 1054 vào cuối giai đoạn. Ta sẽ phân tích sâu hơn từng cluster một ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:1","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#analytical-overview"},{"categories":["projects"],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # --\u003e 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # --\u003e -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # --\u003e -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn trên thị trường quốc tế và phải chịu mức chi phí tài chính cao hơn. Ngày 31/3/2023, Ngân hàng Nhà nước Việt Nam điều chỉnh giảm các mức lã","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#cluster-b"},{"categories":["projects"],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # -- 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn trên thị trường quốc tế và phải chịu mức chi phí tài chính cao hơn. Ngày 31/3/2023, Ngân hàng Nhà nước Việt Nam điều chỉnh giảm các mức lã","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#biến-động-thị-trường"},{"categories":["projects"],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # -- 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn trên thị trường quốc tế và phải chịu mức chi phí tài chính cao hơn. Ngày 31/3/2023, Ngân hàng Nhà nước Việt Nam điều chỉnh giảm các mức lã","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#các-sự-kiện-ảnh-hưởng"},{"categories":["projects"],"content":"Cluster B import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'B'], y=gtb, title='Mean Values by Label B') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label B import plotly.graph_objs as go # Create first trace trace1 = go.Bar(name='mua', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_ban_30'][value_data['label'] == 'B']) # Create second trace trace2 = go.Bar(name='ban', x=value_data.index[value_data['label'] == 'B'], y=value_data['total_mua_30'][value_data['label'] == 'B']) # Create third trace trace3 = go.Scatter(name=\"GiaKL\",x=value_data.index[value_data['label'] == 'B'], y=value_data['Gia KL'][value_data['label'] == 'B'], mode='lines', yaxis='y2') # Create layout layout = go.Layout(title='The chart shows the correlation between Total Volumn and GiaKL', xaxis=dict(title='Times'), yaxis=dict(title='Total Volumn', side='left'), yaxis2=dict(title='Gia KL', side='right', overlaying='y'), barmode=\"stack\") # Add traces to figure fig = go.Figure(data=[trace1, trace2, trace3], layout=layout) # Show figure fig.show() B-Correlation between Total Volumn and GiaKL Biến động thị trường phantramtangtruong = (1081.55 - 1014.331)/1014.331 print(\"Gia tri cua phien cao nhat so voi phien nho nhat: \",phantramtangtruong) # -- 0.06626929473712223 Giai đoạn này cho thấy sự gia tăng của giá trị các cổ phiếu, với sức tăng lớn nhất trong ngưỡng tiệm cận 0.07% cho thấy sự hấp dẫn của thị trường đang trên đà phục hồi. Vào cuối tháng 3, nhóm dịch vụ tài chính tiếp tục tăng điểm khá mạnh, nhóm ngân hàng cũng có diễn biến tích cực, nhóm bất động sản với thông tin khả năng tiếp tục hạ lãi suất gúp thanh khoản cải thiện tốt, nhiều mã có tín hiệu thoát khỏi xu hướng giảm giá trung hạn kéo dài… Với các phiên tăng giảm đan xen, khối lượng giao dịch về tổng thể trung hạn vẫn đang ở mức thấp. Giai đoạn cuối tháng 3, khối lượng giao dịch đang trong xu hướng giảm, cho thấy các trader đang giảm dần các vị thế nắm giữ và có thể dịch chuyển sang ở thị trường cơ sở khi trên thị trường cơ sở đang có nhiều cơ hội sinh lợi ngắn hạn tốt. Thị trường đã hồi phục 6 phiên liên tiếp dù điểm số tăng từng phiên không lớn nhưng đủ giúp thị trường quay trở lại kênh tăng giá ngắn hạn và nằm trên đường MA20. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.637694475209741 total_ban_theo_ngay = value_data.total_mua_30[value_data['label'] == 'B'].to_frame() ngay_dau = total_ban_theo_ngay['total_mua_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_mua_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu mua vao cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.8043500703402909 Có thể thấy, số cổ phiếu được bán ra ở cuối giai đoạn (đầu tháng 4) so với tháng 3 có sự sụt giảm. Cổ phiếu được mua vào cũng tăng trưởng âm, các nhà đầu tư buộc phải giữ lại các cổ phiếu còn tồn đọng tạo nên một thế kìm hãm thanh khoản. Do đó, trong tương lai gần sẽ chịu tác động của sự chênh lệch này. Các sự kiện ảnh hưởng Ngày 22-23/3/2023, lần thứ 9 liên tiếp FED tăng lãi suất, nâng phạm vi lãi suất lên mức dao động từ 4,75% đến 5%. Mục đích để trấn an thị trường, giữ uy tín và chống lạm phát. Trong bối cảnh thanh khoản trên thị trường tài chính quốc tế thắt chặt hơn, Chính phủ và các doanh nghiệp Việt Nam sẽ khó huy động vốn trên thị trường quốc tế và phải chịu mức chi phí tài chính cao hơn. Ngày 31/3/2023, Ngân hàng Nhà nước Việt Nam điều chỉnh giảm các mức lã","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:2","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#kết-luận-xu-hướng"},{"categories":["projects"],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label A A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # --\u003e 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # --\u003e -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục duy trì trong phiên này khi Thời điểm cuối cùng là xấp xỉ 1.055 điểm. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#cluster-a"},{"categories":["projects"],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label A A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # -- 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục duy trì trong phiên này khi Thời điểm cuối cùng là xấp xỉ 1.055 điểm. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#biến-động-thị-trường-1"},{"categories":["projects"],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label A A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # -- 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục duy trì trong phiên này khi Thời điểm cuối cùng là xấp xỉ 1.055 điểm. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#sự-kiện-ảnh-hưởng"},{"categories":["projects"],"content":"Cluster AHãy cùng xem qua Cluster A nhé import plotly.express as px gtb = [col for col in value_data.columns.to_list() if 'gttb_' in col] fig = px.line(value_data[gtb][value_data['label'] == 'A'], y=gtb, title='Mean Values by Label A') fig.update_layout(xaxis_title=\"Times\", yaxis_title=\"Mean Values\", font=dict(family=\"Courier New, monospace\", size=10, color=\"RebeccaPurple\")) fig.show() Mean Values by Label A A-Correlation between Total Volumn and GiaKL Biến động thị trường tb_SAB = value_data['gttb_SAB'].mean() print(\"Gia tri trung binh cua co phieu SAB\",tb_SAB) # -- 179.1409610698177 Đối với giá cổ phiếu trung bình: Có thể thấy giá trung bình của cổ phiếu SAB (Tổng Công ty cổ phần Bia - Rượu - Nước giải khát Sài Gòn SABECO) có giao động khá ổn định xung quanh giá trị 165K. Hơn nữa giá trung bình của SABECO trong tháng 4/2023 là cao hơn hẳn so với các cổ phiếu khác trong VN30, chứng tỏ thị phần của công ty đó trong ngành có thể đang tăng lên, và có thể được đánh giá cao bởi các nhà đầu tư vì nó có khả năng tăng trưởng mạnh trong tương lai. Tuy nhiên từ đầu tháng 4 về sau, đồng hành cùng với SAB là khoảng 13 mã cổ phiếu khác như VJC, GAS, VCB… đang có xu hướng giảm giá nhẹ. Một số mã cổ phiếu như VCB có sự giao động khá mạnh mẽ khi liên tục xuống ngưỡng 44K/1 cổ phiếu trong các ngày 20, 21/03/2023. Và từ đầu tháng 4 là sự tuột dốc khá nhanh khi từ khoảng 93K xuống còn tầm 52K. Ngoài ra có thể thấy rằng cổ phiếu NVL của Tập đoàn Đầu tư Địa ốc NOVA khá “ảm đạm” khi ngưỡng thấp nhất là vào ngày 03/04/2023 với 2.7K và cao nhất là xấp xỉ 11K. Có lẽ cổ phiếu NVL đang ngày càng “rớt giá”, chính vì vậy mà các nhà đầu tư nên bán tháo, thị trường chung cũng nhận định nên BÁN mạnh. Đối với khối lượng mua bán cổ phiếu: Từ biểu đồ trên, nhìn sơ lược qua thì ta có thể thấy được khối lượng giao dịch có xu hương tăng dần từ ngày 1/4, chạm mốc cao nhất vào ngày 6/4. Trong khoảng thời gian này, khối lượng giao dịch mua nhiều, điều này chứng tỏ thị trường đang rất quan tâm và kỳ vọng vào sự tăng giá của các cổ phiếu. Giá phái sinh và khối lượng giao dịch có một sự tương quan với nhau, việc khối lượng giao dịch mua tăng lên, giá phái sinh trong khoảng thời gian này cũng đã được đẩy lên. Sau sự gia tăng giá đến một mức cao nhất là vào ngày 6/4, thì sau đó giá phái sinh lại khá ảm đạm. Sau ngày 17/4 thì bắt đầu có dấu hiệu giảm mạnh và thấp nhất vào ngày 19/4. Trong khoảng thời gian này, khối lượng giao dịch cũng giảm dần, khối lượng giao dịch bán tăng nhiều hơn so với kì trước. Mức giảm này nhận định là khá mạnh, trong khi khối lượng giao dịch mua lại không nhiều. Thị trường không có nhiều điểm nhấn trong bối cảnh phục hồi thanh khoán thấp. total_ban_theo_ngay = value_data.total_ban_30[value_data['label'] == 'A'].to_frame() ngay_dau = total_ban_theo_ngay['total_ban_30'].iat[0] ngay_cuoi = total_ban_theo_ngay['total_ban_30'].iat[-1] phantramtangtruong = (ngay_cuoi - ngay_dau)/ngay_dau print(\"Tong so co phieu ban ra cua ngay cuoi giai doan so voi ngay dau co muc tang truong la: \",phantramtangtruong) # -- -0.7704310701624562 Về biến động giá phái sinh: VN30Index phiên mở đầu tháng 4 giao dịch tăng điểm tích cực và duy trì đà tăng đến cuối phiên. Số liệu cuối cùng ghi nhận được vào ngày 03/04/2023 là 1.081,28 điểm (+1.38% so với 1.070,14 vào 31/03/2023) để hướng đến 1.085-1.095 điểm. Tuy nhiên, vào tuần thứ 2 từ 14/04/2023 VN30Index biến động tiêu cực hơn. Phiên đầu tuần, mở cửa ở vùng giá 1.073 điểm và chịu áp lực điều chỉnh ở vùng giá 1.070-1.075 điểm để kết thúc tuần giao dịch với phiên giảm điểm mạnh. Tuần điều chỉnh đã diễn ra như dự đoán khi trong tuần qua Vn30Index giảm 16.82 điểm (-1,57%) với khối lượng giao dịch tiếp tục ở mức cao, điểm số điều chỉnh tuy không lớn nhưng trong bối cảnh thị trường đang trong khu vực giao động hẹp thì đợt điều chỉnh này cũng tạo ra cảm giác bất an cho nhà đầu tư. Đến ngày 19/04/2023. Nỗ lực phục hồi từ 2 phiên đầu tuần đã không được tiếp tục duy trì trong phiên này khi Thời điểm cuối cùng là xấp xỉ 1.055 điểm. ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:2:3","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#kết-luận-xu-hướng-1"},{"categories":["projects"],"content":"Kết thúcQua đồ án “Stock Analysis” chúng ta đã thực hành dùng các kỹ thuật như Cleaning data, Scaling, PCA, K-means Clustering… cùng các kỹ năng phân tích dữ liệu để làm rõ insight cùng với tình hình của thị trường chứng khoán phái sinh VN30 trong 31 ngày. Từ đây có thể nhận định xu hướng của chứng khoán trong thời gian tới, đó là xu hướng tích lũy. Một số hợp đồng có thể tham khảo như VN30F2306 và VN30F2Q được dẫn dắt bởi các cổ phiếu thuộc nhóm ngành xây dựng (DIG), ngân hàng (VCB), chứng khoán (SSI)… Cảm ơn bạn đã đọc đến giờ phút này, OG cảm động quá 😄 Hy vọng bài viết này sẽ giúp cho bạn cảm thấy thú vị. Chúc bạn một ngày tốt lành 😄 -Mew- ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:3:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#kết-thúc"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Stock Analysis using PCA and K-means Read more... ","date":"05 Sep 2023","objectID":"/stock_analysis_2/:0:0","series":["Stock Analysis"],"tags":["Machine learning","math"],"title":"Stock Analysis P2","uri":"/stock_analysis_2/#related"},{"categories":["projects"],"content":"Stock Analysis using PCA and K-means","date":"31 Aug 2023","objectID":"/stock_analysis/","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/"},{"categories":["projects"],"content":"Warning Đây là kiến thức tích góp từ nhiều nguồn và nghiên cứu của nhóm OG, tất nhiên không thể tránh khỏi sai sót. Hy vọng bài viết lần này thú vị và giúp bạn đọc thư giãn, tham khảo. Source PhongHuynh0394 Stock-Analysis Hello! OG đây. Ở project lần này mình sẽ phân tích gia trị cổ phiếu phái sinh VN30 Index bằng cách sử dụng PCA và K-means. Xin vô cùng cảm ơn sự đóng góp của 5 thành viên team OG và thầy Minh Mẫn và thầy Hoàng Đức đã tận tình hướng dẫn để team có thể hoàn thành đồ án một cách tốt nhất. Rồi bây giờ gét gô thooiii 😄 ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#"},{"categories":["projects"],"content":"IntroStock Analysis hay còn gọi là Market Analysis đề cập đến phương pháp mà nhà đầu tư hoặc nhà giao dịch sử dụng để đánh giá và điều tra một công cụ giao dịch cụ thể, lĩnh vực đầu tư hoặc toàn bộ thị trường chứng khoán. Không những thế, nó liên quan đến việc nghiên cứu dữ liệu thị trường trong quá khứ và hiện tại và tạo ra một phương pháp để chọn cổ phiếu phù hợp để giao dịch. Các nhà đầu tư sẽ đưa ra quyết định mua hoặc bán dựa trên thông tin phân tích chứng khoán. Trong project này ta sẽ phân tích, trực quan hóa bộ dữ liệu giả định được cung cấp bởi khách hàng để đánh giá thị trường chứng khoán trong khoảng thời gian 1 tháng của 30 công ty thuộc VN30 Dưới đây là tóm tắt sơ lược từng bước để xử lý và phân tích: EDA (Exploratory Data Analysis) Data Preprocessing PCA (Principle Component Analysis) K-Means Clustering Data Analysis References (chi tiết trong notebook ở github) Raw Data Source: df_merged.pkl Raw data là dữ liệu bảng giá cổ phiếu của 30 công ty thuộc VN30 Index + 1 trường giá phái sinh trong 1 tháng ","date":"31 Aug 2023","objectID":"/stock_analysis/:1:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#intro"},{"categories":["projects"],"content":"Exploratory Data AnalysisĐây là bước đầu tiên, chúng ta sẽ cùng nhau tìm hiểu sơ lược raw data cũng như tìm hiểu cái nhìn tổng quát về dữ liệu ta sắp phải phân tích để từ đó có cách tiền xử lý phù hợp. Làm gì thì làm cứ phải import packages để đọc data cái đã ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#exploratory-data-analysis"},{"categories":["projects"],"content":"Data AcquistionTa sẽ import một số packages quen thuộc để đọc file df_merged.pkl import pickle import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd.read_pickle('https://github.com/PhongHuynh0394/My-respository/blob/main/df_merged.pkl?raw=true') # Check the data type type(data) # --\u003e list Data nhận được từ pickle file là một list, bây giờ ta sẽ tìm kiếm cái nhìn tổng quan về dữ liệu này ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-acquistion"},{"categories":["projects"],"content":"A Brief View Dữ liệu lưu ở pickle là một list chứa 23 dataframe (df) Mỗi df có index theo datetime (nghĩa là đây là loại dữ liệu thuộc timeseries) Các columns lần lượt là từng mã cổ phiếu, chứa khối lượng/ giá của các lệnh mua/bán sát với lệnh khớp I và khối lượng của các lệnh mua/bán sát với giá khớp lệnh II print('So luong df:', len(data)) # --\u003e So luong df: 23 Raw data là giá lệnh mua/bán I II và khối lượng giao dịch của cổ phiếu 30 công ty VN30raw data \" Raw data là giá lệnh mua/bán I II và khối lượng giao dịch của cổ phiếu 30 công ty VN30 Thời gian thu thập được cập nhật với chu kì là 10 giây bắt đầu từ ngày 20 tháng 3 đến ngày 19 tháng 4, từ 2 giờ 15 đến 7 giờ 30 mỗi ngày. Nhưng có một số ngày bị miss trong bộ dữ liệu này (Chi tiết hơn trong notebook ở source code) Cùng xem qua về số lượng observations của mỗi bảng Tổng cộng ta có 181 fields và mỗi bảng khoảng 1345 observations (tổng cộng 30538 quan sát). Cũng khá nhiều phải không nào. Ta sẽ cùng tiền xử lý chúng nào ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#a-brief-view"},{"categories":["projects"],"content":"Data PreprocessingSau khi đã biết khái quát raw data, ta sẽ cần phải tiền xử lý những dữ liệu thô này trước khi có thể áp dụng các mô hình máy học hoặc giảm chiều dữ liệu ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-preprocessing"},{"categories":["projects"],"content":"Data CleaningHãy sử dụng method describe() của pandas để có cái nhìn sơ bộ nhất về df của chúng ta data[0].describe() Đầu tiên, ta sẽ drop duplicate và định dạng lại index thời gian market = pd.DataFrame(columns=data[0].columns.to_list()) #create empty df # Data cleaning for _, df in enumerate(data): df.drop_duplicates() cols = df.columns.to_list() #convert/ replace 0 for col in cols: df[col] = pd.to_numeric(df[col], errors='coerce') # #missing handling df.fillna(0, inplace=True) market = pd.concat([market,df]).copy() #concat all clean df into market #datetime format market.reset_index(inplace=True) market = market.rename(columns={'index': 'datetime'}) market['datetime'] = market['datetime'].dt.strftime('%Y-%m-%d%H:%M:%S') market['datetime'] = pd.to_datetime(market['datetime']) market = market.sort_values(\"datetime\", ascending=True) market.set_index('datetime', inplace=True) Kế tiếp hãy xử lý missing value bằng phương pháp nội suy (interpolation) với method padding, và sau đó sẽ dùng backfill Đây là phương pháp ước tính giá trị của các điểm dữ liệu chưa biết trong phạm vi của một tập hợp rời rạc chứa một số điểm dữ liệu đã biết. Nghe có vẻ lằng nhằng, đơn giản là thế này: .interpolate(method=‘pad’): fill null values bằng giá trị liền kề nó lần lượt từ trên xuống (nó giống như ffill()) .fillna(method=‘backfill’): Đây là phương pháp ngược lại bên trên, fill null bằng giá trị liền kề từ dưới lên Note Có rất nhiều phương pháp nội suy như linear (default) hay polynomial,… Nhưng OG chọn padding và backfill vì 2 phương pháp này có thể giữ cho data missing ở giá trị sát nhất với giá trị thực gần nhất và giúp cho kết quả sau khi fill sát với thực tế nhất. Ngoài ra 2 phương pháp này có thể fill được vị trí đầu và cuối cùng một cách hiệu quả. def handle_null(X: pd.DataFrame) -\u003e pd.DataFrame: ''' handle missing value ''' for col in X.columns.to_list(): X[col].interpolate(method='pad', inplace=True) X[col].fillna(method='backfill', inplace=True) return X ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-cleaning"},{"categories":["projects"],"content":"Data transformingOG nhận thấy rằng với các trường data hiện tại chưa thực sự giúp ích quá nhiều trong việc phân tích sau này (giá mua/bán và số lượng mua/bán + giá phái sinh (label) ) Do đó OG cần một dataframe mới với các trường mới có nhiều giá trị phân tích hơn: gttb_ (Giá trị trung bình): là column mới được tính trên bình quân giá cả mua vào, bán ra của từng cổ phiếu được giao dịch THÀNH CÔNG trên thị trường. total_ban \u0026 total_mua (Tổng bán/mua khối lượng 1): là column mới để tính tổng giá bán khối lượng 1 cũng như mua khối lượng 1 của từng cố phiếu được giao dịch trên thị trường. Gia_KL: sao chép giá khối lượng của từng mã cổ phiếu từ bộ dữ liệu ban đầu. (label) Cài đặt lại index thời gian: group by các time-series theo phút. def transform_raw(market: pd.DataFrame) -\u003e pd.DataFrame: # split stock name name = [col.split('_1')[-1] for col in market.columns.to_list() if 'mua_gia_1' in col] new_df = pd.DataFrame() for i in name: # calculate gttb (mean) new_df[f'gttb_{i}'] = ((market[f'mua_gia_1{i}'] * market[f'mua_kl_1{i}'] + market[f'ban_gia_1{i}'] * market[f'ban_kl_1{i}']) /(market[f'mua_kl_1{i}'] + market[f'ban_kl_1{i}'])).copy() # get ban_kl and mua_kl new_df[f'total_ban_{i}'] = market[f'ban_kl_1{i}'].copy() new_df[f'total_mua_{i}'] = market[f'mua_kl_1{i}'].copy() # get Gia KL new_df['Gia KL'] = market['Gia KL'].copy() new_df.set_index(market.index, inplace=True) gttb = [col for col in new_df.columns.to_list() if 'gttb' in col] + ['Gia KL'] mua_ban = [col for col in new_df.columns.to_list() if col not in gttb] # Group by minute result = new_df[gttb].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute]).mean() result = pd.concat([result,new_df[mua_ban].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute ]).sum()],axis=1) #Set index in minute index = pd.to_datetime([f\"{d}{h}:{m}:00\" for (d, h, m) in result.index]) result.index = index #handle missing value result = handle_null(result) return result Rồi giờ transform rồi kiểm tra lại số lượng quan sát ở bảng mới thôi # Check the length of new data len(new_market) # --\u003e 5154 Với kết quả mới, chỉ còn lại 5154 quan sát mà thôi, khi rút lại một số lượng quan sát lớn như vậy, ta sẽ phải chấp nhận rủi ro mất đi nhiều thông tin về dữ liệu mà cụ thể là dữ liệu theo giây (cứ 10 giây cập nhật). Nhưng đổi lại, data sẽ cô động hơn và bớt nhiễu vì với sự biến đổi của thị trường trong cả 1 tháng, sự thay đổi của các trường trong mỗi 10 giây là quá nhỏ và không đáng kể. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-transforming"},{"categories":["projects"],"content":"Data ScalingSau khi có bộ dataframe mới tốt hơn và sạch sẽ, bước kế tiếp sẽ là scale lại dữ liệu về một chuẩn để tăng hiệu quả của các thuật toán học máy Có một số phương pháp scale data như: Standardization, Normalization,… Ở project này, OG sẽ dùng phương pháp Normalization để scale data. Phương pháp chuẩn hóa này đưa tỷ lệ dữ liệu từ phạm vi ban đầu về chuẩn phạm vi từ 0 đến 1, giá trị được normalize theo công thức sau: $$ x' = \\frac{x - min}{max - min} $$ Với $x$ là giá trị cần được chuẩn hóa, $max$ và $min$ là lần lượt là giá trị lớn nhất và nhỏ nhất trong tất cả các observations của feature trong tập dữ liệu. Ta sẽ dùng MinMaxScaler của scikit-learn trong tác vụ này. from sklearn.preprocessing import MinMaxScaler # Normalization data using libraries min_max = MinMaxScaler() X = new_market.values X_std = min_max.fit_transform(X) print('Data after scaling: ') X_std # array([[9.10048201e-01, 9.43990665e-01, 9.59215952e-01, ..., # 1.31664615e-02, 1.45711006e-02, 2.90267046e-03], # [9.14492108e-01, 9.61493582e-01, 9.57989455e-01, ..., # 1.42007963e-02, 1.10109072e-04, 3.64335188e-03], # [9.12286536e-01, 9.57992999e-01, 9.56950233e-01, ..., # 3.58702686e-03, 1.43141794e-03, 4.40405173e-04], # ..., # [9.23665190e-01, 9.04317386e-01, 8.96622210e-01, ..., # 1.22024151e-01, 3.04635100e-03, 2.10293470e-02], # [9.24218272e-01, 8.89565349e-01, 8.97159958e-01, ..., # 1.05691866e-02, 1.13779375e-03, 2.88265204e-02], # [9.28532923e-01, 9.04317386e-01, 8.98196897e-01, ..., # 8.84087818e-03, 3.67030241e-04, 7.79383700e-03]] Như vậy là đã chuẩn bị hoàn tất cho bước tiếp theo rồi. Chúng ta sẽ bước vào thuật toán chính đầu tiên trong project này. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-scaling"},{"categories":["projects"],"content":"Principle Component Analysis (PCA)Chúng ta đã đi qua việc tiền xử lý dài ngoằn từ cleaning, transforming đến scaling. Vậy câu hỏi là: dữ liệu đã sẵn sàng để áp dụng cho các mô hình máy học hay chưa ? Câu trả lời cho trường hợp này là: Chưa. Tại sao vậy ? Bởi vì tập dữ liệu của chúng ta có quá nhiều features Feature của tập data là gì ? Dành cho bạn chưa biết, feature của tập data còn được gọi là các trường (hay field) của tập data đó. Đó là các cột, mỗi cột là một “tính chất” khác nhau của đối tượng aka quan sát (observation) thường là các hàng. Hiện tại có thể thấy cleaning data của chúng ta có 91 features: gttb_(cổ phiếu): 30 cột giá trị trung bình giao dịch của 30 cổ phiếu trong 1 phút total_ban_(cổ phiếu): 30 cột tổng khối lượng bán của 30 cổ phiếu trong 1 phút total_mua_(cổ phiếu): 30 cột tổng khối lượng mua của 30 cổ phiếu trong 1 phút Gia_KL: 1 cột giá phái sinh VN30 Index (label) Với số lượng feature lớn như vậy, sẽ vô cùng kém hiệu quả nếu ngay lập tức sử dụng train cho các mô hình machine learning. Giải pháp ở đây chính là ta sẽ giảm chiều dữ liệu xuống mức vừa đạt hiệu năng tốt khi training mà cũng không làm mất quá nhiều thông tin của dữ liệu. Vâng đúng vậy, phương pháp OG muốn giới thiệu chính là PCA hay còn được biết với tên việt hóa là Phân tích thành phần chính. Mục tiêu của phương pháp này là đưa bộ dữ liệu ban đầu sang hệ tọa độ mới dựa trên các thành phần chính. Dữ liệu ở hệ tọa độ mới có ít chiều hơn nhưng vẫn giữ được nhiều nhất thông tin có thể, từ đó giúp tăng tốc độ tính toán và giảm độ phức tạp mô hình hơn rất nhiều. Nói tóm tắt cho dễ hiểu Cơ bản là phương pháp này đưa bộ data của ta vào một “thế giới song song” có số chiều mới ít hơn (chiều aka features). Bạn có thể hiểu như là nhìn dữ liệu của mình ở một góc khác vậy. Ở phần này chúng ta sẽ sử dụng phương pháp này thông qua sự phân rã của ma trận hiệp phương sai (Eigen decomposition of covariance matrix) ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#principle-component-analysis-pca"},{"categories":["projects"],"content":"EigenVector và EigenValueMa trận hiệp phương sai được định nghĩa là: $$ S = \\frac{1}{N}\\hat{X}^T\\hat{X} $$ Với $\\hat{X} = X - \\hat{x}1^T$ là zero-corrected data hay dữ liệu đã được chuẩn hoá. Ta sẽ viết hàm get_eigenpairs() để tìm các vector riêng và giá trị riêng của ma trận hiệp phương sai: $$ Su_i = \\lambda_iu_i $$ Trong đó: các $(\\lambda_i,u_i)$ là các cặp trị riêng (không âm) và vector riêng của ma trận hiệp phương sai $S$ Tại sao lại cần tìm các vector riêng và giá trị riêng của ma trận hiệp phương sai ? Việc sử dụng các giá trị riêng để đánh giá sự quan trọng của mỗi thành phần chính được tạo ra từ việc giảm chiều dữ liệu. Các giá trị riêng càng lớn thì thành phần chính tương ứng càng quan trọng. Các vector riêng tương ứng với các giá trị riêng này được sử dụng để xác định hướng của các thành phần chính. Giá trị riêng (Eigenvalues $\\lambda_i$): Các hệ số được gắn với các vector riêng, cung cấp cho độ lớn của trục. Trong trường hợp này, chúng là thước đo hiệp phương sai của dữ liệu. Vector riêng (EigenVector $u_i$):Các vector (khác 0) không thay đổi hướng khi áp dụng bất kỳ phép biến đổi tuyến tính (linear transformation) nào, nó chỉ thay đổi theo hệ số vô hướng. Hàm sắp xếp các vector riêng (Sort eigenvalues): Bằng cách sắp xếp các vector riêng theo thứ tự của giá trị riêng, ta có thể chọn ra các vector riêng có giá trị riêng lớn nhất để xây dựng các thành phần chính của dữ liệu (đóng góp nhiều nhất vào việc giải thích sự biến thiên của dữ liệu). Các thành phần chính này có thể được sử dụng để tái cấu trúc dữ liệu ban đầu mà vẫn giữ được độ giống nhau của các điểm dữ liệu ban đầu. def get_eigenpairs(X: np.array) -\u003e list: ''' Input: X: np.array (init matrix) return eigenpairs containing eigenvalues and eigenvectors of covariance matrix ''' # Covariance matrix cov_mat = np.cov(X.T) # Eigenvalues and Eigenvectors evals, evecs = np.linalg.eigh(cov_mat) # Sort eigenvalues epairs = [(abs(eval), evec) for (eval, evec) in zip(evals, evecs.T)] epairs = sorted(epairs, key = lambda pair: pair[0], reverse = True) return epairs ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#eigenvector-và-eigenvalue"},{"categories":["projects"],"content":"Cumulative Sum of ComponentsTính tổng tích lũy của các thành phần trong PCA (Cumulative Sum of Explained Variance) để xác định tổng phần trăm phương sai được giải thích bởi các thành phần được giữ lại trong mô hình PCA. $$ r_K = \\frac{\\sum^K_{i=1}\\lambda_i}{\\sum^D_{j=1}\\lambda_j} $$ là lượng thông tin được giữ lại khi số chiều dữ liệu mới sau PCA là K. Hàm findNumVec() thực hiện việc lấy các giá trị riêng từ danh sách các eigenpairs và chuyển đổi chúng thành một mảng numpy. Sau đó, nó tính tổng tích lũy của các giá trị riêng, sử dụng hàm np.cumsum () chuẩn hóa tổng của chúng =\u003e cho ra một danh sách các giá trị (trong khoảng từ 0 đến 1) đại diện cho tỷ lệ phần trăm phương sai được giải thích bởi mỗi thành phần chính. Sau đó, hàm lặp qua danh sách tổng tích lũy và tìm chỉ mục của giá trị đầu tiên lớn hơn hoặc bằng tỷ lệ phần trăm phương sai mong muốn được giải thích. Chỉ số này đại diện cho số lượng thành phần chính cần thiết để giải thích tỷ lệ phần trăm phương sai đó, vì vậy hàm trả về giá trị này cộng với 1 (vì lập chỉ mục Python bắt đầu từ 0). def findNumVec(eigenpairs: list, percent = 0.9): ''' Find number of principal components (eigenvectors) -\u003e return the number of principal components when total accumulate \u003e= percent ''' # Get eigenvalues eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) # Cumulative sum and calculate percent cumsum = np.cumsum(eigenvals) cumsum /= cumsum[-1] # Find number of principal components that accumulate \u003e= percent for i, val in enumerate(cumsum): if val \u003e= percent: return i + 1 Ta sẽ thử tìm xem số thành phần chính cần để giữ được 80% dữ liệu: print(findNumVec(epairs, 0.8)) # --\u003e 28 ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#cumulative-sum-of-components"},{"categories":["projects"],"content":"Scree ChartTa sẽ vẽ một biểu đồ thể hiện quan hệ của số lượng thành phần chính và phần trăm phương sai giải thích tích lũy def screeplot(eigenpairs): ''' Scree plot ''' fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True) eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) cumsum = np.cumsum(eigenvals) # extracts the eigenvalues from the eigenpairs and calculates their cumulative sum cumsum /= cumsum[-1] name = [f'PCA {i}' for i in range(len(cumsum))] # line plot # the eigenvalues are plotted against the number of principal components axes[0].plot(range(len(eigenvals)), eigenvals, marker = '.', color = 'b', label = 'Eigenvalue') # the cumulative proportion of the variance explained by each component is plotted against the number of principal components axes[1].plot(range(len(cumsum)), cumsum, marker = '.', color = 'green', label = 'Cumulative propotion') # y axis label axes[0].set_ylabel('Eigen values') axes[1].set_ylabel('Cumulative explained variance') # item legend axes[0].legend() axes[1].legend() # grid axes[0].grid() axes[1].grid() # title fig.supxlabel('Number of components') plt.tight_layout() plt.show() #print the cumsum of eigenvalues print(pd.DataFrame(cumsum, columns = ['Cumulative total'], index = name)) result = { str(i): f\"PC {i+1}({var:.1f}%)\" for i, var in enumerate(cumsum*100) } return result pca_scree = screeplot(epairs) Scree plotScree plot \" Scree plot Giải thích Đường của giá trị riêng màu xanh nước biển trên biểu đồ cho ta biết độ lớn của mỗi thành phần chính và tầm quan trọng của chúng trong giải thích sự biến thiên của dữ liệu. Nếu giá trị riêng của một thành phần chính là lớn, thì thành phần đó có tầm quan trọng cao trong việc giải thích sự biến thiên của dữ liệu. Đường màu xanh lá thể hiện tổng tích lũy cho ta biết tổng phần trăm độ lớn của sự biến thiên của dữ liệu mà các thành phần chính có thể giải thích. Dựa vào biểu đồ trên có thể nhận thấy nếu chỉ có 2 chiều, ta chỉ giữ được khoảng 37% dữ liệu ban đầu. ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#scree-chart"},{"categories":["projects"],"content":"Visualize PCABây giờ, ta sẽ thực hiện chiếu dữ liệu ban đầu đã chuẩn hóa $\\hat{X}$ xuống không gian con tìm được và lấy ra ma trận các thành phần chính để tiếp tục công việc phân tích và xây dựng mô hình. Hàm getPC() trả về một ma trận các thành phần chính từ ma trận ban đầu, dựa trên số lượng thành phần đã cho hoặc số lượng thành phần giữ được 80% dữ liệu (nếu num_components không được đưa ra). Ma trận trọng số $W$ là ma trận chuyển đổi tuyến tính được sử dụng để chuyển đổi dữ liệu gốc vào không gian mới, trong đó mỗi thành phần chính được sắp xếp theo độ quan trọng giảm dần. Cụ thể, mỗi cột của ma trận $W$ là một vector riêng chuẩn hóa tương ứng với các giá trị riêng của ma trận hiệp phương sai $s$. Thực hiện việc nhân ma trận $W$ với hoán vị của ma trận đã chuẩn hóa $\\hat{X}$ (init_matrix). Ma trận kết quả sau đó tiếp tục được hoán vị để phù hợp với hình dạng ban đầu của init_matrix và trả về kết quả. def getPC(eigenpairs, init_matrix, num_components = None): ''' Return matrix of principal components from init_matrix ''' # default num_components = number which to keep 80% data if num_components is None: num_components = findNumVec(eigenpairs, 0.8) # extracts the eigen vectors corresponding to the top num_components eigenvalues from the eigenpairs list eigenvecs = [eigenvec for (_, eigenvec) in eigenpairs[:num_components]] W = np.array([e.T for e in eigenvecs]) # stacks the eigen vectors into a weight matrix W return (W @ init_matrix.T).T X_pca = getPC(epairs, X_std) Vậy là ta đã giảm được độ phức tạp cho bộ dữ liệu khá “nhọc nhằn” này. Hãy trực quan hóa lên biểu đồ để có một góc nhìn cụ thể và rõ ràng hơn. Biểu đồ scatter plot sau khi PCA có thể giúp cho chúng ta nhìn thấy cách dữ liệu được phân bố trên các thành phần chính (principal components) và kiểm tra xem liệu chúng ta có thể tìm thấy các cluster hoặc pattern nào trong dữ liệu. plt.scatter(X_pca[:,0], X_pca[:,1]) plt.xlabel('PC1') plt.ylabel('PC2') plt.title('Visualizing data through PCA', fontsize=18) plt.gca().set_aspect('equal', 'datalim') plt.grid() plt.show() Visualizing data via PCAVisualizing data via PCA \" Visualizing data via PCA Okayy dựa vào biểu đồ trên, cũng có thể thấy là dữ liệu ở không gian mới đã phân tách khá rõ ràng rồi. Điều này nghĩa là phương pháp PCA đã giảm số chiều của dữ liệu một cách hiệu quả. Bước tiếp theo chính là áp vào mô hình K-Means để phân cụm và tìm pattern. Chúng ta sẽ cùng chiến tiếp ở phần 2 nhé ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:4","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#visualize-pca"},{"categories":["projects"],"content":"To be ContinueChúng ta đã thực hiện các bước tiền xử lý dữ liệu và sau đó là thực hiện PCA để giảm chiều dữ liệu một cách hiệu quả. Bài sau phần 2, OG sẽ thực hiện training mô hình K-means clustering và cuối cùng là phân tích dữ liệu chứng khoáng. Đây là kiến thức tích góp từ nhiều nguồn và nghiên cứu của nhóm OG, tất nhiên không thể tránh khỏi sai sót. Hy vọng bài viết lần này thú vị và giúp bạn đọc thư giãn, tham khảo. -Mew- ","date":"31 Aug 2023","objectID":"/stock_analysis/:5:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#to-be-continue"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Spotify Analysis Analyze data from Spotify platform utilizing the Spotify API and MongoDB, Apache Hadoop, Pyspark, Dremio and Power BI Read more... ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#related"},{"categories":[],"content":"Ngành Data có gì hot mà mình lại dính","date":"30 Aug 2023","objectID":"/start_journey/","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/"},{"categories":[],"content":"Quảng Cáo Chào mừng đến với “Data lú” Giới thiệu với mọi người đây là series đầu tiên của kênh này kể mấy câu chuyện kì thú ảo ma canada của OG trong thế giới data rộng lớn 😂 Đùa chút thôi, đây sẽ là series vui vẻ về câu chuyện Data mà OG trải nghiệm, góp nhặt được. Hy vọng bạn sẽ thích nó hihi 😍 Gòi dzo Hellooo OG đâyy ! Chào mừng bạn đến với số đầu tiên, lần đầu còn bỡ ngỡ, nên mình sẽ kể cơ duyên đưa mình đến với ngành Data và quyết định dấn thân vào con đường trở thành một Data Engineer 😗 Gét Goo! ","date":"30 Aug 2023","objectID":"/start_journey/:0:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#"},{"categories":[],"content":"Ủa ngành Data Science ?Khoan Khoan … Bên trên là Engineer, qua đây là Science là sao OG ? Từ từ nào 😄 Mọi chuyện bắt đầu khi mình đậu vào một ngành được ca ngợi là ngành “quyến rũ” nhất thế kỷ 21 theo Harvard Business Review , đó là Data Science. Khúc này mình nghe cũng oách oách, nhưng chính xác Data Science là gì ? Và các nhà khoa học dữ liệu (data scientist) làm gì ? ","date":"30 Aug 2023","objectID":"/start_journey/:1:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#ủa-ngành-data-science-"},{"categories":[],"content":"Data Science là gì nhỉ?Ngành Khoa học dữ liệu hay Data Science là một lĩnh vực liên ngành ứng dụng các phương pháp khoa học, thuật toán và các phân tích thống kê để tìm kiếm ý nghĩa từ dữ liệu. Hay nói bằng cách dễ hiểu, Data Science là ngành tìm kiếm, phân tích dữ liệu để khai thác tất cả những giá trị mà dữ liệu mang lại để phục vụ nhiều mục đích khác nhau. Data Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán tương lai Một nhà khoa học dữ liệu (Data Scientist) là người chịu trách nhiệm đưa ra các dẫn chứng từ dữ liệu, để từ đó đề xuất các giải pháp, kế hoạch hay định hướng từ ý nghĩa tìm được từ dữ liệu để giải quyết các bài toán kinh doanh khác nhau. Một data scientist cần phải biết kỹ năng gì? Lập trình: Python và R là 2 ngôn ngữ chính được sử dụng đối với ngành này. Python là một ngôn ngữ lập trình linh hoạt phổ biến với rất nhiều thư viện để xử lý dữ liệu như numpy, pandas, matplotlib,… Trong khi đó R tỏ là là một ngôn ngữ mạnh mẽ về phân tích và thống kê, ngoài ra R cũng thường được dùng trong nghiên cứu và học thuật. Thống kê và ứng dụng toán học: Nếu bạn không yêu thích toán học, chắc hẳn bạn cũng sẽ không thể làm điều đó với data science. Hẳn vậy, là một nhà khoa học dữ liệu, bạn cần có một nền tảng kiến thức toán học vững, đặc biệt là về xác suất thống kê và đại số tuyến tính,… SQL và DBMS: Ta phải tiếp xúc rất nhiều với hệ quản trị cơ sở dữ liệu (Database Management System hay DBMS), đó có thể là hệ quản trị cơ sở dữ liệu Quan Hệ (Relational Database Management System) như MySQL, Postgres, SQL server… hay NoSQL database như MongoDB, Cassandra,… Và để tương tác với database (RDBMS), điều không thể thiếu chính là SQL (Structured query language aka si cồ hay ét qui eo 😂 ). Cơ bản thì đây là ngôn ngữ dùng để truy suất dữ liệu, giao tiếp với database, đặc biệt là các RDBMS. AI, Machine learning: Khi có một lượng dữ liệu khổng lồ, một data scientist có thể sẽ dùng chúng để huấn luyện mô hình học máy hoặc mạng để giải các bài toán hồi quy và đưa ra được các dự đoán về xu hướng data hay giải quyết các bài toán phân loại. Có hiểu biết về các thuật toán máy học và kiến trúc mạng noron cũng là một điều cần có ở nhà khoa học dữ liệu. Đọc đến đây, có thể bạn sẽ có cảm giác “Dèjà vu” nhẹ … Sao nhiều chổ giống Data Analyst thế nhỉ ? Mà thiệt ra là không giống đâu nhé, hai ngành này chỉ là anh em xã hội với nhau mà thôi 😂 ","date":"30 Aug 2023","objectID":"/start_journey/:1:1","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-science-là-gì-nhỉ"},{"categories":[],"content":"Data Scientist vs Data AnalystSẵn tiện kể một chút về vai trò của một người Data Analyst. Về cơ bản, vai trò của họ cũng giống với các data scientist, họ cũng phân tích dữ liệu, cố gắng tìm kiếm và rút ra giá trị từ chúng. Nhưng sẽ có một số điểm khác biệt: Data Analyst Data Science Chuyên viên phân tích dữ liệu Nhà khoa học dữ liệu Vẫn làm công việc của DS nhưng với quy mô nhỏ Tỏa sáng với lượng data khổng lồ (BigData) Không cần nhiều kiến thức lập trình Cần kiến thức lập trình Cần có kiến thức về hoạt động kinh doanh nhiều hơn và vững về kiến thức thống kê Cần có kiến thức không chỉ toán thống kê, ứng dụng mà còn phải có kiến thức về computer science, AI/ML,… Dựa vào dữ liệu đưa ra các giá trị có ích và cái nhìn trực quan về dữ liệu Được yêu cầu phát triển “data product” để đưa ra quyết định có ích từ tập dữ liệu lớn Data Science and Data Analytic Rồi okay nãy giờ là cả data science (DS) và data analytic (DA) rồi. Giờ là mới đến data engineer của tui nè hihi 😄 ","date":"30 Aug 2023","objectID":"/start_journey/:1:2","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-scientist-vs-data-analyst"},{"categories":[],"content":"Data Engineer là gì ?Tuy học Data Science, nhưng thực ra ngay từ những lúc còn mơn mởn cấp 3, OG đã từng có ước muốn trở thành một lập trình viên một tay cafe một tay chém code bình loạn thiên hạ 😂 Và thế là tìm được một ngành thích hợp được coi là “Software engineer cho data”, ngành này là một trong các ngành có xu hướng phát triển nhanh nhất trong nhóm ngành công nghệ. Vâng đó chính là Data Engineer Đầu tiên, Data Engineer hay DE được gọi là kỹ sư dữ liệu. Đây là vai trò đảm nhiệm việc phân tích nguồn dữ liệu, xây dựng và duy trì hệ thống cơ sở dữ liệu hiệu quả. Ngoài ra cũng là người đảm bảo chất lượng dữ liệu cho các phòng ban khác sử dụng. Cơ bản để là để cho DS và DA làm việc một cách hiệu quả nhất, họ cần có một nguồn data ổn định và sạch sẽ. Và người đảm nhiệm việc luân chuyển data đó tới cho họ chính là Data Engineer. Không chỉ có DS và DA mà data engineer phục vụ cho tất cả các phòng ban khác Data Engineer Nói tóm lại, Data Engineer là người xây dựng các đường ống dữ liệu (data pipeline) để truyền dữ liệu từ nơi này sang nơi khác một cách chất lượng nhất :)) Khái niệm cơ bản là thế thôi, nghe có vẻ đơn giản phải không. Hãy tiếp tục với mục tiếp theo để xem liệu ta cần gì để trở thành data engineer ","date":"30 Aug 2023","objectID":"/start_journey/:2:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-engineer-là-gì-"},{"categories":[],"content":"Data Engineer thì cần biết gì ?Một data Engineer về bản chất là xây dựng các data pipeline để luân chuyển dữ liệu. Để làm tốt việc đó, kỹ sư dữ liệu phải biết: Kỹ năng lập trình: Tất nhiên rồi, bạn là một nhân viên IT thì điều này là phải có. Các ngôn ngữ mà DE thường dùng là SQL, Python và R. Hệ cơ sở dữ liệu quan hệ và phi quan hệ: Dữ liệu có rất nhiều dạng: Structure/Semi/Unstructure data, do đó cũng cần có nhiều loại database quản lý chúng. Và DE làm việc rất nhiều với database. Họ sẽ là người trực tiếp tương tác kể cả với SQL và NoSQL database. ETL/ELT: ETL aka Extract Transform Load hay ELT aka Extract Load Transform là quy trình xử lý và luân chuyển dữ liệu từ nguồn đến đích. Một DE phải nắm được để thiết kế data pipeline một cách hiệu quả nhất Data Warehouse: hay được biết đến là kho chứa dữ liệu. Bạn có thể sẽ phải xây dựng, thiết kế cấu trúc data warehouse trên cloud platform và xây dựng các kết nối dữ liệu để tối ưu hóa tốc độ truy xuất và đảm bảo việc phân tích dữ liệu. Big Data: Bạn cũng cần phải biết các kiến trúc lưu trữ và xử lý tập dữ liệu lớn như Hadoop, Spark,… Cloud: Tất nhiên là phải có rồi, các cloud platform như Google Cloud Platform, AWS, Azure,… đã rất nổi tiếng trong việc hỗ trợ xây dựng và thiết kế hệ thống pipeline cũng như hỗ trợ tối đa việc xử lý bigdata cũng như deploy hệ thống hạ tầng một cách nhanh chóng. Bạn có thể sẽ phải làm việc với lượng dữ liệu khổng lồ và tập dữ liệu lớn. Và để xây dựng hệ thống xử lý được lượng dữ liệu đó, chắc chắn phải có sự góp mặt của các nền tảng đám mây. Well… Nhìn chung cũng nhiều thứ cần phải biết đấy nhỉ, tất nhiên đó chỉ là một số điều quan trọng nhất. Ngoài ra bạn cũng cần phải biết một số kiến thức khác về Unix và Linux, Docker, Git, Batch/Stream Processing,… Và còn ti tỉ thứ khác mà OG có kể đến răng long đầu bạc có lẽ cũng chưa hết 😂 ","date":"30 Aug 2023","objectID":"/start_journey/:2:1","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-engineer-thì-cần-biết-gì-"},{"categories":[],"content":"Tạm kếtHành trình nào khi bắt đầu cũng gian nan, cả bản thân OG khi bắt đầu cũng không biết gì cả. Nhưng khi nhấc ngón chân lên và đi thì mới cảm nhận được thế giới chứ 😄 Hy vọng bài viết này giúp bạn thư giãn và có một cái nhìn chung về ngành data nhé. Hẹn gặp lại trong bài tiếp theo hehe -Meww- ","date":"30 Aug 2023","objectID":"/start_journey/:3:0","series":["Data lú"],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#tạm-kết"},{"categories":null,"content":"Continuous of Football ETL series","date":"01 Aug 2023","objectID":"/football_etl_2/","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/"},{"categories":null,"content":"Source PhongHuynh0394 Football_ETL_Analysis Hello! Hello! OG đây, sau phần 1 chúng ta đã setup các kiểu và đảm bảo mọi thứ trơn tru rồi, ở phần này chúng ta sẽ chuẩn bị Data Source, và khởi chạy pipeline ở Implement sau đó sẽ Visualize cleaned data có được từ data warehouse thành Dashboard. Bắt đầu thôi nào ! ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#"},{"categories":null,"content":"Data Source","date":"01 Aug 2023","objectID":"/football_etl_2/:1:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#data-source"},{"categories":null,"content":"Chuẩn bị file làm raw dataCác file csv sử dụng làm dữ liệu được tải từ Football Database - Kaggle. Đây là dữ liệu thống kê của cầu thủ, đội bóng đến từ 5 giải bóng hàng đầu Châu Âu (Premier League, Laliga, Seria A, Budesliga, League 1) Ta sẽ có schema như sau: Schema trong đó: games: bảng chứa thông tin thống kê của từng trận đấu (gameID) teams: Bảng chứa tên các đội bóng (teamID) players: Bảng chứa tên các cầu thủ (playerID) leagues: Bảng chưa tên các giải đấu (leagueID) appearances: Bảng thống kê của cầu thủ ở các game mà họ tham gia (gameID, playerID) teamstats: Bảng thống kê của đội bóng ở từng game (gameID, teamID) ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#chuẩn-bị-file-làm-raw-data"},{"categories":null,"content":"Load data vào MySQLCó nhiều cách để load data vào MySQL, ở đây mình sẽ sử dụng cách LOAD LOCAL_INFILE của MySQL luôn. Tip Hãy đảm bảo bạn đã make up lần đầu rồi nhé ! Hãy copy folder chứa các file csv vào de_mysql container: docker cp /football de_mysql:/tmp/dataset/ docker cp /load_data de_mysql:/tmp/dataset/ Sau đó tạo bảng trống sẵn trong MySQL: make mysql_create #Create table in mysql Tiếp tục với lệnh: make to_mysql_root # ----- You will access to MySQL container SET GLOBAL LOCAL_INFILE=TRUE; #Set local_infile variable to load data from local exit; # ----- Exit container make mysql_load #load data make mysql_create_relation #create table relation Thế là đã chuẩn bị xong dữ liệu cho MySQL. ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load-data-vào-mysql"},{"categories":null,"content":"Init PostgreSQL SchemaTa cũng cần phải tạo sẵn schema sẵn trong Posgres như sau: make to_psql CREATE SCHEMA IF NOT EXISTS analysis; exit; Thế là đã hoàn tất việc chuẩn bị data, giờ thì ta bắt đầu vào phần việc chính thôi ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#init-postgresql-schema"},{"categories":null,"content":"ImplementCông việc chính trong phần này là xây dựng các data pipeline bằng dagster. Cơ bản có thể hiểu là ta tạo các Asset và chuyển chúng từ database này sang database khác. ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#implement"},{"categories":null,"content":"ExtractionĐể có thể quản lý việc truy xuất dữ liệu từ MySQL và load vào MinIO để lưu tạm, ta sẽ xây dựng một I/O Manager phục vụ việc đó. Đầu tiên, hãy vào đường dẫn: ./etl_pipeline/etl_pipeline/resources/ Ta sẽ xây dựng MySQL io manager bằng cách tạo file mysql_io_manager.py với nội dung sau: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_mysql(config): conn_info = ( f\"mysql+pymysql://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class MySQLIOManager(IOManager): def __init__(self, config): self.config = config def handle_output(self, context: OutputContext, obj: pd.DataFrame): pass def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def extract_data(self, sql: str) -\u003e pd.DataFrame: with connect_mysql(self.config) as db_conn: pd_data = pd.read_sql_query(sql, db_conn) return pd_data Sau đó, tiếp tục đối với minio_io_manager.py: import os from contextlib import contextmanager from datetime import datetime from typing import Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from dagster import IOManager, InputContext, OutputContext from minio import Minio @contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\"endpoint_url\"), access_key=config.get(\"aws_access_key_id\"), secret_key=config.get(\"aws_secret_access_key\"), secure=False ) try: yield client except Exception: raise class MinIOIOManager(IOManager): def __init__(self, config): self._config= config def _get_path(self, context: Union[InputContext, OutputContext]): layer, schema, table = context.asset_key.path key = \"/\".join([layer, schema, table.replace(f\"{layer}_\", \"\")]) tmp_file_path = \"/tmp/file-{}-{}.parquet\".format( datetime.today().strftime(\"%Y%m%d%H%M%S\"), \"-\".join(context.asset_key.path) ) if context.has_asset_partitions: start, end = context.asset_partitions_time_window dt_format = \"%Y%m%d%H%M%S\" partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format) return os.path.join(key, f\"{partition_str}.pq\"), tmp_file_path else: return f\"{key}.pq\", tmp_file_path def handle_output(self, context: OutputContext, obj: pd.DataFrame): # convert to parquet format key_name, tmp_file_path = self._get_path(context) table = pa.Table.from_pandas(obj) pq.write_table(table, tmp_file_path) # upload to MinIO try: bucket_name = self._config.get(\"bucket\") with connect_minio(self._config) as client: # Make bucket if not exist. found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exists\") client.fput_object(bucket_name, key_name, tmp_file_path) row_count = len(obj) context.add_output_metadata({\"path\": key_name, \"tmp\": tmp_file_path}) # clean up tmp file os.remove(tmp_file_path) except Exception: raise def load_input(self, context: InputContext) -\u003e pd.DataFrame: bucket_name = self._config.get(\"bucket\") key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: #Make bucket if not exist found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exist\") client.fget_object(bucket_name, key_name, tmp_file_path) pd_data = pd.read_parquet(tmp_file_path) return pd_data except Exception: raise Sau khi đã tạo thành công các io manager cho mysql và minio, ta sẽ bắt đầu xây dựng bronze layer Note nho nhỏ Trong project này mình chia các giai đoạn transformation thành các layer: bronze layer: Giai đoạn chỏ mới load raw data, có thể hiểu đây là data chưa transform gì cả siler layer: Transform một phần từ bronze layer, ở đoạn này data đã được cleaning sơ gold layer: Sau khi transform một lần nữa từ silver layer, giai đoạn này sẽ truy vấn ra các thôn","date":"01 Aug 2023","objectID":"/football_etl_2/:2:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#extraction"},{"categories":null,"content":"TransformationTiếp tục tạo file silver_layer.py cùng folder với bronze layer: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teamstats\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"leagues\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], description='Statistic of teams in games', group_name=\"Silver_layer\", compute_kind=\"Pandas\" ) def silver_statsTeamOnGames(teamstats: pd.DataFrame, games: pd.DataFrame, leagues: pd.DataFrame) -\u003e Output[pd.DataFrame]: ts = teamstats.copy() gs = games.copy() lgs = leagues.copy() #Drop unsusable columns in games gs.drop(columns=gs.columns.to_list()[13:], inplace=True) #create StatperLeagueSeason result = pd.merge(ts, gs, on=\"gameID\") result = result.merge(lgs, on=\"leagueID\", how=\"left\") result.drop(columns=['season_y', 'date_y'],inplace=True) result = result.rename(columns={'season_x': 'season', 'date_x': 'date'}) return Output( result, metadata={ \"table\": \"statsTeamOnGames\", \"records\": len(result) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"appearances\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"players\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='statistic of players in games', compute_kind=\"Pandas\" ) def silver_playerAppearances(appearances: pd.DataFrame, games: pd.DataFrame, players: pd.DataFrame) -\u003e Output[pd.DataFrame]: app = appearances.copy() ga = games.copy() pla = players.copy() #Drop unusable column ga.drop(columns=ga.columns.to_list()[13:], inplace=True) #Merge player_appearances = pd.merge(app, pla, on=\"playerID\", how=\"left\") player_appearances = pd.merge(player_appearances, ga, on=\"gameID\", how=\"left\") #drop unecessary columns and rename player_appearances.drop(columns=['leagueID_y'],inplace=True) player_appearances.rename(columns={'leagueID_x': 'leagueID'}, inplace=True) return Output( player_appearances, metadata={ \"table\": \"playerAppearances\", \"records\": len(player_appearances) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teams\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='Teams', compute_kind=\"Pandas\" ) def silver_teams(teams: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( teams, metadata={ \"table\": 'teams', 'records': len(teams) } ) Lúc này mình có 3 silver assets, được join từ các bảng ở bronze Tiếp đến là gold_layer, lúc này ta sẽ tính các thông số thống kê của từng giải đâu từng mùa, các thống kê của cầu thủ trong 90 phút thi đấu, và cả thống kê của từng cầu thủ trong từng mùa giải gold_layer.py sẽ có nội dung: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_statsTeamOnGames\": AssetIn( key_prefix=[\"football\", \"silver\"] ) }, group_name=\"Gold_layer\", key_prefix=[\"football\", \"gold\"], description='Statistic of all league in each season', compute_kind=\"Pandas\" ) def gold_statsPerLeagueSeason(silver_statsTeamOnGames: pd.DataFrame) -\u003e Output[pd.DataFrame]: st = silver_statsTeamOnGames.copy() result = ( st.groupby(['name', 'season']) .agg({\"goals\": \"sum\", \"xGoals\": \"sum\", \"shots\": \"sum\", \"shotsOnTarget\": \"sum\", \"fouls\": \"sum\", \"yellowCards\": \"sum\", \"redCards\": \"sum\",'corners': 'sum', \"gameID\": 'count'}) .reset_index() ) result = result.rename(columns={'gameID':\"games\"}) result['goalPerGame']= result.goals/result.games result['season'] = result['season'].astype('string') return Output( result, metadata={ 'table': 'statPerLeagueSeason', 'records': len(result) } ) @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_playerAppearances\": AssetIn( key_prefix=[\"football\", \"silver\"] ) },","date":"01 Aug 2023","objectID":"/football_etl_2/:2:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#transformation"},{"categories":null,"content":"LoadTrước hết hãy tạo IO Manager cho Postgres để quản lý việc load cleaned data. Ta tạo file psql_io_manager.py ở vị trí mà ta đã tạo 2 io manager trước với nội dung: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_psql(config): conn_info = ( f\"postgresql+psycopg2://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class PostgreSQLIOManager(IOManager): def __init__(self, config): self._config = config def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def handle_output(self, context: OutputContext, obj: pd.DataFrame): schema, table = context.asset_key.path[-2], context.asset_key.path[-1] with connect_psql(self._config) as conn: # insert new data ls_columns = (context.metadata or {}).get(\"columns\", []) obj[ls_columns].to_sql( name=f\"{table}\", con=conn, schema=schema, if_exists=\"replace\", index=False, chunksize=10000, method=\"multi\" ) Sau đó, tạo một asset warehouse_layer.py: from dagster import multi_asset, Output, AssetIn, AssetOut, asset import pandas as pd @multi_asset( ins={ \"gold_statsPerLeagueSeason\": AssetIn( key_prefix=[\"football\", \"gold\"] ) }, outs={ \"statsperleagueseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerLeagueSeason\", 'analysis'], metadata={ \"columns\": [ \"name\", \"season\", \"goals\", \"xGoals\", \"shots\", \"shotsOnTarget\", \"fouls\", \"yellowCards\", \"redCards\", \"corners\", \"games\", \"goalPerGame\" ] } ), }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerLeagueSeason(gold_statsPerLeagueSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerLeagueSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerLeagueSeason\", \"records\": len(gold_statsPerLeagueSeason) } ) @multi_asset( ins={ \"gold_statsPerPlayerSeason\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsperplayerseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerPlayerSeason\", 'analysis'], metadata={ \"columns\": [ \"playerID\", \"name\", \"season\", \"goals\", \"shots\", \"xGoals\", \"xGoalsChain\", \"xGoalsBuildup\", \"assists\", \"keyPasses\", \"xAssists\", \"gDiff\", \"gDiffRatio\" ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerPlayerSeason(gold_statsPerPlayerSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerPlayerSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerPlayerSeason\", \"records\": len(gold_statsPerPlayerSeason) } ) @multi_asset( ins={ \"gold_statsPlayerPer90\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsplayerper90\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPlayerPer90\", 'analysis'], metadata={ \"columns\": [ 'playerID', 'name', 'total_goals', 'total_assists', 'total_time', 'goalsPer90', 'assistsPer90', 'scorers' ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPlayerPer90(gold_statsPlayerPer90: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPlayerPer90, metadata={ \"schema\": \"analysis\", \"table\": \"statsPlayerPer90\", \"records\": len(gold_statsPlayerPer90) } ) ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load"},{"categories":null,"content":"Run systemCuối cùng, ta sẽ kết hợp tất cả các asset lại giúp dagster nhận diện và quản lý với file __init__.py ở etl_pipeline/etl_pipeline/__init__.py import os from dagster import Definitions from .assets.silver_layer import * from .assets.gold_layer import * from .assets.bronze_layer import * from .assets.warehouse_layer import * from .resources.minio_io_manager import MinIOIOManager from .resources.mysql_io_manager import MySQLIOManager from .resources.psql_io_manager import PostgreSQLIOManager MYSQL_CONFIG = { \"host\": os.getenv(\"MYSQL_HOST\"), \"port\": os.getenv(\"MYSQL_PORT\"), \"database\": os.getenv(\"MYSQL_DATABASE\"), \"user\": os.getenv(\"MYSQL_USER\"), \"password\": os.getenv(\"MYSQL_PASSWORD\") } MINIO_CONFIG = { \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\"), \"bucket\": os.getenv(\"DATALAKE_BUCKET\"), \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"), \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\") } PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } ls_asset=[asset_factory(table) for table in tables] + [silver_statsTeamOnGames, silver_teams , silver_playerAppearances, gold_statsPerLeagueSeason, gold_statsPerPlayerSeason, gold_statsPlayerPer90, statsPerLeagueSeason, statsPerPlayerSeason, statsPlayerPer90] defs = Definitions( assets=ls_asset, resources={ \"mysql_io_manager\": MySQLIOManager(MYSQL_CONFIG), \"minio_io_manager\": MinIOIOManager(MINIO_CONFIG), \"psql_io_manager\": PostgreSQLIOManager(PSQL_CONFIG), } ) sau đó hãy dùng lệnh sau để cập nhật các assets docker restart etl_pipeline ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:4","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#run-system"},{"categories":null,"content":"Check UIHãy kiểm tra Dagit UI ở localhost:3001 để chắc chắn rằng mọi thứ vẫn ổn Ngoài ra cũng có thể check MinIO: localhost:9000 ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:5","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#check-ui"},{"categories":null,"content":"VisualizationCuối cùng là vẽ dashboard, đầu tiên ta cần phải lấy được data từ psql, hãy vào tạo file: ./streamlit/src/psql_connect.py: import os import psycopg2 from dotenv import load_dotenv import pandas as pd #load environment load_dotenv() #list table in database table = ['statsperleagueseason','statsperplayerseason', 'statsplayerper90'] PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } #create connection def init_connection(config): return psycopg2.connect( database=config['database'], user=config['user'], password=config['password'], host=config['host'], port=config['port'] ) def extract_data(): conn = init_connection(PSQL_CONFIG) return [pd.read_sql(f'SELECT * FROM analysis.{tab}', conn) for tab in table] Cuối cùng là tạo main.py ngay trong thư mục scr: import streamlit as st import pandas as pd import plotly.express as px import plotly.graph_objects as go from psql_connect import extract_data import numpy as np # #extract data from PostgreSQL ls_df = extract_data() l_season = ls_df[0] p_season = ls_df[1] p_match = ls_df[2] st.set_page_config(page_title = 'Dashboard Football', layout='wide', page_icon='chart_with_upwards_trend') #Overview def overview(table: pd.DataFrame, detail: str): if (st.checkbox('Do you want to see Data ?')): table col1, col2 = st.columns(2) co_df = table.columns.to_list() with col1: st.bar_chart(table.describe()) if (st.checkbox('Do you want to see describe each column ?')): for col in co_df: if table[col].dtypes not in ['int64', 'float64']: continue st.bar_chart(table[col].describe()) with col2: st.caption(f':red[Columns]: {len(co_df)}') st.caption(f':red[Records]: {len(table)}') st.caption(f':red[Description]: {detail}') st.caption(f':red[Columns name]:{co_df}') #league statistic def statleague(): Cards = l_season[['name','season','yellowCards', 'redCards', 'fouls']] #Card_fouls col1, col2 = st.columns(2) with col1: #Goals per games fig = px.bar(l_season, x=\"name\", y=\"goalPerGame\", color=\"name\", barmode=\"stack\", facet_col=\"season\", labels={\"name\": \"League\", \"goals/games\": \"GPG\"}) fig.update_layout(showlegend=False, title='Goals per Game') st.plotly_chart(fig) #fouls fig = px.line(Cards, x='season', y='fouls', color='name') fig.update_layout(title='Fouls of leagues', xaxis_title='Season', yaxis_title='Fouls', legend_title='League') st.plotly_chart(fig) with col2: #Red card fig = px.line(Cards, x='season', y='redCards', color='name') fig.update_layout(title='Red Cards of leagues', xaxis_title='Season', yaxis_title='RedCards', legend_title='League') st.plotly_chart(fig) #yellow card fig = px.line(Cards, x='season', y='yellowCards', color='name') fig.update_layout(title='Yellow Cards of leagues', xaxis_title='Season', yaxis_title='YellowCards', legend_title='League') st.plotly_chart(fig) #Player statistic def statplayer(): col1, col2 = st.columns(2) with col1: #Best offensive player top_player90= p_match[(p_match['goalsPer90'] \u003e 0.8) | (p_match['assistsPer90'] \u003e 0.4)] fig = px.scatter(p_match[['name','goalsPer90', 'assistsPer90']], x='goalsPer90', y='assistsPer90', hover_name='name') fig.add_trace( go.Scatter(x=top_player90['goalsPer90'], y=top_player90['assistsPer90'], mode='markers+text', marker_size=5, text=top_player90['name'], textposition='bottom center', textfont=dict(size=15)) ) fig.update_layout(title='Best offensive Players (2018-2020)', xaxis_title='Goals Per 90min', yaxis_title='Assists Per 90min') st.plotly_chart(fig) #goals-xgoal fig = px.scatter(p_season, x=\"xGoals\", y=\"goals\", color=(p_season['xGoals'] - p_season['goals'] \u003c 10), color_discrete_sequence=[\"red\", \"green\"], opacity=0.5) fig.update_layout(title=\"Goals (G) and Expected Goals (xG)\", xaxis_title=\"xG\", yaxis_title=\"G\", ) st.plotly_chart(fig) with col2: #Top score player topPlayer = p_season.groupby(['name']).agg({'goals': 'sum'}).sort_values('goals', ascending=False).res","date":"01 Aug 2023","objectID":"/football_etl_2/:3:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#visualization"},{"categories":null,"content":"ConclusionCuối cùng cũng đã xong một project xây dựng data pipeline cơ bản, trong lúc thực hiện chắc chắn sẽ có cả tấn lỗi xảy ra, nhưng OG tin là mọi gian khó đều sẽ vượt quan được, thứ đọng lại chính là kiến thức và kỹ năng của chúng ta. Chúc bạn thành công và đón xem tiếp các dự án tiếp theo của OG nhé ! -Mew- ","date":"01 Aug 2023","objectID":"/football_etl_2/:4:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#conclusion"},{"categories":null,"content":"Related Content Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#related-content"},{"categories":["projects"],"content":"A Data Engineer project building pipeline to analyze football data","date":"31 Jul 2023","objectID":"/football_etl/","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/"},{"categories":["projects"],"content":"Source PhongHuynh0394 Football_ETL_Analysis ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#"},{"categories":["projects"],"content":"IntroduceTrong project này, OG sẽ build end-to-end ETL data pipeline hoàn chỉnh để phân tích football data từ Kaggle với data pipeline như sau: Data Pipeline Các bước cụ thể: Set up: Dùng Docker tạo container và các images cần thiết cho pipeline, trong đó có cả Dagster dùng xây dựng pipeline. Chuẩn bị data source: load các file csv (có được từ Kaggle) vào MySQL nhằm mục đích lưu trữ raw data (mô phỏng source data) Extract: Lấy data từ MySQL và load vào MinIO chuẩn bị cho bước transform Transform: Sử dụng Pandas để truy vấn các file từ MinIO Load: Cleaned và transformed data được load vào warehouse PostgreSQL Visualization: Sử dụng Streamlit để làm Dashboard ","date":"31 Jul 2023","objectID":"/football_etl/:1:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#introduce"},{"categories":["projects"],"content":"Set upBắt đầu với Docker, ta sẽ xây dựng lần lượt từng image bằng cách viết docker-compose.yml Tip nho nhỏ Hãy pull/build lần lượt từng loại framework lần lượt để chắc chắn rằng chúng hoạt động trơn tru nhất trước Hoặc bạn cũng có thể xem luôn phần hoàn chỉnh Hoàn chỉnh set up ","date":"31 Jul 2023","objectID":"/football_etl/:2:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#set-up"},{"categories":["projects"],"content":"MinIOMinIO là một server lưu trữ đối tượng dạng phân tán với hiệu năng cao và cung cấp các api giống với Amazon S3, ta có thể upload, download file,… một cách đơn giản. minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_network Note Về .env file, đây là file chứa thông tin các biến môi trường thiết lập cho từng image, mình sẽ nói sau ","date":"31 Jul 2023","objectID":"/football_etl/:2:1","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#minio"},{"categories":["projects"],"content":"MySQLMySQL là một trong số các phần mềm RDBMS (Relational DataBase Management Systems) phổ biến nhất, ta sẽ sử dụng database này để lưu raw data mô phỏng cho source data cần ingest de_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:2","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#mysql"},{"categories":["projects"],"content":"PostgeSQLPostgreSQL là một hệ thống quản trị cơ sở dữ liệu quan hệ-đối tượng (object-relational database management system), và ta sẽ dung nó làm data warehouse cho project lần này. de_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:3","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#postgesql"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagit"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster-deamon"},{"categories":["projects"],"content":"PipelineTất cả mọi việc xây dựng pipeline ta sẽ hoạt động ở đây Đầu tiên ta cần init một dagster project dagster project scaffold --name etl_pipeline và thư mục mới tạo sẽ trông như thế này: Tip Để chạy được lệnh dagster ở bước tạo dagster project, ta cần phải có dagster package, nếu chưa có hãy cài đặt bằng: pip install dagster –\u003e Nên cài đặt trong môi trường ảo Sau đó vào thư mục vừa tạo vào viết Dockerfile thôi: FROMpython:3.10-slim# Add repository codeWORKDIR/opt/dagster/appCOPY requirements.txt /opt/dagster/appRUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txtWORKDIR/opt/dagster/appCOPY . /opt/dagster/app/etl_pipeline# CMD allows this to be overridden from run launchers or executors that want to run other commands against your repositoryCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"etl_pipeline\"] cùng với requirements.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-aws==0.17.20 dagster-dbt==0.17.20 pandas==1.5.3 SQLAlchemy==1.4.46 pymysql==1.0.2 cryptography==39.0.0 pyarrow==10.0.1 boto3==1.26.57 fsspec==2023.1.0 s3fs==0.4.2 minio==7.1.13 Cuối cùng là viết vào docker-compose.yml: etl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:5","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#pipeline"},{"categories":["projects"],"content":"StreamlitCuối cùng là việc là Dashboard, Streamlit chắc chắc là công cụ siêu phù hợp làm việc này. Đây là framework hỗ trợ việc xây dựng giao diện ưu nhìn chỉ bằng Python, quá đã phải không nào :)) Hãy tạo folder ./streamlit/scr/ cùng với ./streamlit/Dockerfile: FROMpython:3.10EXPOSE8501WORKDIR/usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . . và streamlit/requirements.txt: pandas plotly matplotlib numpy streamlit psycopg2-binary sqlalchemy python-dotenv Cuối cùng là ghi trong yml streamlit:build:./streamymlcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:6","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#streamlit"},{"categories":["projects"],"content":"Hoàn chỉnh setupCuối cùng, file yaml sẽ trông như thế này: # version: '3.9'services:minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_networkde_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_networkde_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_networkstreamlit:build:./streamlitcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_networkde_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagsterde_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_networkde_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network# Pipelinesetl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_networknetworks:de_network:driver:bridgename:de_network Và .env file: # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_DB=football POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_HOST_AUTH_METHOD=trust # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=football # MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=football MYSQL_ROOT_PASSWORD=admin123 MYSQL_USER=admin MYSQL_PASSWORD=admin123 # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=warehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 Warning Nếu bạn chỉ đọc phần Hoàn chỉnh setup thì có thể hệ thống vễ sẽ gặp lỗi vì có thể thiếu các configuration cần thiết cho dagster, dagit hay pipeline. Bạn cần đọc qua phần Dagster, Dagit, Pipeline Chạy thử: sau khi hoàn tất toàn bộ set up dài ngoằn, cũng đã đến lúc chạy chương trình thôi. Note nho nhỏ Nếu bạn đã build lần lượt các images rồi, thì chỉ cần compose up thôi, nếu không, hãy build bằng lệnh docker compose build trước khi chạy compose up. docker compose --env-file .env up -d Lại là một tip có thể hữu ích Để đơn giản hóa việc ghi lệnh dài dòng, hãy tạo một make file tên Makefile với nội dung sau: include .env build: docker compose build up: docker compose --env-file .env up -d down: docker compose --env-file .env down restart: make down \u0026\u0026 make up to_psql: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} psql_create: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} -f /tmp/psql_schema.sql to_mysql: docker exec -it de_mysql mysql --local-infile=1 -u\"${MYSQL_U","date":"31 Jul 2023","objectID":"/football_etl/:2:7","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#hoàn-chỉnh-setup"},{"categories":["projects"],"content":"To be ContinueBài đến đây cũng quá là dài rồi, mình sẽ viết tiếp ở phần 2 :))) Chúc bạn một ngày tốt lành -Mew- ","date":"31 Jul 2023","objectID":"/football_etl/:3:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#to-be-continue"},{"categories":["projects"],"content":"Related Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... Spotify Analysis Analyze data from Spotify platform utilizing the Spotify API and MongoDB, Apache Hadoop, Pyspark, Dremio and Power BI Read more... ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#related"},{"categories":[],"content":"Data Engineering Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Spotify Analysis Analyze data from Spotify platform utilizing the Spotify API and MongoDB, Apache Hadoop, Pyspark, Dremio and Power BI Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#data-engineering"},{"categories":[],"content":"Machine learning Basic Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#machine-learning-basic"},{"categories":null,"content":"BackgroundHey there! My name is Huynh Luu Vinh Phong, the author of this page. I am currently a final year student at University of Science Ho Chi Minh City, majoring in Data Science. My love for math and solving problems is what first got me interested in data, but over time, I discovered that coding is what truly excites me. Now, my goal is to become a Data Engineer, where I can work with data everyday and turn it into something meaningful. University of Science Ho Chi Minh City\" University of Science Ho Chi Minh City ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#background"},{"categories":null,"content":"Learning is a journeyThe world of data is huge, indeed. And I am just at the beginning of my journey. I enjoy learning new things and diving deeper into data tools and technologies. Every day is a chance to grow and explore more of this exciting field, and I’m ready to take on the challenges ahead. There is a quote that motivate me during my journey: The most beautiful thing about learning is that no one can take it away from you This quote is about the power of learning is truly your inside power and your instinct. Therefore, no one can take it away from you. And going along with the ability to learn more, you will have the power to change the world. When I’m not busy with school or coding, I enjoy playing badminton and getting lost in anime or manga. I’ve always been fascinated by Japanese culture, and exploring it through anime is one of my favorite ways to unwind. I believe that learning is a lifelong journey, and I’m always eager to pick up new skills, whether it’s in data science or in my personal hobbies. To me, learning isn’t just about getting better grades or landing a job—it’s about shaping my future. I truly believe that by continuously learning, I can create the life I want and make an impact in the world of data. And that’s what keeps me motivated every day. -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huỳnh Lưu Vĩnh Phong Facebook: Phong Huynh Instagram: phong_huynh Visit my Github PhongHuynh0394 Huỳnh Lưu Vĩnh Phong ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#learning-is-a-journey"}]