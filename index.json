[{"categories":[],"content":" Tháº±ng nhÃ³c thÃ­ch code vÃ  data NgÃ nh Data cÃ³ gÃ¬ hot mÃ  mÃ¬nh láº¡i dÃ­nh Read more... ","date":"24 Sep 2023","objectID":"/blogs/:0:0","series":[],"tags":[],"title":"Blogs","uri":"/blogs/#"},{"categories":["projects"],"content":"Stock Analysis using PCA and K-means","date":"31 Aug 2023","objectID":"/stock_analysis/","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/"},{"categories":["projects"],"content":"Warning ÄÃ¢y lÃ  kiáº¿n thá»©c tÃ­ch gÃ³p tá»« nhiá»u nguá»“n vÃ  nghiÃªn cá»©u cá»§a nhÃ³m OG, táº¥t nhiÃªn khÃ´ng thá»ƒ trÃ¡nh khá»i sai sÃ³t. Hy vá»ng bÃ i viáº¿t láº§n nÃ y thÃº vá»‹ vÃ  giÃºp báº¡n Ä‘á»c thÆ° giÃ£n, tham kháº£o. Source github: Stock Analysis Hello! OG Ä‘Ã¢y. á» project láº§n nÃ y mÃ¬nh sáº½ phÃ¢n tÃ­ch gia trá»‹ cá»• phiáº¿u phÃ¡i sinh VN30 Index báº±ng cÃ¡ch sá»­ dá»¥ng PCA vÃ  K-means. Xin vÃ´ cÃ¹ng cáº£m Æ¡n sá»± Ä‘Ã³ng gÃ³p cá»§a 5 thÃ nh viÃªn team OG vÃ  tháº§y Minh Máº«n vÃ  tháº§y HoÃ ng Äá»©c Ä‘Ã£ táº­n tÃ¬nh hÆ°á»›ng dáº«n Ä‘á»ƒ team cÃ³ thá»ƒ hoÃ n thÃ nh Ä‘á»“ Ã¡n má»™t cÃ¡ch tá»‘t nháº¥t. Rá»“i bÃ¢y giá» gÃ©t gÃ´ thooiii ğŸ˜„ ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#"},{"categories":["projects"],"content":"IntroStock Analysis hay cÃ²n gá»i lÃ  Market Analysis Ä‘á» cáº­p Ä‘áº¿n phÆ°Æ¡ng phÃ¡p mÃ  nhÃ  Ä‘áº§u tÆ° hoáº·c nhÃ  giao dá»‹ch sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ vÃ  Ä‘iá»u tra má»™t cÃ´ng cá»¥ giao dá»‹ch cá»¥ thá»ƒ, lÄ©nh vá»±c Ä‘áº§u tÆ° hoáº·c toÃ n bá»™ thá»‹ trÆ°á»ng chá»©ng khoÃ¡n. KhÃ´ng nhá»¯ng tháº¿, nÃ³ liÃªn quan Ä‘áº¿n viá»‡c nghiÃªn cá»©u dá»¯ liá»‡u thá»‹ trÆ°á»ng trong quÃ¡ khá»© vÃ  hiá»‡n táº¡i vÃ  táº¡o ra má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ chá»n cá»• phiáº¿u phÃ¹ há»£p Ä‘á»ƒ giao dá»‹ch. CÃ¡c nhÃ  Ä‘áº§u tÆ° sáº½ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh mua hoáº·c bÃ¡n dá»±a trÃªn thÃ´ng tin phÃ¢n tÃ­ch chá»©ng khoÃ¡n. Trong project nÃ y ta sáº½ phÃ¢n tÃ­ch, trá»±c quan hÃ³a bá»™ dá»¯ liá»‡u giáº£ Ä‘á»‹nh Ä‘Æ°á»£c cung cáº¥p bá»Ÿi khÃ¡ch hÃ ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ thá»‹ trÆ°á»ng chá»©ng khoÃ¡n trong khoáº£ng thá»i gian 1 thÃ¡ng cá»§a 30 cÃ´ng ty thuá»™c VN30 DÆ°á»›i Ä‘Ã¢y lÃ  tÃ³m táº¯t sÆ¡ lÆ°á»£c tá»«ng bÆ°á»›c Ä‘á»ƒ xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch: EDA (Exploratory Data Analysis) Data Preprocessing PCA (Principle Component Analysis) K-Means Clustering Data Analysis References Raw Data Source: df_merged.pkl Raw data lÃ  dá»¯ liá»‡u báº£ng giÃ¡ cá»• phiáº¿u cá»§a 30 cÃ´ng ty thuá»™c VN30 Index + 1 trÆ°á»ng giÃ¡ phÃ¡i sinh trong 1 thÃ¡ng ","date":"31 Aug 2023","objectID":"/stock_analysis/:1:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#intro"},{"categories":["projects"],"content":"Exploratory Data AnalysisÄÃ¢y lÃ  bÆ°á»›c Ä‘áº§u tiÃªn, chÃºng ta sáº½ cÃ¹ng nhau tÃ¬m hiá»ƒu sÆ¡ lÆ°á»£c raw data cÅ©ng nhÆ° tÃ¬m hiá»ƒu cÃ¡i nhÃ¬n tá»•ng quÃ¡t vá» dá»¯ liá»‡u ta sáº¯p pháº£i phÃ¢n tÃ­ch Ä‘á»ƒ tá»« Ä‘Ã³ cÃ³ cÃ¡ch tiá»n xá»­ lÃ½ phÃ¹ há»£p. LÃ m gÃ¬ thÃ¬ lÃ m cá»© pháº£i import packages Ä‘á»ƒ Ä‘á»c data cÃ¡i Ä‘Ã£ ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#exploratory-data-analysis"},{"categories":["projects"],"content":"Data AcquistionTa sáº½ import má»™t sá»‘ packages quen thuá»™c Ä‘á»ƒ Ä‘á»c file df_merged.pkl import pickle import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd.read_pickle('https://github.com/PhongHuynh0394/My-respository/blob/main/df_merged.pkl?raw=true') # Check the data type type(data) # --\u003e list Data nháº­n Ä‘Æ°á»£c tá»« pickle file lÃ  má»™t list, bÃ¢y giá» ta sáº½ tÃ¬m kiáº¿m cÃ¡i nhÃ¬n tá»•ng quan vá» dá»¯ liá»‡u nÃ y ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-acquistion"},{"categories":["projects"],"content":"A Brief View Dá»¯ liá»‡u lÆ°u á»Ÿ pickle lÃ  má»™t list chá»©a 23 dataframe (df) Má»—i df cÃ³ index theo datetime (nghÄ©a lÃ  Ä‘Ã¢y lÃ  loáº¡i dá»¯ liá»‡u thuá»™c timeseries) CÃ¡c columns láº§n lÆ°á»£t lÃ  tá»«ng mÃ£ cá»• phiáº¿u, chá»©a khá»‘i lÆ°á»£ng/ giÃ¡ cá»§a cÃ¡c lá»‡nh mua/bÃ¡n sÃ¡t vá»›i lá»‡nh khá»›p I vÃ  khá»‘i lÆ°á»£ng cá»§a cÃ¡c lá»‡nh mua/bÃ¡n sÃ¡t vá»›i giÃ¡ khá»›p lá»‡nh II print('So luong df:', len(data)) # --\u003e So luong df: 23 Raw data lÃ  giÃ¡ lá»‡nh mua/bÃ¡n I II vÃ  khá»‘i lÆ°á»£ng giao dá»‹ch cá»§a cá»• phiáº¿u 30 cÃ´ng ty VN30raw data \" Raw data lÃ  giÃ¡ lá»‡nh mua/bÃ¡n I II vÃ  khá»‘i lÆ°á»£ng giao dá»‹ch cá»§a cá»• phiáº¿u 30 cÃ´ng ty VN30 Thá»i gian thu tháº­p Ä‘Æ°á»£c cáº­p nháº­t vá»›i chu kÃ¬ lÃ  10 giÃ¢y báº¯t Ä‘áº§u tá»« ngÃ y 20 thÃ¡ng 3 Ä‘áº¿n ngÃ y 19 thÃ¡ng 4, tá»« 2 giá» 15 Ä‘áº¿n 7 giá» 30 má»—i ngÃ y. NhÆ°ng cÃ³ má»™t sá»‘ ngÃ y bá»‹ miss trong bá»™ dá»¯ liá»‡u nÃ y (Chi tiáº¿t hÆ¡n trong notebook á»Ÿ source code) CÃ¹ng xem qua vá» sá»‘ lÆ°á»£ng observations cá»§a má»—i báº£ng Tá»•ng cá»™ng ta cÃ³ 181 fields vÃ  má»—i báº£ng khoáº£ng 1345 observations (tá»•ng cá»™ng 30538 quan sÃ¡t). CÅ©ng khÃ¡ nhiá»u pháº£i khÃ´ng nÃ o. Ta sáº½ cÃ¹ng tiá»n xá»­ lÃ½ chÃºng nÃ o ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#a-brief-view"},{"categories":["projects"],"content":"Data PreprocessingSau khi Ä‘Ã£ biáº¿t khÃ¡i quÃ¡t raw data, ta sáº½ cáº§n pháº£i tiá»n xá»­ lÃ½ nhá»¯ng dá»¯ liá»‡u thÃ´ nÃ y trÆ°á»›c khi cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh mÃ¡y há»c hoáº·c giáº£m chiá»u dá»¯ liá»‡u ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-preprocessing"},{"categories":["projects"],"content":"Data CleaningHÃ£y sá»­ dá»¥ng method describe() cá»§a pandas Ä‘á»ƒ cÃ³ cÃ¡i nhÃ¬n sÆ¡ bá»™ nháº¥t vá» df cá»§a chÃºng ta data[0].describe() Äáº§u tiÃªn, ta sáº½ drop duplicate vÃ  Ä‘á»‹nh dáº¡ng láº¡i index thá»i gian market = pd.DataFrame(columns=data[0].columns.to_list()) #create empty df # Data cleaning for _, df in enumerate(data): df.drop_duplicates() cols = df.columns.to_list() #convert/ replace 0 for col in cols: df[col] = pd.to_numeric(df[col], errors='coerce') # #missing handling df.fillna(0, inplace=True) market = pd.concat([market,df]).copy() #concat all clean df into market #datetime format market.reset_index(inplace=True) market = market.rename(columns={'index': 'datetime'}) market['datetime'] = market['datetime'].dt.strftime('%Y-%m-%d%H:%M:%S') market['datetime'] = pd.to_datetime(market['datetime']) market = market.sort_values(\"datetime\", ascending=True) market.set_index('datetime', inplace=True) Káº¿ tiáº¿p hÃ£y xá»­ lÃ½ missing value báº±ng phÆ°Æ¡ng phÃ¡p ná»™i suy (interpolation) vá»›i method padding, vÃ  sau Ä‘Ã³ sáº½ dÃ¹ng backfill ÄÃ¢y lÃ  phÆ°Æ¡ng phÃ¡p Æ°á»›c tÃ­nh giÃ¡ trá»‹ cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u chÆ°a biáº¿t trong pháº¡m vi cá»§a má»™t táº­p há»£p rá»i ráº¡c chá»©a má»™t sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã£ biáº¿t. Nghe cÃ³ váº» láº±ng nháº±ng, Ä‘Æ¡n giáº£n lÃ  tháº¿ nÃ y: .interpolate(method=â€˜padâ€™): fill null values báº±ng giÃ¡ trá»‹ liá»n ká» nÃ³ láº§n lÆ°á»£t tá»« trÃªn xuá»‘ng (nÃ³ giá»‘ng nhÆ° ffill()) .fillna(method=â€˜backfillâ€™): ÄÃ¢y lÃ  phÆ°Æ¡ng phÃ¡p ngÆ°á»£c láº¡i bÃªn trÃªn, fill null báº±ng giÃ¡ trá»‹ liá»n ká» tá»« dÆ°á»›i lÃªn Note CÃ³ ráº¥t nhiá»u phÆ°Æ¡ng phÃ¡p ná»™i suy nhÆ° linear (default) hay polynomial,â€¦ NhÆ°ng OG chá»n padding vÃ  backfill vÃ¬ 2 phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ giá»¯ cho data missing á»Ÿ giÃ¡ trá»‹ sÃ¡t nháº¥t vá»›i giÃ¡ trá»‹ thá»±c gáº§n nháº¥t vÃ  giÃºp cho káº¿t quáº£ sau khi fill sÃ¡t vá»›i thá»±c táº¿ nháº¥t. NgoÃ i ra 2 phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ fill Ä‘Æ°á»£c vá»‹ trÃ­ Ä‘áº§u vÃ  cuá»‘i cÃ¹ng má»™t cÃ¡ch hiá»‡u quáº£. def handle_null(X: pd.DataFrame) -\u003e pd.DataFrame: ''' handle missing value ''' for col in X.columns.to_list(): X[col].interpolate(method='pad', inplace=True) X[col].fillna(method='backfill', inplace=True) return X ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-cleaning"},{"categories":["projects"],"content":"Data transformingOG nháº­n tháº¥y ráº±ng vá»›i cÃ¡c trÆ°á»ng data hiá»‡n táº¡i chÆ°a thá»±c sá»± giÃºp Ã­ch quÃ¡ nhiá»u trong viá»‡c phÃ¢n tÃ­ch sau nÃ y (giÃ¡ mua/bÃ¡n vÃ  sá»‘ lÆ°á»£ng mua/bÃ¡n + giÃ¡ phÃ¡i sinh (label) ) Do Ä‘Ã³ OG cáº§n má»™t dataframe má»›i vá»›i cÃ¡c trÆ°á»ng má»›i cÃ³ nhiá»u giÃ¡ trá»‹ phÃ¢n tÃ­ch hÆ¡n: gttb_ (GiÃ¡ trá»‹ trung bÃ¬nh): lÃ  column má»›i Ä‘Æ°á»£c tÃ­nh trÃªn bÃ¬nh quÃ¢n giÃ¡ cáº£ mua vÃ o, bÃ¡n ra cá»§a tá»«ng cá»• phiáº¿u Ä‘Æ°á»£c giao dá»‹ch THÃ€NH CÃ”NG trÃªn thá»‹ trÆ°á»ng. total_ban \u0026 total_mua (Tá»•ng bÃ¡n/mua khá»‘i lÆ°á»£ng 1): lÃ  column má»›i Ä‘á»ƒ tÃ­nh tá»•ng giÃ¡ bÃ¡n khá»‘i lÆ°á»£ng 1 cÅ©ng nhÆ° mua khá»‘i lÆ°á»£ng 1 cá»§a tá»«ng cá»‘ phiáº¿u Ä‘Æ°á»£c giao dá»‹ch trÃªn thá»‹ trÆ°á»ng. Gia_KL: sao chÃ©p giÃ¡ khá»‘i lÆ°á»£ng cá»§a tá»«ng mÃ£ cá»• phiáº¿u tá»« bá»™ dá»¯ liá»‡u ban Ä‘áº§u. (label) CÃ i Ä‘áº·t láº¡i index thá»i gian: group by cÃ¡c time-series theo phÃºt. def transform_raw(market: pd.DataFrame) -\u003e pd.DataFrame: # split stock name name = [col.split('_1')[-1] for col in market.columns.to_list() if 'mua_gia_1' in col] new_df = pd.DataFrame() for i in name: # calculate gttb (mean) new_df[f'gttb_{i}'] = ((market[f'mua_gia_1{i}'] * market[f'mua_kl_1{i}'] + market[f'ban_gia_1{i}'] * market[f'ban_kl_1{i}']) /(market[f'mua_kl_1{i}'] + market[f'ban_kl_1{i}'])).copy() # get ban_kl and mua_kl new_df[f'total_ban_{i}'] = market[f'ban_kl_1{i}'].copy() new_df[f'total_mua_{i}'] = market[f'mua_kl_1{i}'].copy() # get Gia KL new_df['Gia KL'] = market['Gia KL'].copy() new_df.set_index(market.index, inplace=True) gttb = [col for col in new_df.columns.to_list() if 'gttb' in col] + ['Gia KL'] mua_ban = [col for col in new_df.columns.to_list() if col not in gttb] # Group by minute result = new_df[gttb].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute]).mean() result = pd.concat([result,new_df[mua_ban].groupby([new_df.index.date, new_df.index.hour, new_df.index.minute ]).sum()],axis=1) #Set index in minute index = pd.to_datetime([f\"{d}{h}:{m}:00\" for (d, h, m) in result.index]) result.index = index #handle missing value result = handle_null(result) return result Rá»“i giá» transform rá»“i kiá»ƒm tra láº¡i sá»‘ lÆ°á»£ng quan sÃ¡t á»Ÿ báº£ng má»›i thÃ´i # Check the length of new data len(new_market) # --\u003e 5154 Vá»›i káº¿t quáº£ má»›i, chá»‰ cÃ²n láº¡i 5154 quan sÃ¡t mÃ  thÃ´i, khi rÃºt láº¡i má»™t sá»‘ lÆ°á»£ng quan sÃ¡t lá»›n nhÆ° váº­y, ta sáº½ pháº£i cháº¥p nháº­n rá»§i ro máº¥t Ä‘i nhiá»u thÃ´ng tin vá» dá»¯ liá»‡u mÃ  cá»¥ thá»ƒ lÃ  dá»¯ liá»‡u theo giÃ¢y (cá»© 10 giÃ¢y cáº­p nháº­t). NhÆ°ng Ä‘á»•i láº¡i, data sáº½ cÃ´ Ä‘á»™ng hÆ¡n vÃ  bá»›t nhiá»…u vÃ¬ vá»›i sá»± biáº¿n Ä‘á»•i cá»§a thá»‹ trÆ°á»ng trong cáº£ 1 thÃ¡ng, sá»± thay Ä‘á»•i cá»§a cÃ¡c trÆ°á»ng trong má»—i 10 giÃ¢y lÃ  quÃ¡ nhá» vÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-transforming"},{"categories":["projects"],"content":"Data ScalingSau khi cÃ³ bá»™ dataframe má»›i tá»‘t hÆ¡n vÃ  sáº¡ch sáº½, bÆ°á»›c káº¿ tiáº¿p sáº½ lÃ  scale láº¡i dá»¯ liá»‡u vá» má»™t chuáº©n Ä‘á»ƒ tÄƒng hiá»‡u quáº£ cá»§a cÃ¡c thuáº­t toÃ¡n há»c mÃ¡y CÃ³ má»™t sá»‘ phÆ°Æ¡ng phÃ¡p scale data nhÆ°: Standardization, Normalization,â€¦ á» project nÃ y, OG sáº½ dÃ¹ng phÆ°Æ¡ng phÃ¡p Normalization Ä‘á»ƒ scale data. PhÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a nÃ y Ä‘Æ°a tá»· lá»‡ dá»¯ liá»‡u tá»« pháº¡m vi ban Ä‘áº§u vá» chuáº©n pháº¡m vi tá»« 0 Ä‘áº¿n 1, giÃ¡ trá»‹ Ä‘Æ°á»£c normalize theo cÃ´ng thá»©c sau: $$ x' = \\frac{x - min}{max - min} $$ Vá»›i $x$ lÃ  giÃ¡ trá»‹ cáº§n Ä‘Æ°á»£c chuáº©n hÃ³a, $max$ vÃ  $min$ lÃ  láº§n lÆ°á»£t lÃ  giÃ¡ trá»‹ lá»›n nháº¥t vÃ  nhá» nháº¥t trong táº¥t cáº£ cÃ¡c observations cá»§a feature trong táº­p dá»¯ liá»‡u. Ta sáº½ dÃ¹ng MinMaxScaler cá»§a scikit-learn trong tÃ¡c vá»¥ nÃ y. from sklearn.preprocessing import MinMaxScaler # Normalization data using libraries min_max = MinMaxScaler() X = new_market.values X_std = min_max.fit_transform(X) print('Data after scaling: ') X_std # array([[9.10048201e-01, 9.43990665e-01, 9.59215952e-01, ..., # 1.31664615e-02, 1.45711006e-02, 2.90267046e-03], # [9.14492108e-01, 9.61493582e-01, 9.57989455e-01, ..., # 1.42007963e-02, 1.10109072e-04, 3.64335188e-03], # [9.12286536e-01, 9.57992999e-01, 9.56950233e-01, ..., # 3.58702686e-03, 1.43141794e-03, 4.40405173e-04], # ..., # [9.23665190e-01, 9.04317386e-01, 8.96622210e-01, ..., # 1.22024151e-01, 3.04635100e-03, 2.10293470e-02], # [9.24218272e-01, 8.89565349e-01, 8.97159958e-01, ..., # 1.05691866e-02, 1.13779375e-03, 2.88265204e-02], # [9.28532923e-01, 9.04317386e-01, 8.98196897e-01, ..., # 8.84087818e-03, 3.67030241e-04, 7.79383700e-03]] NhÆ° váº­y lÃ  Ä‘Ã£ chuáº©n bá»‹ hoÃ n táº¥t cho bÆ°á»›c tiáº¿p theo rá»“i. ChÃºng ta sáº½ bÆ°á»›c vÃ o thuáº­t toÃ¡n chÃ­nh Ä‘áº§u tiÃªn trong project nÃ y. ","date":"31 Aug 2023","objectID":"/stock_analysis/:3:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#data-scaling"},{"categories":["projects"],"content":"Principle Component Analysis (PCA)ChÃºng ta Ä‘Ã£ Ä‘i qua viá»‡c tiá»n xá»­ lÃ½ dÃ i ngoáº±n tá»« cleaning, transforming Ä‘áº¿n scaling. Váº­y cÃ¢u há»i lÃ : dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ Ã¡p dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh mÃ¡y há»c hay chÆ°a ? CÃ¢u tráº£ lá»i cho trÆ°á»ng há»£p nÃ y lÃ : ChÆ°a. Táº¡i sao váº­y ? Bá»Ÿi vÃ¬ táº­p dá»¯ liá»‡u cá»§a chÃºng ta cÃ³ quÃ¡ nhiá»u features Feature cá»§a táº­p data lÃ  gÃ¬ ? DÃ nh cho báº¡n chÆ°a biáº¿t, feature cá»§a táº­p data cÃ²n Ä‘Æ°á»£c gá»i lÃ  cÃ¡c trÆ°á»ng (hay field) cá»§a táº­p data Ä‘Ã³. ÄÃ³ lÃ  cÃ¡c cá»™t, má»—i cá»™t lÃ  má»™t â€œtÃ­nh cháº¥tâ€ khÃ¡c nhau cá»§a Ä‘á»‘i tÆ°á»£ng aka quan sÃ¡t (observation) thÆ°á»ng lÃ  cÃ¡c hÃ ng. Hiá»‡n táº¡i cÃ³ thá»ƒ tháº¥y cleaning data cá»§a chÃºng ta cÃ³ 91 features: gttb_(cá»• phiáº¿u): 30 cá»™t giÃ¡ trá»‹ trung bÃ¬nh giao dá»‹ch cá»§a 30 cá»• phiáº¿u trong 1 phÃºt total_ban_(cá»• phiáº¿u): 30 cá»™t tá»•ng khá»‘i lÆ°á»£ng bÃ¡n cá»§a 30 cá»• phiáº¿u trong 1 phÃºt total_mua_(cá»• phiáº¿u): 30 cá»™t tá»•ng khá»‘i lÆ°á»£ng mua cá»§a 30 cá»• phiáº¿u trong 1 phÃºt Gia_KL: 1 cá»™t giÃ¡ phÃ¡i sinh VN30 Index (label) Vá»›i sá»‘ lÆ°á»£ng feature lá»›n nhÆ° váº­y, sáº½ vÃ´ cÃ¹ng kÃ©m hiá»‡u quáº£ náº¿u ngay láº­p tá»©c sá»­ dá»¥ng train cho cÃ¡c mÃ´ hÃ¬nh machine learning. Giáº£i phÃ¡p á»Ÿ Ä‘Ã¢y chÃ­nh lÃ  ta sáº½ giáº£m chiá»u dá»¯ liá»‡u xuá»‘ng má»©c vá»«a Ä‘áº¡t hiá»‡u nÄƒng tá»‘t khi training mÃ  cÅ©ng khÃ´ng lÃ m máº¥t quÃ¡ nhiá»u thÃ´ng tin cá»§a dá»¯ liá»‡u. VÃ¢ng Ä‘Ãºng váº­y, phÆ°Æ¡ng phÃ¡p OG muá»‘n giá»›i thiá»‡u chÃ­nh lÃ  PCA hay cÃ²n Ä‘Æ°á»£c biáº¿t vá»›i tÃªn viá»‡t hÃ³a lÃ  PhÃ¢n tÃ­ch thÃ nh pháº§n chÃ­nh. Má»¥c tiÃªu cá»§a phÆ°Æ¡ng phÃ¡p nÃ y lÃ  Ä‘Æ°a bá»™ dá»¯ liá»‡u ban Ä‘áº§u sang há»‡ tá»a Ä‘á»™ má»›i dá»±a trÃªn cÃ¡c thÃ nh pháº§n chÃ­nh. Dá»¯ liá»‡u á»Ÿ há»‡ tá»a Ä‘á»™ má»›i cÃ³ Ã­t chiá»u hÆ¡n nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c nhiá»u nháº¥t thÃ´ng tin cÃ³ thá»ƒ, tá»« Ä‘Ã³ giÃºp tÄƒng tá»‘c Ä‘á»™ tÃ­nh toÃ¡n vÃ  giáº£m Ä‘á»™ phá»©c táº¡p mÃ´ hÃ¬nh hÆ¡n ráº¥t nhiá»u. NÃ³i tÃ³m táº¯t cho dá»… hiá»ƒu CÆ¡ báº£n lÃ  phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°a bá»™ data cá»§a ta vÃ o má»™t â€œtháº¿ giá»›i song songâ€ cÃ³ sá»‘ chiá»u má»›i Ã­t hÆ¡n (chiá»u aka features). Báº¡n cÃ³ thá»ƒ hiá»ƒu nhÆ° lÃ  nhÃ¬n dá»¯ liá»‡u cá»§a mÃ¬nh á»Ÿ má»™t gÃ³c khÃ¡c váº­y. á» pháº§n nÃ y chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y thÃ´ng qua sá»± phÃ¢n rÃ£ cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai (Eigen decomposition of covariance matrix) ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#principle-component-analysis-pca"},{"categories":["projects"],"content":"EigenVector vÃ  EigenValueMa tráº­n hiá»‡p phÆ°Æ¡ng sai Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ : $$ S = \\frac{1}{N}\\hat{X}^T\\hat{X} $$ Vá»›i $\\hat{X} = X - \\hat{x}1^T$ lÃ  zero-corrected data hay dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hoÃ¡. Ta sáº½ viáº¿t hÃ m get_eigenpairs() Ä‘á»ƒ tÃ¬m cÃ¡c vector riÃªng vÃ  giÃ¡ trá»‹ riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai: $$ Su_i = \\lambda_iu_i $$ Trong Ä‘Ã³: cÃ¡c $(\\lambda_i,u_i)$ lÃ  cÃ¡c cáº·p trá»‹ riÃªng (khÃ´ng Ã¢m) vÃ  vector riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai $S$ Táº¡i sao láº¡i cáº§n tÃ¬m cÃ¡c vector riÃªng vÃ  giÃ¡ trá»‹ riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai ? Viá»‡c sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ riÃªng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ sá»± quan trá»ng cá»§a má»—i thÃ nh pháº§n chÃ­nh Ä‘Æ°á»£c táº¡o ra tá»« viá»‡c giáº£m chiá»u dá»¯ liá»‡u. CÃ¡c giÃ¡ trá»‹ riÃªng cÃ ng lá»›n thÃ¬ thÃ nh pháº§n chÃ­nh tÆ°Æ¡ng á»©ng cÃ ng quan trá»ng. CÃ¡c vector riÃªng tÆ°Æ¡ng á»©ng vá»›i cÃ¡c giÃ¡ trá»‹ riÃªng nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh hÆ°á»›ng cá»§a cÃ¡c thÃ nh pháº§n chÃ­nh. GiÃ¡ trá»‹ riÃªng (Eigenvalues $\\lambda_i$): CÃ¡c há»‡ sá»‘ Ä‘Æ°á»£c gáº¯n vá»›i cÃ¡c vector riÃªng, cung cáº¥p cho Ä‘á»™ lá»›n cá»§a trá»¥c. Trong trÆ°á»ng há»£p nÃ y, chÃºng lÃ  thÆ°á»›c Ä‘o hiá»‡p phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u. Vector riÃªng (EigenVector $u_i$):CÃ¡c vector (khÃ¡c 0) khÃ´ng thay Ä‘á»•i hÆ°á»›ng khi Ã¡p dá»¥ng báº¥t ká»³ phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh (linear transformation) nÃ o, nÃ³ chá»‰ thay Ä‘á»•i theo há»‡ sá»‘ vÃ´ hÆ°á»›ng. HÃ m sáº¯p xáº¿p cÃ¡c vector riÃªng (Sort eigenvalues): Báº±ng cÃ¡ch sáº¯p xáº¿p cÃ¡c vector riÃªng theo thá»© tá»± cá»§a giÃ¡ trá»‹ riÃªng, ta cÃ³ thá»ƒ chá»n ra cÃ¡c vector riÃªng cÃ³ giÃ¡ trá»‹ riÃªng lá»›n nháº¥t Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c thÃ nh pháº§n chÃ­nh cá»§a dá»¯ liá»‡u (Ä‘Ã³ng gÃ³p nhiá»u nháº¥t vÃ o viá»‡c giáº£i thÃ­ch sá»± biáº¿n thiÃªn cá»§a dá»¯ liá»‡u). CÃ¡c thÃ nh pháº§n chÃ­nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¡i cáº¥u trÃºc dá»¯ liá»‡u ban Ä‘áº§u mÃ  váº«n giá»¯ Ä‘Æ°á»£c Ä‘á»™ giá»‘ng nhau cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u ban Ä‘áº§u. def get_eigenpairs(X: np.array) -\u003e list: ''' Input: X: np.array (init matrix) return eigenpairs containing eigenvalues and eigenvectors of covariance matrix ''' # Covariance matrix cov_mat = np.cov(X.T) # Eigenvalues and Eigenvectors evals, evecs = np.linalg.eigh(cov_mat) # Sort eigenvalues epairs = [(abs(eval), evec) for (eval, evec) in zip(evals, evecs.T)] epairs = sorted(epairs, key = lambda pair: pair[0], reverse = True) return epairs ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:1","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#eigenvector-vÃ -eigenvalue"},{"categories":["projects"],"content":"Cumulative Sum of ComponentsTÃ­nh tá»•ng tÃ­ch lÅ©y cá»§a cÃ¡c thÃ nh pháº§n trong PCA (Cumulative Sum of Explained Variance) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh tá»•ng pháº§n trÄƒm phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi cÃ¡c thÃ nh pháº§n Ä‘Æ°á»£c giá»¯ láº¡i trong mÃ´ hÃ¬nh PCA. $$ r_K = \\frac{\\sum^K_{i=1}\\lambda_i}{\\sum^D_{j=1}\\lambda_j} $$ lÃ  lÆ°á»£ng thÃ´ng tin Ä‘Æ°á»£c giá»¯ láº¡i khi sá»‘ chiá»u dá»¯ liá»‡u má»›i sau PCA lÃ  K. HÃ m findNumVec() thá»±c hiá»‡n viá»‡c láº¥y cÃ¡c giÃ¡ trá»‹ riÃªng tá»« danh sÃ¡ch cÃ¡c eigenpairs vÃ  chuyá»ƒn Ä‘á»•i chÃºng thÃ nh má»™t máº£ng numpy. Sau Ä‘Ã³, nÃ³ tÃ­nh tá»•ng tÃ­ch lÅ©y cá»§a cÃ¡c giÃ¡ trá»‹ riÃªng, sá»­ dá»¥ng hÃ m np.cumsum () chuáº©n hÃ³a tá»•ng cá»§a chÃºng =\u003e cho ra má»™t danh sÃ¡ch cÃ¡c giÃ¡ trá»‹ (trong khoáº£ng tá»« 0 Ä‘áº¿n 1) Ä‘áº¡i diá»‡n cho tá»· lá»‡ pháº§n trÄƒm phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi má»—i thÃ nh pháº§n chÃ­nh. Sau Ä‘Ã³, hÃ m láº·p qua danh sÃ¡ch tá»•ng tÃ­ch lÅ©y vÃ  tÃ¬m chá»‰ má»¥c cá»§a giÃ¡ trá»‹ Ä‘áº§u tiÃªn lá»›n hÆ¡n hoáº·c báº±ng tá»· lá»‡ pháº§n trÄƒm phÆ°Æ¡ng sai mong muá»‘n Ä‘Æ°á»£c giáº£i thÃ­ch. Chá»‰ sá»‘ nÃ y Ä‘áº¡i diá»‡n cho sá»‘ lÆ°á»£ng thÃ nh pháº§n chÃ­nh cáº§n thiáº¿t Ä‘á»ƒ giáº£i thÃ­ch tá»· lá»‡ pháº§n trÄƒm phÆ°Æ¡ng sai Ä‘Ã³, vÃ¬ váº­y hÃ m tráº£ vá» giÃ¡ trá»‹ nÃ y cá»™ng vá»›i 1 (vÃ¬ láº­p chá»‰ má»¥c Python báº¯t Ä‘áº§u tá»« 0). def findNumVec(eigenpairs: list, percent = 0.9): ''' Find number of principal components (eigenvectors) -\u003e return the number of principal components when total accumulate \u003e= percent ''' # Get eigenvalues eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) # Cumulative sum and calculate percent cumsum = np.cumsum(eigenvals) cumsum /= cumsum[-1] # Find number of principal components that accumulate \u003e= percent for i, val in enumerate(cumsum): if val \u003e= percent: return i + 1 Ta sáº½ thá»­ tÃ¬m xem sá»‘ thÃ nh pháº§n chÃ­nh cáº§n Ä‘á»ƒ giá»¯ Ä‘Æ°á»£c 80% dá»¯ liá»‡u: print(findNumVec(epairs, 0.8)) # --\u003e 28 ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:2","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#cumulative-sum-of-components"},{"categories":["projects"],"content":"Scree ChartTa sáº½ váº½ má»™t biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n quan há»‡ cá»§a sá»‘ lÆ°á»£ng thÃ nh pháº§n chÃ­nh vÃ  pháº§n trÄƒm phÆ°Æ¡ng sai giáº£i thÃ­ch tÃ­ch lÅ©y def screeplot(eigenpairs): ''' Scree plot ''' fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True) eigenvals = [eigenval for (eigenval, _) in eigenpairs] eigenvals = np.array(eigenvals) cumsum = np.cumsum(eigenvals) # extracts the eigenvalues from the eigenpairs and calculates their cumulative sum cumsum /= cumsum[-1] name = [f'PCA {i}' for i in range(len(cumsum))] # line plot # the eigenvalues are plotted against the number of principal components axes[0].plot(range(len(eigenvals)), eigenvals, marker = '.', color = 'b', label = 'Eigenvalue') # the cumulative proportion of the variance explained by each component is plotted against the number of principal components axes[1].plot(range(len(cumsum)), cumsum, marker = '.', color = 'green', label = 'Cumulative propotion') # y axis label axes[0].set_ylabel('Eigen values') axes[1].set_ylabel('Cumulative explained variance') # item legend axes[0].legend() axes[1].legend() # grid axes[0].grid() axes[1].grid() # title fig.supxlabel('Number of components') plt.tight_layout() plt.show() #print the cumsum of eigenvalues print(pd.DataFrame(cumsum, columns = ['Cumulative total'], index = name)) result = { str(i): f\"PC {i+1}({var:.1f}%)\" for i, var in enumerate(cumsum*100) } return result pca_scree = screeplot(epairs) Scree plotScree plot \" Scree plot Giáº£i thÃ­ch ÄÆ°á»ng cá»§a giÃ¡ trá»‹ riÃªng mÃ u xanh nÆ°á»›c biá»ƒn trÃªn biá»ƒu Ä‘á»“ cho ta biáº¿t Ä‘á»™ lá»›n cá»§a má»—i thÃ nh pháº§n chÃ­nh vÃ  táº§m quan trá»ng cá»§a chÃºng trong giáº£i thÃ­ch sá»± biáº¿n thiÃªn cá»§a dá»¯ liá»‡u. Náº¿u giÃ¡ trá»‹ riÃªng cá»§a má»™t thÃ nh pháº§n chÃ­nh lÃ  lá»›n, thÃ¬ thÃ nh pháº§n Ä‘Ã³ cÃ³ táº§m quan trá»ng cao trong viá»‡c giáº£i thÃ­ch sá»± biáº¿n thiÃªn cá»§a dá»¯ liá»‡u. ÄÆ°á»ng mÃ u xanh lÃ¡ thá»ƒ hiá»‡n tá»•ng tÃ­ch lÅ©y cho ta biáº¿t tá»•ng pháº§n trÄƒm Ä‘á»™ lá»›n cá»§a sá»± biáº¿n thiÃªn cá»§a dá»¯ liá»‡u mÃ  cÃ¡c thÃ nh pháº§n chÃ­nh cÃ³ thá»ƒ giáº£i thÃ­ch. Dá»±a vÃ o biá»ƒu Ä‘á»“ trÃªn cÃ³ thá»ƒ nháº­n tháº¥y náº¿u chá»‰ cÃ³ 2 chiá»u, ta chá»‰ giá»¯ Ä‘Æ°á»£c khoáº£ng 37% dá»¯ liá»‡u ban Ä‘áº§u. ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:3","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#scree-chart"},{"categories":["projects"],"content":"Visualize PCABÃ¢y giá», ta sáº½ thá»±c hiá»‡n chiáº¿u dá»¯ liá»‡u ban Ä‘áº§u Ä‘Ã£ chuáº©n hÃ³a $\\hat{X}$ xuá»‘ng khÃ´ng gian con tÃ¬m Ä‘Æ°á»£c vÃ  láº¥y ra ma tráº­n cÃ¡c thÃ nh pháº§n chÃ­nh Ä‘á»ƒ tiáº¿p tá»¥c cÃ´ng viá»‡c phÃ¢n tÃ­ch vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh. HÃ m getPC() tráº£ vá» má»™t ma tráº­n cÃ¡c thÃ nh pháº§n chÃ­nh tá»« ma tráº­n ban Ä‘áº§u, dá»±a trÃªn sá»‘ lÆ°á»£ng thÃ nh pháº§n Ä‘Ã£ cho hoáº·c sá»‘ lÆ°á»£ng thÃ nh pháº§n giá»¯ Ä‘Æ°á»£c 80% dá»¯ liá»‡u (náº¿u num_components khÃ´ng Ä‘Æ°á»£c Ä‘Æ°a ra). Ma tráº­n trá»ng sá»‘ $W$ lÃ  ma tráº­n chuyá»ƒn Ä‘á»•i tuyáº¿n tÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u gá»‘c vÃ o khÃ´ng gian má»›i, trong Ä‘Ã³ má»—i thÃ nh pháº§n chÃ­nh Ä‘Æ°á»£c sáº¯p xáº¿p theo Ä‘á»™ quan trá»ng giáº£m dáº§n. Cá»¥ thá»ƒ, má»—i cá»™t cá»§a ma tráº­n $W$ lÃ  má»™t vector riÃªng chuáº©n hÃ³a tÆ°Æ¡ng á»©ng vá»›i cÃ¡c giÃ¡ trá»‹ riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai $s$. Thá»±c hiá»‡n viá»‡c nhÃ¢n ma tráº­n $W$ vá»›i hoÃ¡n vá»‹ cá»§a ma tráº­n Ä‘Ã£ chuáº©n hÃ³a $\\hat{X}$ (init_matrix). Ma tráº­n káº¿t quáº£ sau Ä‘Ã³ tiáº¿p tá»¥c Ä‘Æ°á»£c hoÃ¡n vá»‹ Ä‘á»ƒ phÃ¹ há»£p vá»›i hÃ¬nh dáº¡ng ban Ä‘áº§u cá»§a init_matrix vÃ  tráº£ vá» káº¿t quáº£. def getPC(eigenpairs, init_matrix, num_components = None): ''' Return matrix of principal components from init_matrix ''' # default num_components = number which to keep 80% data if num_components is None: num_components = findNumVec(eigenpairs, 0.8) # extracts the eigen vectors corresponding to the top num_components eigenvalues from the eigenpairs list eigenvecs = [eigenvec for (_, eigenvec) in eigenpairs[:num_components]] W = np.array([e.T for e in eigenvecs]) # stacks the eigen vectors into a weight matrix W return (W @ init_matrix.T).T X_pca = getPC(epairs, X_std) Váº­y lÃ  ta Ä‘Ã£ giáº£m Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p cho bá»™ dá»¯ liá»‡u khÃ¡ â€œnhá»c nháº±nâ€ nÃ y. HÃ£y trá»±c quan hÃ³a lÃªn biá»ƒu Ä‘á»“ Ä‘á»ƒ cÃ³ má»™t gÃ³c nhÃ¬n cá»¥ thá»ƒ vÃ  rÃµ rÃ ng hÆ¡n. Biá»ƒu Ä‘á»“ scatter plot sau khi PCA cÃ³ thá»ƒ giÃºp cho chÃºng ta nhÃ¬n tháº¥y cÃ¡ch dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¢n bá»‘ trÃªn cÃ¡c thÃ nh pháº§n chÃ­nh (principal components) vÃ  kiá»ƒm tra xem liá»‡u chÃºng ta cÃ³ thá»ƒ tÃ¬m tháº¥y cÃ¡c cluster hoáº·c pattern nÃ o trong dá»¯ liá»‡u. plt.scatter(X_pca[:,0], X_pca[:,1]) plt.xlabel('PC1') plt.ylabel('PC2') plt.title('Visualizing data through PCA', fontsize=18) plt.gca().set_aspect('equal', 'datalim') plt.grid() plt.show() Visualizing data via PCAVisualizing data via PCA \" Visualizing data via PCA Okayy dá»±a vÃ o biá»ƒu Ä‘á»“ trÃªn, cÅ©ng cÃ³ thá»ƒ tháº¥y lÃ  dá»¯ liá»‡u á»Ÿ khÃ´ng gian má»›i Ä‘Ã£ phÃ¢n tÃ¡ch khÃ¡ rÃµ rÃ ng rá»“i. Äiá»u nÃ y nghÄ©a lÃ  phÆ°Æ¡ng phÃ¡p PCA Ä‘Ã£ giáº£m sá»‘ chiá»u cá»§a dá»¯ liá»‡u má»™t cÃ¡ch hiá»‡u quáº£. BÆ°á»›c tiáº¿p theo chÃ­nh lÃ  Ã¡p vÃ o mÃ´ hÃ¬nh K-Means Ä‘á»ƒ phÃ¢n cá»¥m vÃ  tÃ¬m pattern. ChÃºng ta sáº½ cÃ¹ng chiáº¿n tiáº¿p á»Ÿ pháº§n 2 nhÃ© ","date":"31 Aug 2023","objectID":"/stock_analysis/:4:4","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#visualize-pca"},{"categories":["projects"],"content":"To be ContinueChÃºng ta Ä‘Ã£ thá»±c hiá»‡n cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ dá»¯ liá»‡u vÃ  sau Ä‘Ã³ lÃ  thá»±c hiá»‡n PCA Ä‘á»ƒ giáº£m chiá»u dá»¯ liá»‡u má»™t cÃ¡ch hiá»‡u quáº£. BÃ i sau pháº§n 2, OG sáº½ thá»±c hiá»‡n training mÃ´ hÃ¬nh K-means clustering vÃ  cuá»‘i cÃ¹ng lÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u chá»©ng khoÃ¡ng. ÄÃ¢y lÃ  kiáº¿n thá»©c tÃ­ch gÃ³p tá»« nhiá»u nguá»“n vÃ  nghiÃªn cá»©u cá»§a nhÃ³m OG, táº¥t nhiÃªn khÃ´ng thá»ƒ trÃ¡nh khá»i sai sÃ³t. Hy vá»ng bÃ i viáº¿t láº§n nÃ y thÃº vá»‹ vÃ  giÃºp báº¡n Ä‘á»c thÆ° giÃ£n, tham kháº£o. -Mew- ","date":"31 Aug 2023","objectID":"/stock_analysis/:5:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#to-be-continue"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis P2 Stock Analysis using PCA and K-means Read more... ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":["Stock analysis"],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#related"},{"categories":[],"content":"Hellooo OG Ä‘Ã¢yy ! á» bÃ i nÃ y, mÃ¬nh sáº½ ká»ƒ cÆ¡ duyÃªn Ä‘Æ°a mÃ¬nh Ä‘áº¿n vá»›i ngÃ nh Data vÃ  quyáº¿t Ä‘á»‹nh dáº¥n thÃ¢n vÃ o con Ä‘Æ°á»ng trá»Ÿ thÃ nh má»™t Data Engineer ğŸ˜„ GÃ©t Goo! ","date":"30 Aug 2023","objectID":"/start_journey/:0:0","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#"},{"categories":[],"content":"á»¦a ngÃ nh Data Science ?Khoan Khoan â€¦ BÃªn trÃªn lÃ  Engineer, qua Ä‘Ã¢y lÃ  Science lÃ  sao OG ? Tá»« tá»« nÃ o ğŸ˜„ Má»i chuyá»‡n báº¯t Ä‘áº§u khi mÃ¬nh Ä‘áº­u vÃ o má»™t ngÃ nh Ä‘Æ°á»£c ca ngá»£i lÃ  ngÃ nh â€œquyáº¿n rÅ©â€ nháº¥t tháº¿ ká»· 21 theo Harvard Business Review , Ä‘Ã³ lÃ  Data Science. KhÃºc nÃ y mÃ¬nh nghe cÅ©ng oÃ¡ch oÃ¡ch, nhÆ°ng chÃ­nh xÃ¡c Data Science lÃ  gÃ¬ ? VÃ  cÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u (data scientist) lÃ m gÃ¬ ? ","date":"30 Aug 2023","objectID":"/start_journey/:1:0","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#á»§a-ngÃ nh-data-science-"},{"categories":[],"content":"Data Science lÃ  gÃ¬ nhá»‰?NgÃ nh Khoa há»c dá»¯ liá»‡u hay Data Science lÃ  má»™t lÄ©nh vá»±c liÃªn ngÃ nh á»©ng dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p khoa há»c, thuáº­t toÃ¡n vÃ  cÃ¡c phÃ¢n tÃ­ch thá»‘ng kÃª Ä‘á»ƒ tÃ¬m kiáº¿m Ã½ nghÄ©a tá»« dá»¯ liá»‡u. Hay nÃ³i báº±ng cÃ¡ch dá»… hiá»ƒu, Data Science lÃ  ngÃ nh tÃ¬m kiáº¿m, phÃ¢n tÃ­ch dá»¯ liá»‡u Ä‘á»ƒ khai thÃ¡c táº¥t cáº£ nhá»¯ng giÃ¡ trá»‹ mÃ  dá»¯ liá»‡u mang láº¡i Ä‘á»ƒ phá»¥c vá»¥ nhiá»u má»¥c Ä‘Ã­ch khÃ¡c nhau. Data Science lÃ  á»©ng dá»¥ng khoa há»c Ä‘á»ƒ tÃ¬m kiáº¿m Ã½ nghÄ©a cá»§a dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n tÆ°Æ¡ng laiData Science lÃ  á»©ng dá»¥ng khoa há»c Ä‘á»ƒ tÃ¬m kiáº¿m Ã½ nghÄ©a cá»§a dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n \" Data Science lÃ  á»©ng dá»¥ng khoa há»c Ä‘á»ƒ tÃ¬m kiáº¿m Ã½ nghÄ©a cá»§a dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai Má»™t nhÃ  khoa há»c dá»¯ liá»‡u (Data Scientist) lÃ  ngÆ°á»i chá»‹u trÃ¡ch nhiá»‡m Ä‘Æ°a ra cÃ¡c dáº«n chá»©ng tá»« dá»¯ liá»‡u, Ä‘á»ƒ tá»« Ä‘Ã³ Ä‘á» xuáº¥t cÃ¡c giáº£i phÃ¡p, káº¿ hoáº¡ch hay Ä‘á»‹nh hÆ°á»›ng tá»« Ã½ nghÄ©a tÃ¬m Ä‘Æ°á»£c tá»« dá»¯ liá»‡u Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n kinh doanh khÃ¡c nhau. Má»™t data scientist cáº§n pháº£i biáº¿t ká»¹ nÄƒng gÃ¬? Láº­p trÃ¬nh: Python vÃ  R lÃ  2 ngÃ´n ngá»¯ chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»‘i vá»›i ngÃ nh nÃ y. Python lÃ  má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh linh hoáº¡t phá»• biáº¿n vá»›i ráº¥t nhiá»u thÆ° viá»‡n Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u nhÆ° numpy, pandas, matplotlib,â€¦ Trong khi Ä‘Ã³ R tá» lÃ  lÃ  má»™t ngÃ´n ngá»¯ máº¡nh máº½ vá» phÃ¢n tÃ­ch vÃ  thá»‘ng kÃª, ngoÃ i ra R cÅ©ng thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng trong nghiÃªn cá»©u vÃ  há»c thuáº­t. Thá»‘ng kÃª vÃ  á»©ng dá»¥ng toÃ¡n há»c: Náº¿u báº¡n khÃ´ng yÃªu thÃ­ch toÃ¡n há»c, cháº¯c háº³n báº¡n cÅ©ng sáº½ khÃ´ng thá»ƒ lÃ m Ä‘iá»u Ä‘Ã³ vá»›i data science. Háº³n váº­y, lÃ  má»™t nhÃ  khoa há»c dá»¯ liá»‡u, báº¡n cáº§n cÃ³ má»™t ná»n táº£ng kiáº¿n thá»©c toÃ¡n há»c vá»¯ng, Ä‘áº·c biá»‡t lÃ  vá» xÃ¡c suáº¥t thá»‘ng kÃª vÃ  Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh,â€¦ SQL vÃ  DBMS: Ta pháº£i tiáº¿p xÃºc ráº¥t nhiá»u vá»›i há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u (Database Management System hay DBMS), Ä‘Ã³ cÃ³ thá»ƒ lÃ  há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u Quan Há»‡ (Relational Database Management System) nhÆ° MySQL, Postgres, SQL serverâ€¦ hay NoSQL database nhÆ° MongoDB, Cassandra,â€¦ VÃ  Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i database (RDBMS), Ä‘iá»u khÃ´ng thá»ƒ thiáº¿u chÃ­nh lÃ  SQL (Structured query language aka si cá»“ hay Ã©t qui eo ğŸ˜‚ ). CÆ¡ báº£n thÃ¬ Ä‘Ã¢y lÃ  ngÃ´n ngá»¯ dÃ¹ng Ä‘á»ƒ truy suáº¥t dá»¯ liá»‡u, giao tiáº¿p vá»›i database, Ä‘áº·c biá»‡t lÃ  cÃ¡c RDBMS. AI, Machine learning: Khi cÃ³ má»™t lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“, má»™t data scientist cÃ³ thá»ƒ sáº½ dÃ¹ng chÃºng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y hoáº·c máº¡ng Ä‘á»ƒ giáº£i cÃ¡c bÃ i toÃ¡n há»“i quy vÃ  Ä‘Æ°a ra Ä‘Æ°á»£c cÃ¡c dá»± Ä‘oÃ¡n vá» xu hÆ°á»›ng data hay giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i. CÃ³ hiá»ƒu biáº¿t vá» cÃ¡c thuáº­t toÃ¡n mÃ¡y há»c vÃ  kiáº¿n trÃºc máº¡ng noron cÅ©ng lÃ  má»™t Ä‘iá»u cáº§n cÃ³ á»Ÿ nhÃ  khoa há»c dá»¯ liá»‡u. Äá»c Ä‘áº¿n Ä‘Ã¢y, cÃ³ thá»ƒ báº¡n sáº½ cÃ³ cáº£m giÃ¡c â€œDÃ¨jÃ  vuâ€ nháº¹ â€¦ Sao nhiá»u chá»• giá»‘ng Data Analyst tháº¿ nhá»‰ ? MÃ  thiá»‡t ra lÃ  khÃ´ng giá»‘ng Ä‘Ã¢u nhÃ©, hai ngÃ nh nÃ y chá»‰ lÃ  anh em xÃ£ há»™i vá»›i nhau mÃ  thÃ´i ğŸ˜‚ ","date":"30 Aug 2023","objectID":"/start_journey/:1:1","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#data-science-lÃ -gÃ¬-nhá»‰"},{"categories":[],"content":"Data Scientist vs Data AnalystSáºµn tiá»‡n ká»ƒ má»™t chÃºt vá» vai trÃ² cá»§a má»™t ngÆ°á»i Data Analyst. Vá» cÆ¡ báº£n, vai trÃ² cá»§a há» cÅ©ng giá»‘ng vá»›i cÃ¡c data scientist, há» cÅ©ng phÃ¢n tÃ­ch dá»¯ liá»‡u, cá»‘ gáº¯ng tÃ¬m kiáº¿m vÃ  rÃºt ra giÃ¡ trá»‹ tá»« chÃºng. NhÆ°ng sáº½ cÃ³ má»™t sá»‘ Ä‘iá»ƒm khÃ¡c biá»‡t: Data Analyst Data Science ChuyÃªn viÃªn phÃ¢n tÃ­ch dá»¯ liá»‡u NhÃ  khoa há»c dá»¯ liá»‡u Váº«n lÃ m cÃ´ng viá»‡c cá»§a DS nhÆ°ng vá»›i quy mÃ´ nhá» Tá»a sÃ¡ng vá»›i lÆ°á»£ng data khá»•ng lá»“ (BigData) KhÃ´ng cáº§n nhiá»u kiáº¿n thá»©c láº­p trÃ¬nh Cáº§n kiáº¿n thá»©c láº­p trÃ¬nh Cáº§n cÃ³ kiáº¿n thá»©c vá» hoáº¡t Ä‘á»™ng kinh doanh nhiá»u hÆ¡n vÃ  vá»¯ng vá» kiáº¿n thá»©c thá»‘ng kÃª Cáº§n cÃ³ kiáº¿n thá»©c khÃ´ng chá»‰ toÃ¡n thá»‘ng kÃª, á»©ng dá»¥ng mÃ  cÃ²n pháº£i cÃ³ kiáº¿n thá»©c vá» computer science, AI/ML,â€¦ Dá»±a vÃ o dá»¯ liá»‡u Ä‘Æ°a ra cÃ¡c giÃ¡ trá»‹ cÃ³ Ã­ch vÃ  cÃ¡i nhÃ¬n trá»±c quan vá» dá»¯ liá»‡u ÄÆ°á»£c yÃªu cáº§u phÃ¡t triá»ƒn â€œdata productâ€ Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh cÃ³ Ã­ch tá»« táº­p dá»¯ liá»‡u lá»›n Data Science and Data AnalyticSource: https://www.datascience-pm.com/wp-content/uploads/2021/05/data-scientist-vs-analyst-venn-diagram.png \" Data Science and Data Analytic Rá»“i okay nÃ£y giá» lÃ  cáº£ data science (DS) vÃ  data analytic (DA) rá»“i. Giá» lÃ  má»›i Ä‘áº¿n data engineer cá»§a tui nÃ¨ hihi ğŸ˜„ ","date":"30 Aug 2023","objectID":"/start_journey/:1:2","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#data-scientist-vs-data-analyst"},{"categories":[],"content":"Data Engineer lÃ  gÃ¬ ?Tuy há»c Data Science, nhÆ°ng thá»±c ra ngay tá»« nhá»¯ng lÃºc cÃ²n mÆ¡n má»Ÿn cáº¥p 3, OG Ä‘Ã£ tá»«ng cÃ³ Æ°á»›c muá»‘n trá»Ÿ thÃ nh má»™t láº­p trÃ¬nh viÃªn má»™t tay cafe má»™t tay chÃ©m code bÃ¬nh loáº¡n thiÃªn háº¡ ğŸ˜‚ VÃ  tháº¿ lÃ  tÃ¬m Ä‘Æ°á»£c má»™t ngÃ nh thÃ­ch há»£p Ä‘Æ°á»£c coi lÃ  â€œSoftware engineer cho dataâ€, ngÃ nh nÃ y lÃ  má»™t trong cÃ¡c ngÃ nh cÃ³ xu hÆ°á»›ng phÃ¡t triá»ƒn nhanh nháº¥t trong nhÃ³m ngÃ nh cÃ´ng nghá»‡. VÃ¢ng Ä‘Ã³ chÃ­nh lÃ  Data Engineer Data EngineerData Engineer Ä‘Æ°á»£c coi lÃ  Software Engineer á»Ÿ Data field \" Data Engineer Äáº§u tiÃªn, Data Engineer hay DE Ä‘Æ°á»£c gá»i lÃ  ká»¹ sÆ° dá»¯ liá»‡u. ÄÃ¢y lÃ  vai trÃ² Ä‘áº£m nhiá»‡m viá»‡c phÃ¢n tÃ­ch nguá»“n dá»¯ liá»‡u, xÃ¢y dá»±ng vÃ  duy trÃ¬ há»‡ thá»‘ng cÆ¡ sá»Ÿ dá»¯ liá»‡u hiá»‡u quáº£. NgoÃ i ra cÅ©ng lÃ  ngÆ°á»i Ä‘áº£m báº£o cháº¥t lÆ°á»£ng dá»¯ liá»‡u cho cÃ¡c phÃ²ng ban khÃ¡c sá»­ dá»¥ng. CÆ¡ báº£n Ä‘á»ƒ lÃ  Ä‘á»ƒ cho DS vÃ  DA lÃ m viá»‡c má»™t cÃ¡ch hiá»‡u quáº£ nháº¥t, há» cáº§n cÃ³ má»™t nguá»“n data á»•n Ä‘á»‹nh vÃ  sáº¡ch sáº½. VÃ  ngÆ°á»i Ä‘áº£m nhiá»‡m viá»‡c luÃ¢n chuyá»ƒn data Ä‘Ã³ tá»›i cho há» chÃ­nh lÃ  Data Engineer. KhÃ´ng chá»‰ cÃ³ DS vÃ  DA mÃ  data engineer phá»¥c vá»¥ cho táº¥t cáº£ cÃ¡c phÃ²ng ban khÃ¡c Data Engineer NÃ³i tÃ³m láº¡i, Data Engineer lÃ  ngÆ°á»i xÃ¢y dá»±ng cÃ¡c Ä‘Æ°á»ng á»‘ng dá»¯ liá»‡u (data pipeline) Ä‘á»ƒ truyá»n dá»¯ liá»‡u tá»« nÆ¡i nÃ y sang nÆ¡i khÃ¡c má»™t cÃ¡ch cháº¥t lÆ°á»£ng nháº¥t :)) KhÃ¡i niá»‡m cÆ¡ báº£n lÃ  tháº¿ thÃ´i, nghe cÃ³ váº» Ä‘Æ¡n giáº£n pháº£i khÃ´ng. HÃ£y tiáº¿p tá»¥c vá»›i má»¥c tiáº¿p theo Ä‘á»ƒ xem liá»‡u ta cáº§n gÃ¬ Ä‘á»ƒ trá»Ÿ thÃ nh data engineer ","date":"30 Aug 2023","objectID":"/start_journey/:2:0","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#data-engineer-lÃ -gÃ¬-"},{"categories":[],"content":"Data Engineer thÃ¬ cáº§n biáº¿t gÃ¬ ?Má»™t data Engineer vá» báº£n cháº¥t lÃ  xÃ¢y dá»±ng cÃ¡c data pipeline Ä‘á»ƒ luÃ¢n chuyá»ƒn dá»¯ liá»‡u. Äá»ƒ lÃ m tá»‘t viá»‡c Ä‘Ã³, ká»¹ sÆ° dá»¯ liá»‡u pháº£i biáº¿t: Ká»¹ nÄƒng láº­p trÃ¬nh: Táº¥t nhiÃªn rá»“i, báº¡n lÃ  má»™t nhÃ¢n viÃªn IT thÃ¬ Ä‘iá»u nÃ y lÃ  pháº£i cÃ³. CÃ¡c ngÃ´n ngá»¯ mÃ  DE thÆ°á»ng dÃ¹ng lÃ  SQL, Python vÃ  R. Há»‡ cÆ¡ sá»Ÿ dá»¯ liá»‡u quan há»‡ vÃ  phi quan há»‡: Dá»¯ liá»‡u cÃ³ ráº¥t nhiá»u dáº¡ng: Structure/Semi/Unstructure data, do Ä‘Ã³ cÅ©ng cáº§n cÃ³ nhiá»u loáº¡i database quáº£n lÃ½ chÃºng. VÃ  DE lÃ m viá»‡c ráº¥t nhiá»u vá»›i database. Há» sáº½ lÃ  ngÆ°á»i trá»±c tiáº¿p tÆ°Æ¡ng tÃ¡c ká»ƒ cáº£ vá»›i SQL vÃ  NoSQL database. ETL/ELT: ETL aka Extract Transform Load hay ELT aka Extract Load Transform lÃ  quy trÃ¬nh xá»­ lÃ½ vÃ  luÃ¢n chuyá»ƒn dá»¯ liá»‡u tá»« nguá»“n Ä‘áº¿n Ä‘Ã­ch. Má»™t DE pháº£i náº¯m Ä‘Æ°á»£c Ä‘á»ƒ thiáº¿t káº¿ data pipeline má»™t cÃ¡ch hiá»‡u quáº£ nháº¥t Data Warehouse: hay Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  kho chá»©a dá»¯ liá»‡u. Báº¡n cÃ³ thá»ƒ sáº½ pháº£i xÃ¢y dá»±ng, thiáº¿t káº¿ cáº¥u trÃºc data warehouse trÃªn cloud platform vÃ  xÃ¢y dá»±ng cÃ¡c káº¿t ná»‘i dá»¯ liá»‡u Ä‘á»ƒ tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ truy xuáº¥t vÃ  Ä‘áº£m báº£o viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u. Big Data: Báº¡n cÅ©ng cáº§n pháº£i biáº¿t cÃ¡c kiáº¿n trÃºc lÆ°u trá»¯ vÃ  xá»­ lÃ½ táº­p dá»¯ liá»‡u lá»›n nhÆ° Hadoop, Spark,â€¦ Cloud: Táº¥t nhiÃªn lÃ  pháº£i cÃ³ rá»“i, cÃ¡c cloud platform nhÆ° Google Cloud Platform, AWS, Azure,â€¦ Ä‘Ã£ ráº¥t ná»•i tiáº¿ng trong viá»‡c há»— trá»£ xÃ¢y dá»±ng vÃ  thiáº¿t káº¿ há»‡ thá»‘ng pipeline cÅ©ng nhÆ° há»— trá»£ tá»‘i Ä‘a viá»‡c xá»­ lÃ½ bigdata cÅ©ng nhÆ° deploy há»‡ thá»‘ng háº¡ táº§ng má»™t cÃ¡ch nhanh chÃ³ng. Báº¡n cÃ³ thá»ƒ sáº½ pháº£i lÃ m viá»‡c vá»›i lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ vÃ  táº­p dá»¯ liá»‡u lá»›n. VÃ  Ä‘á»ƒ xÃ¢y dá»±ng há»‡ thá»‘ng xá»­ lÃ½ Ä‘Æ°á»£c lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã³, cháº¯c cháº¯n pháº£i cÃ³ sá»± gÃ³p máº·t cá»§a cÃ¡c ná»n táº£ng Ä‘Ã¡m mÃ¢y. Wellâ€¦ NhÃ¬n chung cÅ©ng nhiá»u thá»© cáº§n pháº£i biáº¿t Ä‘áº¥y nhá»‰, táº¥t nhiÃªn Ä‘Ã³ chá»‰ lÃ  má»™t sá»‘ Ä‘iá»u quan trá»ng nháº¥t. NgoÃ i ra báº¡n cÅ©ng cáº§n pháº£i biáº¿t má»™t sá»‘ kiáº¿n thá»©c khÃ¡c vá» Unix vÃ  Linux, Docker, Git, Batch/Stream Processing,â€¦ VÃ  cÃ²n ti tá»‰ thá»© khÃ¡c mÃ  OG cÃ³ ká»ƒ Ä‘áº¿n rÄƒng long Ä‘áº§u báº¡c cÃ³ láº½ cÅ©ng chÆ°a háº¿t ğŸ˜‚ ","date":"30 Aug 2023","objectID":"/start_journey/:2:1","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#data-engineer-thÃ¬-cáº§n-biáº¿t-gÃ¬-"},{"categories":[],"content":"Táº¡m káº¿tHÃ nh trÃ¬nh nÃ o khi báº¯t Ä‘áº§u cÅ©ng gian nan, cáº£ báº£n thÃ¢n OG khi báº¯t Ä‘áº§u cÅ©ng khÃ´ng biáº¿t gÃ¬ cáº£. NhÆ°ng khi nháº¥c ngÃ³n chÃ¢n lÃªn vÃ  Ä‘i thÃ¬ má»›i cáº£m nháº­n Ä‘Æ°á»£c tháº¿ giá»›i chá»© ğŸ˜„ Hy vá»ng bÃ i viáº¿t nÃ y giÃºp báº¡n thÆ° giÃ£n vÃ  cÃ³ má»™t cÃ¡i nhÃ¬n chung vá» ngÃ nh data nhÃ©. Háº¹n gáº·p láº¡i trong bÃ i tiáº¿p theo hehe -Meww ","date":"30 Aug 2023","objectID":"/start_journey/:3:0","series":[],"tags":[],"title":"Tháº±ng nhÃ³c thÃ­ch code vÃ  data","uri":"/start_journey/#táº¡m-káº¿t"},{"categories":[],"content":"Giá»›i thiá»‡u vá» trang web Ä‘Ã¡ng yÃªu nÃ y","date":"30 Aug 2023","objectID":"/intro_blog/","series":[],"tags":[],"title":"Intro: Tui lÃ  ai ? ÄÃ¢y lÃ  Ä‘Ã¢u ?","uri":"/intro_blog/"},{"categories":[],"content":"Hello! Hello! mÃ¬nh lÃ  OG Ä‘Ã¢y. ÄÃ¢y lÃ  bÃ i blog Ä‘áº§u tiÃªn cá»§a mÃ¬nh á»Ÿ Ä‘Ã¢y. CÃ³ thá»ƒ báº¡n Ä‘ang tá»± há»i ráº±ng mÃ¬nh lÃ  ai vÃ  Ä‘Ã¢y lÃ  nÆ¡i nÃ o Ä‘Ãºng khÃ´ng, váº­y hÃ£y cÃ¹ng mÃ¬nh tÃ¬m hiá»ƒu thá»­ nhÃ© ","date":"30 Aug 2023","objectID":"/intro_blog/:0:0","series":[],"tags":[],"title":"Intro: Tui lÃ  ai ? ÄÃ¢y lÃ  Ä‘Ã¢u ?","uri":"/intro_blog/#"},{"categories":[],"content":"OG lÃ  ai ?Má»™t láº§n ná»¯a giá»›i thiá»‡u mÃ¬nh tÃªn lÃ  VÄ©nh Phong - má»™t anh chÃ ng thÃ­ch tÃ¬m tÃ²i Ä‘iá»u má»›iâ€¦ hmmm tháº¿ thÃ´i :))) Äá»ƒ hiá»ƒu thÃªm vá» mÃ¬nh: About CÃ²n OG (nickname) lÃ  biá»‡t danh mÃ¬nh láº¥y cáº£m há»©ng tá»« má»™t nhÃ¢t váº­t hoáº¡t hÃ¬nh ráº¥t hÃ³m há»‰nh Ä‘áº¥y nhÃ© hehe. ÄÃ³ lÃ  tÃªn má»™t chÃº mÃ¨o xanh dÆ°Æ¡ng hay bá»‹ máº¥y con giÃ¡n quáº­y :)) báº¡n thá»­ Ä‘oÃ¡n xem ÄÃ¡p Ã¡n á»¦a lá»™n nÃ y lÃ  tui:))Tui \" á»¦a lá»™n nÃ y lÃ  tui:)) ÄÃ¢y má»›i lÃ  Ä‘Ã¡p Ã¡n Náº¿u báº¡n hong biáº¿t, thÃ¬ con mÃ¨o xanh lÃ¨ nÃ y tÃªn lÃ  Oggy vÃ  nickname mÃ¬nh cÅ©ng váº­y :)) Biáº¿t mÃ¬nh lÃ  ai rá»“i, tháº¿ thÃ¬â€¦ ","date":"30 Aug 2023","objectID":"/intro_blog/:1:0","series":[],"tags":[],"title":"Intro: Tui lÃ  ai ? ÄÃ¢y lÃ  Ä‘Ã¢u ?","uri":"/intro_blog/#OG-la-ai"},{"categories":[],"content":"ÄÃ¢y lÃ  nÆ¡i nÃ o Ä‘Ã¢y ?ÄÃ¢y trang mÃ  mÃ¬nh Ä‘Äƒng lÃªn cÃ¡c bÃ i Blogs vá» cÃ´ng nghá»‡, vá» ngÃ nh Data nÃ³i chung vÃ  vá» hÃ nh trÃ¬nh há»c táº­p cá»§a OG Ä‘á»ƒ trá»Ÿ thÃ nh má»™t Data Engineer trong tÆ°Æ¡ng lai. Táº¥t nhiÃªn khÃ´ng chá»‰ nhÆ° váº­y MÃ¬nh cÅ©ng viáº¿t blogs vá» Ä‘á»i sá»‘ng, vá» nhá»¯ng Ä‘iá»u thÃº vá»‹ cá»§a cuá»™c sá»‘ng xung quanh VÃ  mÃ¬nh hy vá»ng trang cÅ©ng nÃ y sáº½ lÃ  nÆ¡i mang láº¡i sá»± thoáº£i mÃ¡i vÃ  thÆ° giáº£n cho má»i ngÆ°á»i ","date":"30 Aug 2023","objectID":"/intro_blog/:2:0","series":[],"tags":[],"title":"Intro: Tui lÃ  ai ? ÄÃ¢y lÃ  Ä‘Ã¢u ?","uri":"/intro_blog/#day-la-dau"},{"categories":[],"content":"Tháº¿ á»Ÿ Ä‘Ã¢y cÃ³ gÃ¬ hay?TÃ³m táº¯t cÃ¡c trang: Blogs: báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y cÃ¡c bÃ i blogs mÃ¬nh viáº¿t á»Ÿ Ä‘Ã¢y Projects: Nhá»¯ng dá»± Ã¡n mÃ¬nh Ä‘Ã£ lÃ m About: Náº¿u báº¡n muá»‘n hiá»ƒu thÃªm vá» mÃ¬nh ÄÃ³ lÃ  tá»•ng quan â€œcÃ¡c thá»© cÃ³ thá»ƒ sáº½ xuáº¥t hiá»‡nâ€ á»Ÿ trang web nÃ y. ","date":"30 Aug 2023","objectID":"/intro_blog/:2:1","series":[],"tags":[],"title":"Intro: Tui lÃ  ai ? ÄÃ¢y lÃ  Ä‘Ã¢u ?","uri":"/intro_blog/#tháº¿-á»Ÿ-Ä‘Ã¢y-cÃ³-gÃ¬-hay"},{"categories":[],"content":"Cuá»‘i cÃ¹ngOG hy vá»ng Ä‘Ã¢y sáº½ lÃ  nÆ¡i giÃºp báº¡n thÆ° giÃ£n hay há»c há»i Ä‘Æ°á»£c nhiá»u Ä‘iá»u thÃº vá»‹ nhÃ© ğŸ˜„ -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huá»³nh LÆ°u VÄ©nh Phong Facebook: Phong Huynh Instagram: phong_huynh Hoáº·c báº¡n cÅ©ng cÃ³ thá»ƒ ghÃ© thÄƒm Github cá»§a mÃ¬nh: PhongHuynh0394 ","date":"30 Aug 2023","objectID":"/intro_blog/:3:0","series":[],"tags":[],"title":"Intro: Tui lÃ  ai ? ÄÃ¢y lÃ  Ä‘Ã¢u ?","uri":"/intro_blog/#cuoi-cung"},{"categories":null,"content":"Continuous of Football ETL series","date":"01 Aug 2023","objectID":"/football_etl_2/","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/"},{"categories":null,"content":"Hello! Hello! OG Ä‘Ã¢y, sau pháº§n 1 chÃºng ta Ä‘Ã£ setup cÃ¡c kiá»ƒu vÃ  Ä‘áº£m báº£o má»i thá»© trÆ¡n tru rá»“i, á»Ÿ pháº§n nÃ y chÃºng ta sáº½ chuáº©n bá»‹ Data Source, vÃ  khá»Ÿi cháº¡y pipeline á»Ÿ Implement sau Ä‘Ã³ sáº½ Visualize cleaned data cÃ³ Ä‘Æ°á»£c tá»« data warehouse thÃ nh Dashboard. Báº¯t Ä‘áº§u thÃ´i nÃ o ! ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#"},{"categories":null,"content":"Data Source","date":"01 Aug 2023","objectID":"/football_etl_2/:1:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#data-source"},{"categories":null,"content":"Chuáº©n bá»‹ file lÃ m raw dataCÃ¡c file csv sá»­ dá»¥ng lÃ m dá»¯ liá»‡u Ä‘Æ°á»£c táº£i tá»« Football Database - Kaggle. ÄÃ¢y lÃ  dá»¯ liá»‡u thá»‘ng kÃª cá»§a cáº§u thá»§, Ä‘á»™i bÃ³ng Ä‘áº¿n tá»« 5 giáº£i bÃ³ng hÃ ng Ä‘áº§u ChÃ¢u Ã‚u (Premier League, Laliga, Seria A, Budesliga, League 1) Ta sáº½ cÃ³ schema nhÆ° sau: SchemaSchema \" Schema trong Ä‘Ã³: games: báº£ng chá»©a thÃ´ng tin thá»‘ng kÃª cá»§a tá»«ng tráº­n Ä‘áº¥u (gameID) teams: Báº£ng chá»©a tÃªn cÃ¡c Ä‘á»™i bÃ³ng (teamID) players: Báº£ng chá»©a tÃªn cÃ¡c cáº§u thá»§ (playerID) leagues: Báº£ng chÆ°a tÃªn cÃ¡c giáº£i Ä‘áº¥u (leagueID) appearances: Báº£ng thá»‘ng kÃª cá»§a cáº§u thá»§ á»Ÿ cÃ¡c game mÃ  há» tham gia (gameID, playerID) teamstats: Báº£ng thá»‘ng kÃª cá»§a Ä‘á»™i bÃ³ng á»Ÿ tá»«ng game (gameID, teamID) ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#chuáº©n-bá»‹-file-lÃ m-raw-data"},{"categories":null,"content":"Load data vÃ o MySQLCÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ load data vÃ o MySQL, á»Ÿ Ä‘Ã¢y mÃ¬nh sáº½ sá»­ dá»¥ng cÃ¡ch LOAD LOCAL_INFILE cá»§a MySQL luÃ´n. Tip HÃ£y Ä‘áº£m báº£o báº¡n Ä‘Ã£ make up láº§n Ä‘áº§u rá»“i nhÃ© ! HÃ£y copy folder chá»©a cÃ¡c file csv vÃ o de_mysql container: docker cp /football de_mysql:/tmp/dataset/ docker cp /load_data de_mysql:/tmp/dataset/ Sau Ä‘Ã³ táº¡o báº£ng trá»‘ng sáºµn trong MySQL: make mysql_create #Create table in mysql Tiáº¿p tá»¥c vá»›i lá»‡nh: make to_mysql_root # ----- You will access to MySQL container SET GLOBAL LOCAL_INFILE=TRUE; #Set local_infile variable to load data from local exit; # ----- Exit container make mysql_load #load data make mysql_create_relation #create table relation Tháº¿ lÃ  Ä‘Ã£ chuáº©n bá»‹ xong dá»¯ liá»‡u cho MySQL. ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load-data-vÃ o-mysql"},{"categories":null,"content":"Init PostgreSQL SchemaTa cÅ©ng cáº§n pháº£i táº¡o sáºµn schema sáºµn trong Posgres nhÆ° sau: make to_psql CREATE SCHEMA IF NOT EXISTS analysis; exit; Tháº¿ lÃ  Ä‘Ã£ hoÃ n táº¥t viá»‡c chuáº©n bá»‹ data, giá» thÃ¬ ta báº¯t Ä‘áº§u vÃ o pháº§n viá»‡c chÃ­nh thÃ´i ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#init-postgresql-schema"},{"categories":null,"content":"ImplementCÃ´ng viá»‡c chÃ­nh trong pháº§n nÃ y lÃ  xÃ¢y dá»±ng cÃ¡c data pipeline báº±ng dagster. CÆ¡ báº£n cÃ³ thá»ƒ hiá»ƒu lÃ  ta táº¡o cÃ¡c Asset vÃ  chuyá»ƒn chÃºng tá»« database nÃ y sang database khÃ¡c. ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#implement"},{"categories":null,"content":"ExtractionÄá»ƒ cÃ³ thá»ƒ quáº£n lÃ½ viá»‡c truy xuáº¥t dá»¯ liá»‡u tá»« MySQL vÃ  load vÃ o MinIO Ä‘á»ƒ lÆ°u táº¡m, ta sáº½ xÃ¢y dá»±ng má»™t I/O Manager phá»¥c vá»¥ viá»‡c Ä‘Ã³. Äáº§u tiÃªn, hÃ£y vÃ o Ä‘Æ°á»ng dáº«n: ./etl_pipeline/etl_pipeline/resources/ Ta sáº½ xÃ¢y dá»±ng MySQL io manager báº±ng cÃ¡ch táº¡o file mysql_io_manager.py vá»›i ná»™i dung sau: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_mysql(config): conn_info = ( f\"mysql+pymysql://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class MySQLIOManager(IOManager): def __init__(self, config): self.config = config def handle_output(self, context: OutputContext, obj: pd.DataFrame): pass def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def extract_data(self, sql: str) -\u003e pd.DataFrame: with connect_mysql(self.config) as db_conn: pd_data = pd.read_sql_query(sql, db_conn) return pd_data Sau Ä‘Ã³, tiáº¿p tá»¥c Ä‘á»‘i vá»›i minio_io_manager.py: import os from contextlib import contextmanager from datetime import datetime from typing import Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from dagster import IOManager, InputContext, OutputContext from minio import Minio @contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\"endpoint_url\"), access_key=config.get(\"aws_access_key_id\"), secret_key=config.get(\"aws_secret_access_key\"), secure=False ) try: yield client except Exception: raise class MinIOIOManager(IOManager): def __init__(self, config): self._config= config def _get_path(self, context: Union[InputContext, OutputContext]): layer, schema, table = context.asset_key.path key = \"/\".join([layer, schema, table.replace(f\"{layer}_\", \"\")]) tmp_file_path = \"/tmp/file-{}-{}.parquet\".format( datetime.today().strftime(\"%Y%m%d%H%M%S\"), \"-\".join(context.asset_key.path) ) if context.has_asset_partitions: start, end = context.asset_partitions_time_window dt_format = \"%Y%m%d%H%M%S\" partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format) return os.path.join(key, f\"{partition_str}.pq\"), tmp_file_path else: return f\"{key}.pq\", tmp_file_path def handle_output(self, context: OutputContext, obj: pd.DataFrame): # convert to parquet format key_name, tmp_file_path = self._get_path(context) table = pa.Table.from_pandas(obj) pq.write_table(table, tmp_file_path) # upload to MinIO try: bucket_name = self._config.get(\"bucket\") with connect_minio(self._config) as client: # Make bucket if not exist. found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exists\") client.fput_object(bucket_name, key_name, tmp_file_path) row_count = len(obj) context.add_output_metadata({\"path\": key_name, \"tmp\": tmp_file_path}) # clean up tmp file os.remove(tmp_file_path) except Exception: raise def load_input(self, context: InputContext) -\u003e pd.DataFrame: bucket_name = self._config.get(\"bucket\") key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: #Make bucket if not exist found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exist\") client.fget_object(bucket_name, key_name, tmp_file_path) pd_data = pd.read_parquet(tmp_file_path) return pd_data except Exception: raise Sau khi Ä‘Ã£ táº¡o thÃ nh cÃ´ng cÃ¡c io manager cho mysql vÃ  minio, ta sáº½ báº¯t Ä‘áº§u xÃ¢y dá»±ng bronze layer Note nho nhá» Trong project nÃ y mÃ¬nh chia cÃ¡c giai Ä‘oáº¡n transformation thÃ nh cÃ¡c layer: bronze layer: Giai Ä‘oáº¡n chá» má»›i load raw data, cÃ³ thá»ƒ hiá»ƒu Ä‘Ã¢y lÃ  data chÆ°a transform gÃ¬ cáº£ siler layer: Transform má»™t pháº§n tá»« bronze layer, á»Ÿ Ä‘oáº¡n nÃ y data Ä‘Ã£ Ä‘Æ°á»£c cleaning sÆ¡ gold layer: Sau khi transform má»™t láº§n ná»¯a tá»« silver layer, giai Ä‘oáº¡n nÃ y sáº½ truy váº¥n ra cÃ¡c thÃ´n","date":"01 Aug 2023","objectID":"/football_etl_2/:2:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#extraction"},{"categories":null,"content":"TransformationTiáº¿p tá»¥c táº¡o file silver_layer.py cÃ¹ng folder vá»›i bronze layer: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teamstats\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"leagues\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], description='Statistic of teams in games', group_name=\"Silver_layer\", compute_kind=\"Pandas\" ) def silver_statsTeamOnGames(teamstats: pd.DataFrame, games: pd.DataFrame, leagues: pd.DataFrame) -\u003e Output[pd.DataFrame]: ts = teamstats.copy() gs = games.copy() lgs = leagues.copy() #Drop unsusable columns in games gs.drop(columns=gs.columns.to_list()[13:], inplace=True) #create StatperLeagueSeason result = pd.merge(ts, gs, on=\"gameID\") result = result.merge(lgs, on=\"leagueID\", how=\"left\") result.drop(columns=['season_y', 'date_y'],inplace=True) result = result.rename(columns={'season_x': 'season', 'date_x': 'date'}) return Output( result, metadata={ \"table\": \"statsTeamOnGames\", \"records\": len(result) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"appearances\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"players\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='statistic of players in games', compute_kind=\"Pandas\" ) def silver_playerAppearances(appearances: pd.DataFrame, games: pd.DataFrame, players: pd.DataFrame) -\u003e Output[pd.DataFrame]: app = appearances.copy() ga = games.copy() pla = players.copy() #Drop unusable column ga.drop(columns=ga.columns.to_list()[13:], inplace=True) #Merge player_appearances = pd.merge(app, pla, on=\"playerID\", how=\"left\") player_appearances = pd.merge(player_appearances, ga, on=\"gameID\", how=\"left\") #drop unecessary columns and rename player_appearances.drop(columns=['leagueID_y'],inplace=True) player_appearances.rename(columns={'leagueID_x': 'leagueID'}, inplace=True) return Output( player_appearances, metadata={ \"table\": \"playerAppearances\", \"records\": len(player_appearances) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teams\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='Teams', compute_kind=\"Pandas\" ) def silver_teams(teams: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( teams, metadata={ \"table\": 'teams', 'records': len(teams) } ) LÃºc nÃ y mÃ¬nh cÃ³ 3 silver assets, Ä‘Æ°á»£c join tá»« cÃ¡c báº£ng á»Ÿ bronze Tiáº¿p Ä‘áº¿n lÃ  gold_layer, lÃºc nÃ y ta sáº½ tÃ­nh cÃ¡c thÃ´ng sá»‘ thá»‘ng kÃª cá»§a tá»«ng giáº£i Ä‘Ã¢u tá»«ng mÃ¹a, cÃ¡c thá»‘ng kÃª cá»§a cáº§u thá»§ trong 90 phÃºt thi Ä‘áº¥u, vÃ  cáº£ thá»‘ng kÃª cá»§a tá»«ng cáº§u thá»§ trong tá»«ng mÃ¹a giáº£i gold_layer.py sáº½ cÃ³ ná»™i dung: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_statsTeamOnGames\": AssetIn( key_prefix=[\"football\", \"silver\"] ) }, group_name=\"Gold_layer\", key_prefix=[\"football\", \"gold\"], description='Statistic of all league in each season', compute_kind=\"Pandas\" ) def gold_statsPerLeagueSeason(silver_statsTeamOnGames: pd.DataFrame) -\u003e Output[pd.DataFrame]: st = silver_statsTeamOnGames.copy() result = ( st.groupby(['name', 'season']) .agg({\"goals\": \"sum\", \"xGoals\": \"sum\", \"shots\": \"sum\", \"shotsOnTarget\": \"sum\", \"fouls\": \"sum\", \"yellowCards\": \"sum\", \"redCards\": \"sum\",'corners': 'sum', \"gameID\": 'count'}) .reset_index() ) result = result.rename(columns={'gameID':\"games\"}) result['goalPerGame']= result.goals/result.games result['season'] = result['season'].astype('string') return Output( result, metadata={ 'table': 'statPerLeagueSeason', 'records': len(result) } ) @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_playerAppearances\": AssetIn( key_prefix=[\"football\", \"silver\"] ) },","date":"01 Aug 2023","objectID":"/football_etl_2/:2:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#transformation"},{"categories":null,"content":"LoadTrÆ°á»›c háº¿t hÃ£y táº¡o IO Manager cho Postgres Ä‘á»ƒ quáº£n lÃ½ viá»‡c load cleaned data. Ta táº¡o file psql_io_manager.py á»Ÿ vá»‹ trÃ­ mÃ  ta Ä‘Ã£ táº¡o 2 io manager trÆ°á»›c vá»›i ná»™i dung: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_psql(config): conn_info = ( f\"postgresql+psycopg2://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class PostgreSQLIOManager(IOManager): def __init__(self, config): self._config = config def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def handle_output(self, context: OutputContext, obj: pd.DataFrame): schema, table = context.asset_key.path[-2], context.asset_key.path[-1] with connect_psql(self._config) as conn: # insert new data ls_columns = (context.metadata or {}).get(\"columns\", []) obj[ls_columns].to_sql( name=f\"{table}\", con=conn, schema=schema, if_exists=\"replace\", index=False, chunksize=10000, method=\"multi\" ) Sau Ä‘Ã³, táº¡o má»™t asset warehouse_layer.py: from dagster import multi_asset, Output, AssetIn, AssetOut, asset import pandas as pd @multi_asset( ins={ \"gold_statsPerLeagueSeason\": AssetIn( key_prefix=[\"football\", \"gold\"] ) }, outs={ \"statsperleagueseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerLeagueSeason\", 'analysis'], metadata={ \"columns\": [ \"name\", \"season\", \"goals\", \"xGoals\", \"shots\", \"shotsOnTarget\", \"fouls\", \"yellowCards\", \"redCards\", \"corners\", \"games\", \"goalPerGame\" ] } ), }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerLeagueSeason(gold_statsPerLeagueSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerLeagueSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerLeagueSeason\", \"records\": len(gold_statsPerLeagueSeason) } ) @multi_asset( ins={ \"gold_statsPerPlayerSeason\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsperplayerseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerPlayerSeason\", 'analysis'], metadata={ \"columns\": [ \"playerID\", \"name\", \"season\", \"goals\", \"shots\", \"xGoals\", \"xGoalsChain\", \"xGoalsBuildup\", \"assists\", \"keyPasses\", \"xAssists\", \"gDiff\", \"gDiffRatio\" ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerPlayerSeason(gold_statsPerPlayerSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerPlayerSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerPlayerSeason\", \"records\": len(gold_statsPerPlayerSeason) } ) @multi_asset( ins={ \"gold_statsPlayerPer90\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsplayerper90\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPlayerPer90\", 'analysis'], metadata={ \"columns\": [ 'playerID', 'name', 'total_goals', 'total_assists', 'total_time', 'goalsPer90', 'assistsPer90', 'scorers' ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPlayerPer90(gold_statsPlayerPer90: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPlayerPer90, metadata={ \"schema\": \"analysis\", \"table\": \"statsPlayerPer90\", \"records\": len(gold_statsPlayerPer90) } ) ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load"},{"categories":null,"content":"Run systemCuá»‘i cÃ¹ng, ta sáº½ káº¿t há»£p táº¥t cáº£ cÃ¡c asset láº¡i giÃºp dagster nháº­n diá»‡n vÃ  quáº£n lÃ½ vá»›i file __init__.py á»Ÿ etl_pipeline/etl_pipeline/__init__.py import os from dagster import Definitions from .assets.silver_layer import * from .assets.gold_layer import * from .assets.bronze_layer import * from .assets.warehouse_layer import * from .resources.minio_io_manager import MinIOIOManager from .resources.mysql_io_manager import MySQLIOManager from .resources.psql_io_manager import PostgreSQLIOManager MYSQL_CONFIG = { \"host\": os.getenv(\"MYSQL_HOST\"), \"port\": os.getenv(\"MYSQL_PORT\"), \"database\": os.getenv(\"MYSQL_DATABASE\"), \"user\": os.getenv(\"MYSQL_USER\"), \"password\": os.getenv(\"MYSQL_PASSWORD\") } MINIO_CONFIG = { \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\"), \"bucket\": os.getenv(\"DATALAKE_BUCKET\"), \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"), \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\") } PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } ls_asset=[asset_factory(table) for table in tables] + [silver_statsTeamOnGames, silver_teams , silver_playerAppearances, gold_statsPerLeagueSeason, gold_statsPerPlayerSeason, gold_statsPlayerPer90, statsPerLeagueSeason, statsPerPlayerSeason, statsPlayerPer90] defs = Definitions( assets=ls_asset, resources={ \"mysql_io_manager\": MySQLIOManager(MYSQL_CONFIG), \"minio_io_manager\": MinIOIOManager(MINIO_CONFIG), \"psql_io_manager\": PostgreSQLIOManager(PSQL_CONFIG), } ) sau Ä‘Ã³ hÃ£y dÃ¹ng lá»‡nh sau Ä‘á»ƒ cáº­p nháº­t cÃ¡c assets docker restart etl_pipeline ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:4","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#run-system"},{"categories":null,"content":"Check UIHÃ£y kiá»ƒm tra Dagit UI á»Ÿ localhost:3001 Ä‘á»ƒ cháº¯c cháº¯n ráº±ng má»i thá»© váº«n á»•n NgoÃ i ra cÅ©ng cÃ³ thá»ƒ check MinIO: localhost:9000 ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:5","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#check-ui"},{"categories":null,"content":"VisualizationCuá»‘i cÃ¹ng lÃ  váº½ dashboard, Ä‘áº§u tiÃªn ta cáº§n pháº£i láº¥y Ä‘Æ°á»£c data tá»« psql, hÃ£y vÃ o táº¡o file: ./streamlit/src/psql_connect.py: import os import psycopg2 from dotenv import load_dotenv import pandas as pd #load environment load_dotenv() #list table in database table = ['statsperleagueseason','statsperplayerseason', 'statsplayerper90'] PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } #create connection def init_connection(config): return psycopg2.connect( database=config['database'], user=config['user'], password=config['password'], host=config['host'], port=config['port'] ) def extract_data(): conn = init_connection(PSQL_CONFIG) return [pd.read_sql(f'SELECT * FROM analysis.{tab}', conn) for tab in table] Cuá»‘i cÃ¹ng lÃ  táº¡o main.py ngay trong thÆ° má»¥c scr: import streamlit as st import pandas as pd import plotly.express as px import plotly.graph_objects as go from psql_connect import extract_data import numpy as np # #extract data from PostgreSQL ls_df = extract_data() l_season = ls_df[0] p_season = ls_df[1] p_match = ls_df[2] st.set_page_config(page_title = 'Dashboard Football', layout='wide', page_icon='chart_with_upwards_trend') #Overview def overview(table: pd.DataFrame, detail: str): if (st.checkbox('Do you want to see Data ?')): table col1, col2 = st.columns(2) co_df = table.columns.to_list() with col1: st.bar_chart(table.describe()) if (st.checkbox('Do you want to see describe each column ?')): for col in co_df: if table[col].dtypes not in ['int64', 'float64']: continue st.bar_chart(table[col].describe()) with col2: st.caption(f':red[Columns]: {len(co_df)}') st.caption(f':red[Records]: {len(table)}') st.caption(f':red[Description]: {detail}') st.caption(f':red[Columns name]:{co_df}') #league statistic def statleague(): Cards = l_season[['name','season','yellowCards', 'redCards', 'fouls']] #Card_fouls col1, col2 = st.columns(2) with col1: #Goals per games fig = px.bar(l_season, x=\"name\", y=\"goalPerGame\", color=\"name\", barmode=\"stack\", facet_col=\"season\", labels={\"name\": \"League\", \"goals/games\": \"GPG\"}) fig.update_layout(showlegend=False, title='Goals per Game') st.plotly_chart(fig) #fouls fig = px.line(Cards, x='season', y='fouls', color='name') fig.update_layout(title='Fouls of leagues', xaxis_title='Season', yaxis_title='Fouls', legend_title='League') st.plotly_chart(fig) with col2: #Red card fig = px.line(Cards, x='season', y='redCards', color='name') fig.update_layout(title='Red Cards of leagues', xaxis_title='Season', yaxis_title='RedCards', legend_title='League') st.plotly_chart(fig) #yellow card fig = px.line(Cards, x='season', y='yellowCards', color='name') fig.update_layout(title='Yellow Cards of leagues', xaxis_title='Season', yaxis_title='YellowCards', legend_title='League') st.plotly_chart(fig) #Player statistic def statplayer(): col1, col2 = st.columns(2) with col1: #Best offensive player top_player90= p_match[(p_match['goalsPer90'] \u003e 0.8) | (p_match['assistsPer90'] \u003e 0.4)] fig = px.scatter(p_match[['name','goalsPer90', 'assistsPer90']], x='goalsPer90', y='assistsPer90', hover_name='name') fig.add_trace( go.Scatter(x=top_player90['goalsPer90'], y=top_player90['assistsPer90'], mode='markers+text', marker_size=5, text=top_player90['name'], textposition='bottom center', textfont=dict(size=15)) ) fig.update_layout(title='Best offensive Players (2018-2020)', xaxis_title='Goals Per 90min', yaxis_title='Assists Per 90min') st.plotly_chart(fig) #goals-xgoal fig = px.scatter(p_season, x=\"xGoals\", y=\"goals\", color=(p_season['xGoals'] - p_season['goals'] \u003c 10), color_discrete_sequence=[\"red\", \"green\"], opacity=0.5) fig.update_layout(title=\"Goals (G) and Expected Goals (xG)\", xaxis_title=\"xG\", yaxis_title=\"G\", ) st.plotly_chart(fig) with col2: #Top score player topPlayer = p_season.groupby(['name']).agg({'goals': 'sum'}).sort_values('goals', ascending=False).res","date":"01 Aug 2023","objectID":"/football_etl_2/:3:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#visualization"},{"categories":null,"content":"ConclusionCuá»‘i cÃ¹ng cÅ©ng Ä‘Ã£ xong má»™t project xÃ¢y dá»±ng data pipeline cÆ¡ báº£n, trong lÃºc thá»±c hiá»‡n cháº¯c cháº¯n sáº½ cÃ³ cáº£ táº¥n lá»—i xáº£y ra, nhÆ°ng OG tin lÃ  má»i gian khÃ³ Ä‘á»u sáº½ vÆ°á»£t quan Ä‘Æ°á»£c, thá»© Ä‘á»ng láº¡i chÃ­nh lÃ  kiáº¿n thá»©c vÃ  ká»¹ nÄƒng cá»§a chÃºng ta. ChÃºc báº¡n thÃ nh cÃ´ng vÃ  Ä‘Ã³n xem tiáº¿p cÃ¡c dá»± Ã¡n tiáº¿p theo cá»§a OG nhÃ© ! -Mew- ","date":"01 Aug 2023","objectID":"/football_etl_2/:4:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#conclusion"},{"categories":null,"content":"Related Content Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#related-content"},{"categories":["projects"],"content":"A Data Engineer project building pipeline to analyze football data","date":"31 Jul 2023","objectID":"/football_etl/","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/"},{"categories":["projects"],"content":"Source code: Football_ETL_Analysis ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#"},{"categories":["projects"],"content":"IntroduceTrong project nÃ y, OG sáº½ build end-to-end ETL data pipeline hoÃ n chá»‰nh Ä‘á»ƒ phÃ¢n tÃ­ch football data tá»« Kaggle vá»›i data pipeline nhÆ° sau: Data pipelinedata pipeline \" Data pipeline CÃ¡c bÆ°á»›c cá»¥ thá»ƒ: Set up: DÃ¹ng Docker táº¡o container vÃ  cÃ¡c images cáº§n thiáº¿t cho pipeline, trong Ä‘Ã³ cÃ³ cáº£ Dagster dÃ¹ng xÃ¢y dá»±ng pipeline. Chuáº©n bá»‹ data source: load cÃ¡c file csv (cÃ³ Ä‘Æ°á»£c tá»« Kaggle) vÃ o MySQL nháº±m má»¥c Ä‘Ã­ch lÆ°u trá»¯ raw data (mÃ´ phá»ng source data) Extract: Láº¥y data tá»« MySQL vÃ  load vÃ o MinIO chuáº©n bá»‹ cho bÆ°á»›c transform Transform: Sá»­ dá»¥ng Pandas Ä‘á»ƒ truy váº¥n cÃ¡c file tá»« MinIO Load: Cleaned vÃ  transformed data Ä‘Æ°á»£c load vÃ o warehouse PostgreSQL Visualization: Sá»­ dá»¥ng Streamlit Ä‘á»ƒ lÃ m Dashboard ","date":"31 Jul 2023","objectID":"/football_etl/:1:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#introduce"},{"categories":["projects"],"content":"Set upBáº¯t Ä‘áº§u vá»›i Docker, ta sáº½ xÃ¢y dá»±ng láº§n lÆ°á»£t tá»«ng image báº±ng cÃ¡ch viáº¿t docker-compose.yml Tip nho nhá» HÃ£y pull/build láº§n lÆ°á»£t tá»«ng loáº¡i framework láº§n lÆ°á»£t Ä‘á»ƒ cháº¯c cháº¯n ráº±ng chÃºng hoáº¡t Ä‘á»™ng trÆ¡n tru nháº¥t trÆ°á»›c Hoáº·c báº¡n cÅ©ng cÃ³ thá»ƒ xem luÃ´n pháº§n hoÃ n chá»‰nh HoÃ n chá»‰nh set up ","date":"31 Jul 2023","objectID":"/football_etl/:2:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#set-up"},{"categories":["projects"],"content":"MinIOMinIO lÃ  má»™t server lÆ°u trá»¯ Ä‘á»‘i tÆ°á»£ng dáº¡ng phÃ¢n tÃ¡n vá»›i hiá»‡u nÄƒng cao vÃ  cung cáº¥p cÃ¡c api giá»‘ng vá»›i Amazon S3, ta cÃ³ thá»ƒ upload, download file,â€¦ má»™t cÃ¡ch Ä‘Æ¡n giáº£n. minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_network Note Vá» .env file, Ä‘Ã¢y lÃ  file chá»©a thÃ´ng tin cÃ¡c biáº¿n mÃ´i trÆ°á»ng thiáº¿t láº­p cho tá»«ng image, mÃ¬nh sáº½ nÃ³i sau ","date":"31 Jul 2023","objectID":"/football_etl/:2:1","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#minio"},{"categories":["projects"],"content":"MySQLMySQL lÃ  má»™t trong sá»‘ cÃ¡c pháº§n má»m RDBMS (Relational DataBase Management Systems) phá»• biáº¿n nháº¥t, ta sáº½ sá»­ dá»¥ng database nÃ y Ä‘á»ƒ lÆ°u raw data mÃ´ phá»ng cho source data cáº§n ingest de_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:2","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#mysql"},{"categories":["projects"],"content":"PostgeSQLPostgreSQL lÃ  má»™t há»‡ thá»‘ng quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u quan há»‡-Ä‘á»‘i tÆ°á»£ng (object-relational database management system), vÃ  ta sáº½ dung nÃ³ lÃ m data warehouse cho project láº§n nÃ y. de_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:3","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#postgesql"},{"categories":["projects"],"content":"DagsterDagster lÃ  má»™t cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ há»— trá»£ Orchestrate Task (quáº£n lÃ½, tá»• chá»©c, Ä‘iá»u phá»‘i cÃ¡c tÃ¡c vá»¥ vÃ  cÃ´ng viá»‡c), há»— trá»£ giÃºp xÃ¢y dá»±ng data pipeline khÃ¡ tá»‘t. MÃ¬nh tháº¥y cÃ´ng cá»¥ nÃ y gá»n nháº¹ hÆ¡n Apache Airflow vÃ  cÅ©ng Ä‘Æ¡n giáº£n hÆ¡n. Äáº§u tiÃªn hÃ£y viáº¿t trong docker-compose.yml vá» dagster nhÆ° sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Äá»‘i vá»›i Dagster, ta sáº½ tá»± config báº±ng Dockerfile, hÃ£y táº¡o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau Ä‘Ã³ táº¡o Dockerfile cÃ³ ná»™i dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt vÃ  requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit lÃ  giao diá»‡n UI cá»§a Dagster, vÃ  ta sáº½ thao tÃ¡c trÃªn Dagit Ä‘á»ƒ quáº£n lÃ½ cÃ¡c assets, jobs,â€¦ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau Ä‘Ã³ hÃ£y táº¡o 1 folder ./dagster_home, trong Ä‘Ã³ táº¡o 2 file config cho workspace cá»§a dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage vÃ  workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonÄá»ƒ quáº£n lÃ½ cÃ¡c Schedules, sensors,â€¦ ta cáº§n pháº£i cÃ³ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster"},{"categories":["projects"],"content":"DagsterDagster lÃ  má»™t cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ há»— trá»£ Orchestrate Task (quáº£n lÃ½, tá»• chá»©c, Ä‘iá»u phá»‘i cÃ¡c tÃ¡c vá»¥ vÃ  cÃ´ng viá»‡c), há»— trá»£ giÃºp xÃ¢y dá»±ng data pipeline khÃ¡ tá»‘t. MÃ¬nh tháº¥y cÃ´ng cá»¥ nÃ y gá»n nháº¹ hÆ¡n Apache Airflow vÃ  cÅ©ng Ä‘Æ¡n giáº£n hÆ¡n. Äáº§u tiÃªn hÃ£y viáº¿t trong docker-compose.yml vá» dagster nhÆ° sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Äá»‘i vá»›i Dagster, ta sáº½ tá»± config báº±ng Dockerfile, hÃ£y táº¡o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau Ä‘Ã³ táº¡o Dockerfile cÃ³ ná»™i dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt vÃ  requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit lÃ  giao diá»‡n UI cá»§a Dagster, vÃ  ta sáº½ thao tÃ¡c trÃªn Dagit Ä‘á»ƒ quáº£n lÃ½ cÃ¡c assets, jobs,â€¦ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau Ä‘Ã³ hÃ£y táº¡o 1 folder ./dagster_home, trong Ä‘Ã³ táº¡o 2 file config cho workspace cá»§a dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage vÃ  workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonÄá»ƒ quáº£n lÃ½ cÃ¡c Schedules, sensors,â€¦ ta cáº§n pháº£i cÃ³ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagit"},{"categories":["projects"],"content":"DagsterDagster lÃ  má»™t cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ há»— trá»£ Orchestrate Task (quáº£n lÃ½, tá»• chá»©c, Ä‘iá»u phá»‘i cÃ¡c tÃ¡c vá»¥ vÃ  cÃ´ng viá»‡c), há»— trá»£ giÃºp xÃ¢y dá»±ng data pipeline khÃ¡ tá»‘t. MÃ¬nh tháº¥y cÃ´ng cá»¥ nÃ y gá»n nháº¹ hÆ¡n Apache Airflow vÃ  cÅ©ng Ä‘Æ¡n giáº£n hÆ¡n. Äáº§u tiÃªn hÃ£y viáº¿t trong docker-compose.yml vá» dagster nhÆ° sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Äá»‘i vá»›i Dagster, ta sáº½ tá»± config báº±ng Dockerfile, hÃ£y táº¡o 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau Ä‘Ã³ táº¡o Dockerfile cÃ³ ná»™i dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt vÃ  requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit lÃ  giao diá»‡n UI cá»§a Dagster, vÃ  ta sáº½ thao tÃ¡c trÃªn Dagit Ä‘á»ƒ quáº£n lÃ½ cÃ¡c assets, jobs,â€¦ de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau Ä‘Ã³ hÃ£y táº¡o 1 folder ./dagster_home, trong Ä‘Ã³ táº¡o 2 file config cho workspace cá»§a dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage vÃ  workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonÄá»ƒ quáº£n lÃ½ cÃ¡c Schedules, sensors,â€¦ ta cáº§n pháº£i cÃ³ dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster-deamon"},{"categories":["projects"],"content":"PipelineTáº¥t cáº£ má»i viá»‡c xÃ¢y dá»±ng pipeline ta sáº½ hoáº¡t Ä‘á»™ng á»Ÿ Ä‘Ã¢y Äáº§u tiÃªn ta cáº§n init má»™t dagster project dagster project scaffold --name etl_pipeline vÃ  thÆ° má»¥c má»›i táº¡o sáº½ trÃ´ng nhÆ° tháº¿ nÃ y: Tip Äá»ƒ cháº¡y Ä‘Æ°á»£c lá»‡nh dagster á»Ÿ bÆ°á»›c táº¡o dagster project, ta cáº§n pháº£i cÃ³ dagster package, náº¿u chÆ°a cÃ³ hÃ£y cÃ i Ä‘áº·t báº±ng: pip install dagster â€“\u003e NÃªn cÃ i Ä‘áº·t trong mÃ´i trÆ°á»ng áº£o Sau Ä‘Ã³ vÃ o thÆ° má»¥c vá»«a táº¡o vÃ o viáº¿t Dockerfile thÃ´i: FROMpython:3.10-slim# Add repository codeWORKDIR/opt/dagster/appCOPY requirements.txt /opt/dagster/appRUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txtWORKDIR/opt/dagster/appCOPY . /opt/dagster/app/etl_pipeline# CMD allows this to be overridden from run launchers or executors that want to run other commands against your repositoryCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"etl_pipeline\"] cÃ¹ng vá»›i requirements.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-aws==0.17.20 dagster-dbt==0.17.20 pandas==1.5.3 SQLAlchemy==1.4.46 pymysql==1.0.2 cryptography==39.0.0 pyarrow==10.0.1 boto3==1.26.57 fsspec==2023.1.0 s3fs==0.4.2 minio==7.1.13 Cuá»‘i cÃ¹ng lÃ  viáº¿t vÃ o docker-compose.yml: etl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:5","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#pipeline"},{"categories":["projects"],"content":"StreamlitCuá»‘i cÃ¹ng lÃ  viá»‡c lÃ  Dashboard, Streamlit cháº¯c cháº¯c lÃ  cÃ´ng cá»¥ siÃªu phÃ¹ há»£p lÃ m viá»‡c nÃ y. ÄÃ¢y lÃ  framework há»— trá»£ viá»‡c xÃ¢y dá»±ng giao diá»‡n Æ°u nhÃ¬n chá»‰ báº±ng Python, quÃ¡ Ä‘Ã£ pháº£i khÃ´ng nÃ o :)) HÃ£y táº¡o folder ./streamlit/scr/ cÃ¹ng vá»›i ./streamlit/Dockerfile: FROMpython:3.10EXPOSE8501WORKDIR/usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . . vÃ  streamlit/requirements.txt: pandas plotly matplotlib numpy streamlit psycopg2-binary sqlalchemy python-dotenv Cuá»‘i cÃ¹ng lÃ  ghi trong yml streamlit:build:./streamymlcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:6","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#streamlit"},{"categories":["projects"],"content":"HoÃ n chá»‰nh setupCuá»‘i cÃ¹ng, file yaml sáº½ trÃ´ng nhÆ° tháº¿ nÃ y: # version: '3.9'services:minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_networkde_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_networkde_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_networkstreamlit:build:./streamlitcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_networkde_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagsterde_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_networkde_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network# Pipelinesetl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_networknetworks:de_network:driver:bridgename:de_network VÃ  .env file: # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_DB=football POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_HOST_AUTH_METHOD=trust # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=football # MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=football MYSQL_ROOT_PASSWORD=admin123 MYSQL_USER=admin MYSQL_PASSWORD=admin123 # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=warehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 Warning Náº¿u báº¡n chá»‰ Ä‘á»c pháº§n HoÃ n chá»‰nh setup thÃ¬ cÃ³ thá»ƒ há»‡ thá»‘ng vá»… sáº½ gáº·p lá»—i vÃ¬ cÃ³ thá»ƒ thiáº¿u cÃ¡c configuration cáº§n thiáº¿t cho dagster, dagit hay pipeline. Báº¡n cáº§n Ä‘á»c qua pháº§n Dagster, Dagit, Pipeline Cháº¡y thá»­: sau khi hoÃ n táº¥t toÃ n bá»™ set up dÃ i ngoáº±n, cÅ©ng Ä‘Ã£ Ä‘áº¿n lÃºc cháº¡y chÆ°Æ¡ng trÃ¬nh thÃ´i. Note nho nhá» Náº¿u báº¡n Ä‘Ã£ build láº§n lÆ°á»£t cÃ¡c images rá»“i, thÃ¬ chá»‰ cáº§n compose up thÃ´i, náº¿u khÃ´ng, hÃ£y build báº±ng lá»‡nh docker compose build trÆ°á»›c khi cháº¡y compose up. docker compose --env-file .env up -d Láº¡i lÃ  má»™t tip cÃ³ thá»ƒ há»¯u Ã­ch Äá»ƒ Ä‘Æ¡n giáº£n hÃ³a viá»‡c ghi lá»‡nh dÃ i dÃ²ng, hÃ£y táº¡o má»™t make file tÃªn Makefile vá»›i ná»™i dung sau: include .env build: docker compose build up: docker compose --env-file .env up -d down: docker compose --env-file .env down restart: make down \u0026\u0026 make up to_psql: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} psql_create: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} -f /tmp/psql_schema.sql to_mysql: docker exec -it de_mysql mysql --local-infile=1 -u\"${MYSQL_U","date":"31 Jul 2023","objectID":"/football_etl/:2:7","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#hoÃ n-chá»‰nh-setup"},{"categories":["projects"],"content":"To be ContinueBÃ i Ä‘áº¿n Ä‘Ã¢y cÅ©ng quÃ¡ lÃ  dÃ i rá»“i, mÃ¬nh sáº½ viáº¿t tiáº¿p á»Ÿ pháº§n 2 :))) ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh -Mew- ","date":"31 Jul 2023","objectID":"/football_etl/:3:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#to-be-continue"},{"categories":["projects"],"content":"Related Content Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... Football ETL Analysis P2 Continuous of Football ETL series Read more... ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#related-content"},{"categories":[],"content":"Data Engineering Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#data-engineering"},{"categories":[],"content":"Machine learning Basic Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#machine-learning-basic"},{"categories":null,"content":" Hello! MÃ¬nh tÃªn lÃ  VÄ©nh Phong hay chÃ­nh lÃ  OG (tÃ¡c giáº£ cá»§a cÃ¡c blogs á»Ÿ trang nÃ y) Hiá»‡n táº¡i mÃ¬nh lÃ  sinh viÃªn ngÃ nh Khoa há»c Dá»¯ liá»‡u (Data Science) táº¡i Äáº¡i há»c Khoa há»c Tá»± nhiÃªn, ÄHQG-HCM (HCMUS). Tuy nhiÃªn mÃ¬nh cÅ©ng yÃªu thÃ­ch cÃ´ng nghá»‡, code vÃ  mÃ¬nh Ä‘ang trÃªn con Ä‘Æ°á»ng há»c táº­p má»—i ngÃ y Ä‘á»ƒ trá»Ÿ thÃ nh má»™t Data Engineer. OG há»“i cuá»‘i lá»›p 12OG há»“i cÃºi lá»›p 12 :)) \" OG há»“i cuá»‘i lá»›p 12 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#"},{"categories":null,"content":"MÃ¬nh cá»§a hiá»‡n táº¡iÄáº§u tiÃªn quan trá»ng nháº¥t chÃ­nh lÃ  viá»‡c há»c táº¡i HCMUS. NgoÃ i ra, mÃ¬nh cÃ²n tá»± há»c vá» cÃ¡c chá»§ Ä‘á» liÃªn quan nhÆ° Data pipeline, Data Streaming,â€¦ Viá»‡c luyá»‡n táº­p, há»c há»i cÅ©ng Äƒn sÃ¢u dÃ´ mÃ¡u mÃ¬nh lÃºc nÃ o khÃ´ng hay :)) MÃ¬nh Ä‘Ã£ tá»«ng Ä‘á»c tháº¥y Ä‘Ã¢u Ä‘Ã³ ráº±ng: The most beautiful thing about learning is that no one take that away from you VÃ  Ä‘iá»u Ä‘Ã³ Ä‘Ã£ truyá»n cáº£m há»©ng mÃ¬nh ráº¥t nhiá»u, thÃºc Ä‘áº©y báº£n thÃ¢n tá»± há»c má»—i ngÃ y vÃ  tá»± lÃ m má»›i báº£n thÃ¢n tá»«ng chÃºt má»™t. Tiáº¿p Ä‘áº¿n chÃ­nh lÃ  xÃ¢y dá»±ng trang web nÃ y vÃ  viáº¿t cÃ¡c bÃ i blogs giÃºp cho cÃ¡c báº¡n cÃ³ thá»ƒ há»c thÃªm kiáº¿n thá»©c ngÃ nh, biáº¿t thÃªm Ä‘iá»u thÃº vá»‹ vÃ  thÆ° giÃ£n. Hiá»‡n táº¡i thÃ¬ OG váº«n cÃ²n ngá»“i trÃªn gháº¿ giáº£ng Ä‘Æ°á»ng, vÃ  vá»«a Ä‘áº·t nhá»¯ng viÃªn gáº¡ch Ä‘áº§u tiÃªn trÃªn con Ä‘Æ°á»ng tá»± trÆ°á»Ÿng thÃ nh. HÃ£y luÃ´n theo dÃµi OG nhÃ©! -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huá»³nh LÆ°u VÄ©nh Phong Facebook: Phong Huynh Instagram: phong_huynh Hoáº·c báº¡n cÅ©ng cÃ³ thá»ƒ ghÃ© thÄƒm Github cá»§a mÃ¬nh: PhongHuynh0394 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#mÃ¬nh-cá»§a-hiá»‡n-táº¡i"}]