[{"categories":["projects"],"content":"Source github: Stock Analysis Hello! OG đây. Ở project lần này mình sẽ phân tích gia trị cổ phiếu phái sinh VN30 Index bằng cách sử dụng PCA và K-means. Xin vô cùng cảm ơn sự đóng góp của 5 thành viên team OG và thầy Minh Mẫn và thầy Hoàng Đức đã tận tình hướng dẫn để team có thể hoàn thành đồ án một cách tốt nhất. Rồi bây giờ gét gô thooiii 😄 Stock AnalysisStock Analysis \" Stock Analysis ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#"},{"categories":["projects"],"content":"IntroStock Analysis hay còn gọi là Market Analysis đề cập đến phương pháp mà nhà đầu tư hoặc nhà giao dịch sử dụng để đánh giá và điều tra một công cụ giao dịch cụ thể, lĩnh vực đầu tư hoặc toàn bộ thị trường chứng khoán. Không những thế, nó liên quan đến việc nghiên cứu dữ liệu thị trường trong quá khứ và hiện tại và tạo ra một phương pháp để chọn cổ phiếu phù hợp để giao dịch. Các nhà đầu tư sẽ đưa ra quyết định mua hoặc bán dựa trên thông tin phân tích chứng khoán. Trong project này ta sẽ phân tích, trực quan hóa bộ dữ liệu giả định được cung cấp bởi khách hàng để đánh giá thị trường chứng khoán trong khoảng thời gian 1 tháng của 30 công ty thuộc VN30 Dưới đây là tóm tắt sơ lược từng bước để xử lý và phân tích: EDA (Exploratory Data Analysis) Data Preprocessing PCA (Principle Component Analysis) K-Means Clustering Data Analysis References Raw Data Source: df_merged.pkl ","date":"31 Aug 2023","objectID":"/stock_analysis/:1:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#intro"},{"categories":["projects"],"content":"Exploratory Data AnalysisĐây là bước đầu tiên, chúng ta sẽ cùng nhau Warning Bản demo nên mình chưa ghi gì :v Bạn đọc notebook ở source github trước nhé! ","date":"31 Aug 2023","objectID":"/stock_analysis/:2:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#exploratory-data-analysis"},{"categories":["projects"],"content":"Related Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Aug 2023","objectID":"/stock_analysis/:0:0","series":[],"tags":["Machine learning","math"],"title":"Stock analysis","uri":"/stock_analysis/#related"},{"categories":[],"content":"Hellooo OG đâyy ! Ở bài này, mình sẽ kể cơ duyên đưa mình đến với ngành Data và quyết định dấn thân vào con đường trở thành một Data Engineer 😄 Gét Goo! ","date":"30 Aug 2023","objectID":"/start_journey/:0:0","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#"},{"categories":[],"content":"Ủa ngành Data Science ?Khoan Khoan … Bên trên là Engineer, qua đây là Science là sao OG ? Từ từ nào 😄 Mọi chuyện bắt đầu khi mình đậu vào một ngành được ca ngợi là ngành “quyến rũ” nhất thế kỷ 21 theo Harvard Business Review , đó là Data Science. Khúc này mình nghe cũng oách oách, nhưng chính xác Data Science là gì ? Và các nhà khoa học dữ liệu (data scientist) làm gì ? ","date":"30 Aug 2023","objectID":"/start_journey/:1:0","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#ủa-ngành-data-science-"},{"categories":[],"content":"Data Science là gì nhỉ?Ngành Khoa học dữ liệu hay Data Science là một lĩnh vực liên ngành ứng dụng các phương pháp khoa học, thuật toán và các phân tích thống kê để tìm kiếm ý nghĩa từ dữ liệu. Hay nói bằng cách dễ hiểu, Data Science là ngành tìm kiếm, phân tích dữ liệu để khai thác tất cả những giá trị mà dữ liệu mang lại để phục vụ nhiều mục đích khác nhau. Data Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán tương laiData Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán \" Data Science là ứng dụng khoa học để tìm kiếm ý nghĩa của dữ liệu để dự đoán tương lai Một nhà khoa học dữ liệu (Data Scientist) là người chịu trách nhiệm đưa ra các dẫn chứng từ dữ liệu, để từ đó đề xuất các giải pháp, kế hoạch hay định hướng từ ý nghĩa tìm được từ dữ liệu để giải quyết các bài toán kinh doanh khác nhau. Một data scientist cần phải biết kỹ năng gì? Lập trình: Python và R là 2 ngôn ngữ chính được sử dụng đối với ngành này. Python là một ngôn ngữ lập trình linh hoạt phổ biến với rất nhiều thư viện để xử lý dữ liệu như numpy, pandas, matplotlib,… Trong khi đó R tỏ là là một ngôn ngữ mạnh mẽ về phân tích và thống kê, ngoài ra R cũng thường được dùng trong nghiên cứu và học thuật. Thống kê và ứng dụng toán học: Nếu bạn không yêu thích toán học, chắc hẳn bạn cũng sẽ không thể làm điều đó với data science. Hẳn vậy, là một nhà khoa học dữ liệu, bạn cần có một nền tảng kiến thức toán học vững, đặc biệt là về xác suất thống kê và đại số tuyến tính,… SQL và DBMS: Ta phải tiếp xúc rất nhiều với hệ quản trị cơ sở dữ liệu (Database Management System hay DBMS), đó có thể là hệ quản trị cơ sở dữ liệu Quan Hệ (Relational Database Management System) như MySQL, Postgres, SQL server… hay NoSQL database như MongoDB, Cassandra,… Và để tương tác với database (RDBMS), điều không thể thiếu chính là SQL (Structured query language aka si cồ hay ét qui eo 😂 ). Cơ bản thì đây là ngôn ngữ dùng để truy suất dữ liệu, giao tiếp với database, đặc biệt là các RDBMS. AI, Machine learning: Khi có một lượng dữ liệu khổng lồ, một data scientist có thể sẽ dùng chúng để huấn luyện mô hình học máy hoặc mạng để giải các bài toán hồi quy và đưa ra được các dự đoán về xu hướng data hay giải quyết các bài toán phân loại. Có hiểu biết về các thuật toán máy học và kiến trúc mạng noron cũng là một điều cần có ở nhà khoa học dữ liệu. Đọc đến đây, có thể bạn sẽ có cảm giác “Dèjà vu” nhẹ … Sao nhiều chổ giống Data Analyst thế nhỉ ? Mà thiệt ra là không giống đâu nhé, hai ngành này chỉ là anh em xã hội với nhau mà thôi 😂 ","date":"30 Aug 2023","objectID":"/start_journey/:1:1","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-science-là-gì-nhỉ"},{"categories":[],"content":"Data Scientist vs Data AnalystSẵn tiện kể một chút về vai trò của một người Data Analyst. Về cơ bản, vai trò của họ cũng giống với các data scientist, họ cũng phân tích dữ liệu, cố gắng tìm kiếm và rút ra giá trị từ chúng. Nhưng sẽ có một số điểm khác biệt: Data Analyst: hay thường được biết đến là chuyên viên phân tích dữ liệu, họ làm công việc của data science nhưng họ không bắt buộc phải biết nhiều về lập trình. Họ được đòi hỏi phải có nhiều kiến thức hơn về hoạt động kinh doanh và vững về kiến thức thống kê. Data Scientist: Nhà khoa học dữ liệu, họ đảm nhận công việc với quy mô lớn hơn rất nhiều (Bigdata) và họ được đào tạo để phát triển “data product” nhằm đưa ra quyết định có ích cho công ty từ lượng dữ liệu khổng lồ từ tập dữ liệu lớn. Ngoài ra nhà khoa học dữ liệu cũng cần phải biết một số ngôn ngữ lập trình nhất đinh và có kiến thức về computer science. Data Science and Data AnalyticSource: https://www.datascience-pm.com/wp-content/uploads/2021/05/data-scientist-vs-analyst-venn-diagram.png \" Data Science and Data Analytic Rồi okay nãy giờ là cả data science (DS) và data analytic (DA) rồi. Giờ là mới đến data engineer của tui nè hihi 😄 ","date":"30 Aug 2023","objectID":"/start_journey/:1:2","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-scientist-vs-data-analyst"},{"categories":[],"content":"Data Engineer là gì ?Tuy học Data Science, nhưng thực ra ngay từ những lúc còn mơn mởn cấp 3, OG đã từng có ước muốn trở thành một lập trình viên một tay cafe một tay chém code bình loạn thiên hạ 😂 Và thế là tìm được một ngành thích hợp được coi là “Software engineer cho data”, ngành này là một trong các ngành có xu hướng phát triển nhanh nhất trong nhóm ngành công nghệ. Vâng đó chính là Data Engineer Data EngineerData Engineer được coi là Software Engineer ở Data field \" Data Engineer Đầu tiên, Data Engineer hay DE được gọi là kỹ sư dữ liệu. Đây là vai trò đảm nhiệm việc phân tích nguồn dữ liệu, xây dựng và duy trì hệ thống cơ sở dữ liệu hiệu quả. Ngoài ra cũng là người đảm bảo chất lượng dữ liệu cho các phòng ban khác sử dụng. Cơ bản để là để cho DS và DA làm việc một cách hiệu quả nhất, họ cần có một nguồn data ổn định và sạch sẽ. Và người đảm nhiệm việc luân chuyển data đó tới cho họ chính là Data Engineer. Không chỉ có DS và DA mà data engineer phục vụ cho tất cả các phòng ban khác Data Engineer Nói tóm lại, Data Engineer là người xây dựng các đường ống dữ liệu (data pipeline) để truyền dữ liệu từ nơi này sang nơi khác một cách chất lượng nhất :)) Khái niệm cơ bản là thế thôi, nghe có vẻ đơn giản phải không. Hãy tiếp tục với mục tiếp theo để xem liệu ta cần gì để trở thành data engineer ","date":"30 Aug 2023","objectID":"/start_journey/:2:0","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-engineer-là-gì-"},{"categories":[],"content":"Data Engineer thì cần biết gì ?Một data Engineer về bản chất là xây dựng các data pipeline để luân chuyển dữ liệu. Để làm tốt việc đó, kỹ sư dữ liệu phải biết: Kỹ năng lập trình: Tất nhiên rồi, bạn là một nhân viên IT thì điều này là phải có. Các ngôn ngữ mà DE thường dùng là SQL, Python và R. Hệ cơ sở dữ liệu quan hệ và phi quan hệ: Dữ liệu có rất nhiều dạng: Structure/Semi/Unstructure data, do đó cũng cần có nhiều loại database quản lý chúng. Và DE làm việc rất nhiều với database. Họ sẽ là người trực tiếp tương tác kể cả với SQL và NoSQL database. ETL/ELT: ETL aka Extract Transform Load hay ELT aka Extract Load Transform là quy trình xử lý và luân chuyển dữ liệu từ nguồn đến đích. Một DE phải nắm được để thiết kế data pipeline một cách hiệu quả nhất Data Warehouse: hay được biết đến là kho chứa dữ liệu. Bạn có thể sẽ phải xây dựng, thiết kế cấu trúc data warehouse trên cloud platform và xây dựng các kết nối dữ liệu để tối ưu hóa tốc độ truy xuất và đảm bảo việc phân tích dữ liệu. Big Data: Bạn cũng cần phải biết các kiến trúc lưu trữ và xử lý tập dữ liệu lớn như Hadoop, Spark,… Cloud: Tất nhiên là phải có rồi, các cloud platform như Google Cloud Platform, AWS, Azure,… đã rất nổi tiếng trong việc hỗ trợ xây dựng và thiết kế hệ thống pipeline cũng như hỗ trợ tối đa việc xử lý bigdata cũng như deploy hệ thống hạ tầng một cách nhanh chóng. Bạn có thể sẽ phải làm việc với lượng dữ liệu khổng lồ và tập dữ liệu lớn. Và để xây dựng hệ thống xử lý được lượng dữ liệu đó, chắc chắn phải có sự góp mặt của các nền tảng đám mây. Well… Nhìn chung cũng nhiều thứ cần phải biết đấy nhỉ, tất nhiên đó chỉ là một số điều quan trọng nhất. Ngoài ra bạn cũng cần phải biết một số kiến thức khác về Unix và Linux, Docker, Git, Batch/Stream Processing,… Và còn ti tỉ thứ khác mà OG có kể đến răng long đầu bạc có lẽ cũng chưa hết 😂 ","date":"30 Aug 2023","objectID":"/start_journey/:2:1","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#data-engineer-thì-cần-biết-gì-"},{"categories":[],"content":"Tạm kếtHành trình nào khi bắt đầu cũng gian nan, cả bản thân OG khi bắt đầu cũng không biết gì cả. Nhưng khi nhấc ngón chân lên và đi thì mới cảm nhận được thế giới chứ 😄 Hy vọng bài viết này giúp bạn thư giãn và có một cái nhìn chung về ngành data nhé. Hẹn gặp lại trong bài tiếp theo hehe -Meww ","date":"30 Aug 2023","objectID":"/start_journey/:3:0","series":[],"tags":[],"title":"Thằng nhóc thích code và data","uri":"/start_journey/#tạm-kết"},{"categories":[],"content":"Giới thiệu về trang web đáng yêu này","date":"30 Aug 2023","objectID":"/intro_blog/","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/"},{"categories":[],"content":"Hello! Hello! mình là OG đây. Đây là bài blog đầu tiên của mình ở đây. Có thể bạn đang tự hỏi rằng mình là ai và đây là nơi nào đúng không, vậy hãy cùng mình tìm hiểu thử nhé ","date":"30 Aug 2023","objectID":"/intro_blog/:0:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#"},{"categories":[],"content":"OG là ai ?Một lần nữa giới thiệu mình tên là Vĩnh Phong - một anh chàng thích tìm tòi điều mới… hmmm thế thôi :))) Để hiểu thêm về mình: About Còn OG (nickname) là biệt danh mình lấy cảm hứng từ một nhât vật hoạt hình rất hóm hỉnh đấy nhé hehe. Đó là tên một chú mèo xanh dương hay bị mấy con gián quậy :)) bạn thử đoán xem Đáp án Ủa lộn này là tui:))Tui \" Ủa lộn này là tui:)) Đây mới là đáp án Nếu bạn hong biết, thì con mèo xanh lè này tên là Oggy và nickname mình cũng vậy :)) Biết mình là ai rồi, thế thì… ","date":"30 Aug 2023","objectID":"/intro_blog/:1:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#OG-la-ai"},{"categories":[],"content":"Đây là nơi nào đây ?Đây trang mà mình đăng lên các bài blogs về công nghệ, về ngành Data nói chung và về hành trình học tập của OG để trở thành một Data Engineer trong tương lai. Tất nhiên không chỉ như vậy Mình cũng viết blogs về đời sống, về những điều thú vị của cuộc sống xung quanh Và mình hy vọng trang cũng này sẽ là nơi mang lại sự thoải mái và thư giản cho mọi người ","date":"30 Aug 2023","objectID":"/intro_blog/:2:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#day-la-dau"},{"categories":[],"content":"Thế ở đây có gì hay?Tóm tắt các trang: Blogs: bạn có thể tìm thấy các bài blogs mình viết ở đây Projects: Những dự án mình đã làm About: Nếu bạn muốn hiểu thêm về mình Đó là tổng quan “các thứ có thể sẽ xuất hiện” ở trang web này. ","date":"30 Aug 2023","objectID":"/intro_blog/:2:1","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#thế-ở-đây-có-gì-hay"},{"categories":[],"content":"Cuối cùngOG hy vọng đây sẽ là nơi giúp bạn thư giãn hay học hỏi được nhiều điều thú vị nhé 😄 -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huỳnh Lưu Vĩnh Phong Facebook: Phong Huynh Instagram: phong_huynh Hoặc bạn cũng có thể ghé thăm Github của mình: PhongHuynh0394 ","date":"30 Aug 2023","objectID":"/intro_blog/:3:0","series":[],"tags":[],"title":"Intro: Tui là ai ? Đây là đâu ?","uri":"/intro_blog/#cuoi-cung"},{"categories":null,"content":"Continuous of Football ETL series","date":"01 Aug 2023","objectID":"/football_etl_2/","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/"},{"categories":null,"content":"Hello! Hello! OG đây, sau phần 1 chúng ta đã setup các kiểu và đảm bảo mọi thứ trơn tru rồi, ở phần này chúng ta sẽ chuẩn bị Data Source, và khởi chạy pipeline ở Implement sau đó sẽ Visualize cleaned data có được từ data warehouse thành Dashboard. Bắt đầu thôi nào ! ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#"},{"categories":null,"content":"Data Source","date":"01 Aug 2023","objectID":"/football_etl_2/:1:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#data-source"},{"categories":null,"content":"Chuẩn bị file làm raw dataCác file csv sử dụng làm dữ liệu được tải từ Football Database - Kaggle. Đây là dữ liệu thống kê của cầu thủ, đội bóng đến từ 5 giải bóng hàng đầu Châu Âu (Premier League, Laliga, Seria A, Budesliga, League 1) Ta sẽ có schema như sau: SchemaSchema \" Schema trong đó: games: bảng chứa thông tin thống kê của từng trận đấu (gameID) teams: Bảng chứa tên các đội bóng (teamID) players: Bảng chứa tên các cầu thủ (playerID) leagues: Bảng chưa tên các giải đấu (leagueID) appearances: Bảng thống kê của cầu thủ ở các game mà họ tham gia (gameID, playerID) teamstats: Bảng thống kê của đội bóng ở từng game (gameID, teamID) ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#chuẩn-bị-file-làm-raw-data"},{"categories":null,"content":"Load data vào MySQLCó nhiều cách để load data vào MySQL, ở đây mình sẽ sử dụng cách LOAD LOCAL_INFILE của MySQL luôn. Tip Hãy đảm bảo bạn đã make up lần đầu rồi nhé ! Hãy copy folder chứa các file csv vào de_mysql container: docker cp /football de_mysql:/tmp/dataset/ docker cp /load_data de_mysql:/tmp/dataset/ Sau đó tạo bảng trống sẵn trong MySQL: make mysql_create #Create table in mysql Tiếp tục với lệnh: make to_mysql_root # ----- You will access to MySQL container SET GLOBAL LOCAL_INFILE=TRUE; #Set local_infile variable to load data from local exit; # ----- Exit container make mysql_load #load data make mysql_create_relation #create table relation Thế là đã chuẩn bị xong dữ liệu cho MySQL. ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load-data-vào-mysql"},{"categories":null,"content":"Init PostgreSQL SchemaTa cũng cần phải tạo sẵn schema sẵn trong Posgres như sau: make to_psql CREATE SCHEMA IF NOT EXISTS analysis; exit; Thế là đã hoàn tất việc chuẩn bị data, giờ thì ta bắt đầu vào phần việc chính thôi ","date":"01 Aug 2023","objectID":"/football_etl_2/:1:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#init-postgresql-schema"},{"categories":null,"content":"ImplementCông việc chính trong phần này là xây dựng các data pipeline bằng dagster. Cơ bản có thể hiểu là ta tạo các Asset và chuyển chúng từ database này sang database khác. ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#implement"},{"categories":null,"content":"ExtractionĐể có thể quản lý việc truy xuất dữ liệu từ MySQL và load vào MinIO để lưu tạm, ta sẽ xây dựng một I/O Manager phục vụ việc đó. Đầu tiên, hãy vào đường dẫn: ./etl_pipeline/etl_pipeline/resources/ Ta sẽ xây dựng MySQL io manager bằng cách tạo file mysql_io_manager.py với nội dung sau: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_mysql(config): conn_info = ( f\"mysql+pymysql://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class MySQLIOManager(IOManager): def __init__(self, config): self.config = config def handle_output(self, context: OutputContext, obj: pd.DataFrame): pass def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def extract_data(self, sql: str) -\u003e pd.DataFrame: with connect_mysql(self.config) as db_conn: pd_data = pd.read_sql_query(sql, db_conn) return pd_data Sau đó, tiếp tục đối với minio_io_manager.py: import os from contextlib import contextmanager from datetime import datetime from typing import Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from dagster import IOManager, InputContext, OutputContext from minio import Minio @contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\"endpoint_url\"), access_key=config.get(\"aws_access_key_id\"), secret_key=config.get(\"aws_secret_access_key\"), secure=False ) try: yield client except Exception: raise class MinIOIOManager(IOManager): def __init__(self, config): self._config= config def _get_path(self, context: Union[InputContext, OutputContext]): layer, schema, table = context.asset_key.path key = \"/\".join([layer, schema, table.replace(f\"{layer}_\", \"\")]) tmp_file_path = \"/tmp/file-{}-{}.parquet\".format( datetime.today().strftime(\"%Y%m%d%H%M%S\"), \"-\".join(context.asset_key.path) ) if context.has_asset_partitions: start, end = context.asset_partitions_time_window dt_format = \"%Y%m%d%H%M%S\" partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format) return os.path.join(key, f\"{partition_str}.pq\"), tmp_file_path else: return f\"{key}.pq\", tmp_file_path def handle_output(self, context: OutputContext, obj: pd.DataFrame): # convert to parquet format key_name, tmp_file_path = self._get_path(context) table = pa.Table.from_pandas(obj) pq.write_table(table, tmp_file_path) # upload to MinIO try: bucket_name = self._config.get(\"bucket\") with connect_minio(self._config) as client: # Make bucket if not exist. found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exists\") client.fput_object(bucket_name, key_name, tmp_file_path) row_count = len(obj) context.add_output_metadata({\"path\": key_name, \"tmp\": tmp_file_path}) # clean up tmp file os.remove(tmp_file_path) except Exception: raise def load_input(self, context: InputContext) -\u003e pd.DataFrame: bucket_name = self._config.get(\"bucket\") key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: #Make bucket if not exist found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\"Bucket {bucket_name}already exist\") client.fget_object(bucket_name, key_name, tmp_file_path) pd_data = pd.read_parquet(tmp_file_path) return pd_data except Exception: raise Sau khi đã tạo thành công các io manager cho mysql và minio, ta sẽ bắt đầu xây dựng bronze layer Note nho nhỏ Trong project này mình chia các giai đoạn transformation thành các layer: bronze layer: Giai đoạn chỏ mới load raw data, có thể hiểu đây là data chưa transform gì cả siler layer: Transform một phần từ bronze layer, ở đoạn này data đã được cleaning sơ gold layer: Sau khi transform một lần nữa từ silver layer, giai đoạn này sẽ truy vấn ra các thôn","date":"01 Aug 2023","objectID":"/football_etl_2/:2:1","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#extraction"},{"categories":null,"content":"TransformationTiếp tục tạo file silver_layer.py cùng folder với bronze layer: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teamstats\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"leagues\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], description='Statistic of teams in games', group_name=\"Silver_layer\", compute_kind=\"Pandas\" ) def silver_statsTeamOnGames(teamstats: pd.DataFrame, games: pd.DataFrame, leagues: pd.DataFrame) -\u003e Output[pd.DataFrame]: ts = teamstats.copy() gs = games.copy() lgs = leagues.copy() #Drop unsusable columns in games gs.drop(columns=gs.columns.to_list()[13:], inplace=True) #create StatperLeagueSeason result = pd.merge(ts, gs, on=\"gameID\") result = result.merge(lgs, on=\"leagueID\", how=\"left\") result.drop(columns=['season_y', 'date_y'],inplace=True) result = result.rename(columns={'season_x': 'season', 'date_x': 'date'}) return Output( result, metadata={ \"table\": \"statsTeamOnGames\", \"records\": len(result) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"appearances\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"games\": AssetIn( key_prefix=[\"football\", \"bronze\"] ), \"players\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='statistic of players in games', compute_kind=\"Pandas\" ) def silver_playerAppearances(appearances: pd.DataFrame, games: pd.DataFrame, players: pd.DataFrame) -\u003e Output[pd.DataFrame]: app = appearances.copy() ga = games.copy() pla = players.copy() #Drop unusable column ga.drop(columns=ga.columns.to_list()[13:], inplace=True) #Merge player_appearances = pd.merge(app, pla, on=\"playerID\", how=\"left\") player_appearances = pd.merge(player_appearances, ga, on=\"gameID\", how=\"left\") #drop unecessary columns and rename player_appearances.drop(columns=['leagueID_y'],inplace=True) player_appearances.rename(columns={'leagueID_x': 'leagueID'}, inplace=True) return Output( player_appearances, metadata={ \"table\": \"playerAppearances\", \"records\": len(player_appearances) } ) @asset( io_manager_key=\"minio_io_manager\", required_resource_keys={\"minio_io_manager\"}, ins={ \"teams\": AssetIn( key_prefix=[\"football\", \"bronze\"] ) }, key_prefix=[\"football\", \"silver\"], group_name=\"Silver_layer\", description='Teams', compute_kind=\"Pandas\" ) def silver_teams(teams: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( teams, metadata={ \"table\": 'teams', 'records': len(teams) } ) Lúc này mình có 3 silver assets, được join từ các bảng ở bronze Tiếp đến là gold_layer, lúc này ta sẽ tính các thông số thống kê của từng giải đâu từng mùa, các thống kê của cầu thủ trong 90 phút thi đấu, và cả thống kê của từng cầu thủ trong từng mùa giải gold_layer.py sẽ có nội dung: from dagster import asset, Output, AssetIn import pandas as pd @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_statsTeamOnGames\": AssetIn( key_prefix=[\"football\", \"silver\"] ) }, group_name=\"Gold_layer\", key_prefix=[\"football\", \"gold\"], description='Statistic of all league in each season', compute_kind=\"Pandas\" ) def gold_statsPerLeagueSeason(silver_statsTeamOnGames: pd.DataFrame) -\u003e Output[pd.DataFrame]: st = silver_statsTeamOnGames.copy() result = ( st.groupby(['name', 'season']) .agg({\"goals\": \"sum\", \"xGoals\": \"sum\", \"shots\": \"sum\", \"shotsOnTarget\": \"sum\", \"fouls\": \"sum\", \"yellowCards\": \"sum\", \"redCards\": \"sum\",'corners': 'sum', \"gameID\": 'count'}) .reset_index() ) result = result.rename(columns={'gameID':\"games\"}) result['goalPerGame']= result.goals/result.games result['season'] = result['season'].astype('string') return Output( result, metadata={ 'table': 'statPerLeagueSeason', 'records': len(result) } ) @asset( io_manager_key=\"minio_io_manager\", ins={ \"silver_playerAppearances\": AssetIn( key_prefix=[\"football\", \"silver\"] ) },","date":"01 Aug 2023","objectID":"/football_etl_2/:2:2","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#transformation"},{"categories":null,"content":"LoadTrước hết hãy tạo IO Manager cho Postgres để quản lý việc load cleaned data. Ta tạo file psql_io_manager.py ở vị trí mà ta đã tạo 2 io manager trước với nội dung: from contextlib import contextmanager from datetime import datetime import pandas as pd from dagster import IOManager, OutputContext, InputContext from sqlalchemy import create_engine @contextmanager def connect_psql(config): conn_info = ( f\"postgresql+psycopg2://{config['user']}:{config['password']}\" + f\"@{config['host']}:{config['port']}\" + f\"/{config['database']}\" ) db_conn = create_engine(conn_info) try: yield db_conn except Exception: raise class PostgreSQLIOManager(IOManager): def __init__(self, config): self._config = config def load_input(self, context: InputContext) -\u003e pd.DataFrame: pass def handle_output(self, context: OutputContext, obj: pd.DataFrame): schema, table = context.asset_key.path[-2], context.asset_key.path[-1] with connect_psql(self._config) as conn: # insert new data ls_columns = (context.metadata or {}).get(\"columns\", []) obj[ls_columns].to_sql( name=f\"{table}\", con=conn, schema=schema, if_exists=\"replace\", index=False, chunksize=10000, method=\"multi\" ) Sau đó, tạo một asset warehouse_layer.py: from dagster import multi_asset, Output, AssetIn, AssetOut, asset import pandas as pd @multi_asset( ins={ \"gold_statsPerLeagueSeason\": AssetIn( key_prefix=[\"football\", \"gold\"] ) }, outs={ \"statsperleagueseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerLeagueSeason\", 'analysis'], metadata={ \"columns\": [ \"name\", \"season\", \"goals\", \"xGoals\", \"shots\", \"shotsOnTarget\", \"fouls\", \"yellowCards\", \"redCards\", \"corners\", \"games\", \"goalPerGame\" ] } ), }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerLeagueSeason(gold_statsPerLeagueSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerLeagueSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerLeagueSeason\", \"records\": len(gold_statsPerLeagueSeason) } ) @multi_asset( ins={ \"gold_statsPerPlayerSeason\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsperplayerseason\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPerPlayerSeason\", 'analysis'], metadata={ \"columns\": [ \"playerID\", \"name\", \"season\", \"goals\", \"shots\", \"xGoals\", \"xGoalsChain\", \"xGoalsBuildup\", \"assists\", \"keyPasses\", \"xAssists\", \"gDiff\", \"gDiffRatio\" ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPerPlayerSeason(gold_statsPerPlayerSeason: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPerPlayerSeason, metadata={ \"schema\": \"analysis\", \"table\": \"statsPerPlayerSeason\", \"records\": len(gold_statsPerPlayerSeason) } ) @multi_asset( ins={ \"gold_statsPlayerPer90\": AssetIn( key_prefix=['football', 'gold'] ) }, outs={ \"statsplayerper90\": AssetOut( io_manager_key=\"psql_io_manager\", key_prefix=[\"statsPlayerPer90\", 'analysis'], metadata={ \"columns\": [ 'playerID', 'name', 'total_goals', 'total_assists', 'total_time', 'goalsPer90', 'assistsPer90', 'scorers' ] } ) }, compute_kind=\"PostgreSQL\", group_name=\"Warehouse_layer\" ) def statsPlayerPer90(gold_statsPlayerPer90: pd.DataFrame) -\u003e Output[pd.DataFrame]: return Output( gold_statsPlayerPer90, metadata={ \"schema\": \"analysis\", \"table\": \"statsPlayerPer90\", \"records\": len(gold_statsPlayerPer90) } ) ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:3","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#load"},{"categories":null,"content":"Run systemCuối cùng, ta sẽ kết hợp tất cả các asset lại giúp dagster nhận diện và quản lý với file __init__.py ở etl_pipeline/etl_pipeline/__init__.py import os from dagster import Definitions from .assets.silver_layer import * from .assets.gold_layer import * from .assets.bronze_layer import * from .assets.warehouse_layer import * from .resources.minio_io_manager import MinIOIOManager from .resources.mysql_io_manager import MySQLIOManager from .resources.psql_io_manager import PostgreSQLIOManager MYSQL_CONFIG = { \"host\": os.getenv(\"MYSQL_HOST\"), \"port\": os.getenv(\"MYSQL_PORT\"), \"database\": os.getenv(\"MYSQL_DATABASE\"), \"user\": os.getenv(\"MYSQL_USER\"), \"password\": os.getenv(\"MYSQL_PASSWORD\") } MINIO_CONFIG = { \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\"), \"bucket\": os.getenv(\"DATALAKE_BUCKET\"), \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"), \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\") } PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } ls_asset=[asset_factory(table) for table in tables] + [silver_statsTeamOnGames, silver_teams , silver_playerAppearances, gold_statsPerLeagueSeason, gold_statsPerPlayerSeason, gold_statsPlayerPer90, statsPerLeagueSeason, statsPerPlayerSeason, statsPlayerPer90] defs = Definitions( assets=ls_asset, resources={ \"mysql_io_manager\": MySQLIOManager(MYSQL_CONFIG), \"minio_io_manager\": MinIOIOManager(MINIO_CONFIG), \"psql_io_manager\": PostgreSQLIOManager(PSQL_CONFIG), } ) sau đó hãy dùng lệnh sau để cập nhật các assets docker restart etl_pipeline ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:4","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#run-system"},{"categories":null,"content":"Check UIHãy kiểm tra Dagit UI ở localhost:3001 để chắc chắn rằng mọi thứ vẫn ổn Ngoài ra cũng có thể check MinIO: localhost:9000 ","date":"01 Aug 2023","objectID":"/football_etl_2/:2:5","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#check-ui"},{"categories":null,"content":"VisualizationCuối cùng là vẽ dashboard, đầu tiên ta cần phải lấy được data từ psql, hãy vào tạo file: ./streamlit/src/psql_connect.py: import os import psycopg2 from dotenv import load_dotenv import pandas as pd #load environment load_dotenv() #list table in database table = ['statsperleagueseason','statsperplayerseason', 'statsplayerper90'] PSQL_CONFIG = { \"host\": os.getenv(\"POSTGRES_HOST\"), \"port\": os.getenv(\"POSTGRES_PORT\"), \"database\": os.getenv(\"POSTGRES_DB\"), \"user\": os.getenv(\"POSTGRES_USER\"), \"password\": os.getenv(\"POSTGRES_PASSWORD\") } #create connection def init_connection(config): return psycopg2.connect( database=config['database'], user=config['user'], password=config['password'], host=config['host'], port=config['port'] ) def extract_data(): conn = init_connection(PSQL_CONFIG) return [pd.read_sql(f'SELECT * FROM analysis.{tab}', conn) for tab in table] Cuối cùng là tạo main.py ngay trong thư mục scr: import streamlit as st import pandas as pd import plotly.express as px import plotly.graph_objects as go from psql_connect import extract_data import numpy as np # #extract data from PostgreSQL ls_df = extract_data() l_season = ls_df[0] p_season = ls_df[1] p_match = ls_df[2] st.set_page_config(page_title = 'Dashboard Football', layout='wide', page_icon='chart_with_upwards_trend') #Overview def overview(table: pd.DataFrame, detail: str): if (st.checkbox('Do you want to see Data ?')): table col1, col2 = st.columns(2) co_df = table.columns.to_list() with col1: st.bar_chart(table.describe()) if (st.checkbox('Do you want to see describe each column ?')): for col in co_df: if table[col].dtypes not in ['int64', 'float64']: continue st.bar_chart(table[col].describe()) with col2: st.caption(f':red[Columns]: {len(co_df)}') st.caption(f':red[Records]: {len(table)}') st.caption(f':red[Description]: {detail}') st.caption(f':red[Columns name]:{co_df}') #league statistic def statleague(): Cards = l_season[['name','season','yellowCards', 'redCards', 'fouls']] #Card_fouls col1, col2 = st.columns(2) with col1: #Goals per games fig = px.bar(l_season, x=\"name\", y=\"goalPerGame\", color=\"name\", barmode=\"stack\", facet_col=\"season\", labels={\"name\": \"League\", \"goals/games\": \"GPG\"}) fig.update_layout(showlegend=False, title='Goals per Game') st.plotly_chart(fig) #fouls fig = px.line(Cards, x='season', y='fouls', color='name') fig.update_layout(title='Fouls of leagues', xaxis_title='Season', yaxis_title='Fouls', legend_title='League') st.plotly_chart(fig) with col2: #Red card fig = px.line(Cards, x='season', y='redCards', color='name') fig.update_layout(title='Red Cards of leagues', xaxis_title='Season', yaxis_title='RedCards', legend_title='League') st.plotly_chart(fig) #yellow card fig = px.line(Cards, x='season', y='yellowCards', color='name') fig.update_layout(title='Yellow Cards of leagues', xaxis_title='Season', yaxis_title='YellowCards', legend_title='League') st.plotly_chart(fig) #Player statistic def statplayer(): col1, col2 = st.columns(2) with col1: #Best offensive player top_player90= p_match[(p_match['goalsPer90'] \u003e 0.8) | (p_match['assistsPer90'] \u003e 0.4)] fig = px.scatter(p_match[['name','goalsPer90', 'assistsPer90']], x='goalsPer90', y='assistsPer90', hover_name='name') fig.add_trace( go.Scatter(x=top_player90['goalsPer90'], y=top_player90['assistsPer90'], mode='markers+text', marker_size=5, text=top_player90['name'], textposition='bottom center', textfont=dict(size=15)) ) fig.update_layout(title='Best offensive Players (2018-2020)', xaxis_title='Goals Per 90min', yaxis_title='Assists Per 90min') st.plotly_chart(fig) #goals-xgoal fig = px.scatter(p_season, x=\"xGoals\", y=\"goals\", color=(p_season['xGoals'] - p_season['goals'] \u003c 10), color_discrete_sequence=[\"red\", \"green\"], opacity=0.5) fig.update_layout(title=\"Goals (G) and Expected Goals (xG)\", xaxis_title=\"xG\", yaxis_title=\"G\", ) st.plotly_chart(fig) with col2: #Top score player topPlayer = p_season.groupby(['name']).agg({'goals': 'sum'}).sort_values('goals', ascending=False).res","date":"01 Aug 2023","objectID":"/football_etl_2/:3:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#visualization"},{"categories":null,"content":"ConclusionCuối cùng cũng đã xong một project xây dựng data pipeline cơ bản, trong lúc thực hiện chắc chắn sẽ có cả tấn lỗi xảy ra, nhưng OG tin là mọi gian khó đều sẽ vượt quan được, thứ đọng lại chính là kiến thức và kỹ năng của chúng ta. Chúc bạn thành công và đón xem tiếp các dự án tiếp theo của OG nhé ! -Mew- ","date":"01 Aug 2023","objectID":"/football_etl_2/:4:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#conclusion"},{"categories":null,"content":"Related Content Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"01 Aug 2023","objectID":"/football_etl_2/:0:0","series":["Football ETL"],"tags":[],"title":"Football ETL Analysis P2","uri":"/football_etl_2/#related-content"},{"categories":["projects"],"content":"A Data Engineer project building pipeline to analyze football data","date":"31 Jul 2023","objectID":"/football_etl/","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/"},{"categories":["projects"],"content":"Source code: Football_ETL_Analysis ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#"},{"categories":["projects"],"content":"IntroduceTrong project này, OG sẽ build end-to-end ETL data pipeline hoàn chỉnh để phân tích football data từ Kaggle với data pipeline như sau: Data pipelinedata pipeline \" Data pipeline Các bước cụ thể: Set up: Dùng Docker tạo container và các images cần thiết cho pipeline, trong đó có cả Dagster dùng xây dựng pipeline. Chuẩn bị data source: load các file csv (có được từ Kaggle) vào MySQL nhằm mục đích lưu trữ raw data (mô phỏng source data) Extract: Lấy data từ MySQL và load vào MinIO chuẩn bị cho bước transform Transform: Sử dụng Pandas để truy vấn các file từ MinIO Load: Cleaned và transformed data được load vào warehouse PostgreSQL Visualization: Sử dụng Streamlit để làm Dashboard ","date":"31 Jul 2023","objectID":"/football_etl/:1:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#introduce"},{"categories":["projects"],"content":"Set upBắt đầu với Docker, ta sẽ xây dựng lần lượt từng image bằng cách viết docker-compose.yml Tip nho nhỏ Hãy pull/build lần lượt từng loại framework lần lượt để chắc chắn rằng chúng hoạt động trơn tru nhất trước Hoặc bạn cũng có thể xem luôn phần hoàn chỉnh Hoàn chỉnh set up ","date":"31 Jul 2023","objectID":"/football_etl/:2:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#set-up"},{"categories":["projects"],"content":"MinIOMinIO là một server lưu trữ đối tượng dạng phân tán với hiệu năng cao và cung cấp các api giống với Amazon S3, ta có thể upload, download file,… một cách đơn giản. minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_network Note Về .env file, đây là file chứa thông tin các biến môi trường thiết lập cho từng image, mình sẽ nói sau ","date":"31 Jul 2023","objectID":"/football_etl/:2:1","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#minio"},{"categories":["projects"],"content":"MySQLMySQL là một trong số các phần mềm RDBMS (Relational DataBase Management Systems) phổ biến nhất, ta sẽ sử dụng database này để lưu raw data mô phỏng cho source data cần ingest de_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:2","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#mysql"},{"categories":["projects"],"content":"PostgeSQLPostgreSQL là một hệ thống quản trị cơ sở dữ liệu quan hệ-đối tượng (object-relational database management system), và ta sẽ dung nó làm data warehouse cho project lần này. de_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:3","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#postgesql"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagit"},{"categories":["projects"],"content":"DagsterDagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối các tác vụ và công việc), hỗ trợ giúp xây dựng data pipeline khá tốt. Mình thấy công cụ này gọn nhẹ hơn Apache Airflow và cũng đơn giản hơn. Đầu tiên hãy viết trong docker-compose.yml về dagster như sau: de_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagster Đối với Dagster, ta sẽ tự config bằng Dockerfile, hãy tạo 1 folder ./dagster/: mkdir dagster cd dagster touch Dockerfile requirements.txt Sau đó tạo Dockerfile có nội dung sau: # Dagster libraries to run both dagit and the dagster-daemon. Does not# need to have access to any pipeline code.FROMpython:3.10-slim# Set $DAGSTER_HOME and copy dagster instance and workspace YAML thereENV DAGSTER_HOME=/opt/dagster/dagster_homeRUN mkdir -p $DAGSTER_HOME \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/storage \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/compute_logs \u0026\u0026 \\ mkdir -p $DAGSTER_HOME/local_artifact_storageWORKDIR$DAGSTER_HOMECOPY requirements.txt $DAGSTER_HOMERUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txt và requirement.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-dbt==0.17.20 DagitDagit là giao diện UI của Dagster, và ta sẽ thao tác trên Dagit để quản lý các assets, jobs,… de_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network Sau đó hãy tạo 1 folder ./dagster_home, trong đó tạo 2 file config cho workspace của dagster: dagster.yaml run_coordinator:module:dagster.core.run_coordinatorclass:QueuedRunCoordinatorconfig:max_concurrent_runs:3scheduler:module:dagster.core.schedulerclass:DagsterDaemonSchedulerconfig:max_catchup_runs:5storage:postgres:postgres_db:username:env:DAGSTER_PG_USERNAMEpassword:env:DAGSTER_PG_PASSWORDhostname:env:DAGSTER_PG_HOSTNAMEdb_name:env:DAGSTER_PG_DBport:5432run_launcher:module:dagster.core.launcherclass:DefaultRunLaunchercompute_logs:module:dagster.core.storage.local_compute_log_managerclass:LocalComputeLogManagerconfig:base_dir:/opt/dagster/dagster_home/compute_logslocal_artifact_storage:module:dagster.core.storage.rootclass:LocalArtifactStorageconfig:base_dir:/opt/dagster/dagster_home/local_artifact_storage và workspace.yaml load_from:- grpc_server:host:etl_pipelineport:4000location_name:\"etl_pipeline\" Dagster DeamonĐể quản lý các Schedules, sensors,… ta cần phải có dagster-deamon de_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:4","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#dagster-deamon"},{"categories":["projects"],"content":"PipelineTất cả mọi việc xây dựng pipeline ta sẽ hoạt động ở đây Đầu tiên ta cần init một dagster project dagster project scaffold --name etl_pipeline và thư mục mới tạo sẽ trông như thế này: Tip Để chạy được lệnh dagster ở bước tạo dagster project, ta cần phải có dagster package, nếu chưa có hãy cài đặt bằng: pip install dagster –\u003e Nên cài đặt trong môi trường ảo Sau đó vào thư mục vừa tạo vào viết Dockerfile thôi: FROMpython:3.10-slim# Add repository codeWORKDIR/opt/dagster/appCOPY requirements.txt /opt/dagster/appRUN pip install --upgrade pip \u0026\u0026 pip install -r requirements.txtWORKDIR/opt/dagster/appCOPY . /opt/dagster/app/etl_pipeline# CMD allows this to be overridden from run launchers or executors that want to run other commands against your repositoryCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"etl_pipeline\"] cùng với requirements.txt: dagster==1.1.20 dagit==1.1.20 dagster-postgres==0.17.20 dagster-aws==0.17.20 dagster-dbt==0.17.20 pandas==1.5.3 SQLAlchemy==1.4.46 pymysql==1.0.2 cryptography==39.0.0 pyarrow==10.0.1 boto3==1.26.57 fsspec==2023.1.0 s3fs==0.4.2 minio==7.1.13 Cuối cùng là viết vào docker-compose.yml: etl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:5","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#pipeline"},{"categories":["projects"],"content":"StreamlitCuối cùng là việc là Dashboard, Streamlit chắc chắc là công cụ siêu phù hợp làm việc này. Đây là framework hỗ trợ việc xây dựng giao diện ưu nhìn chỉ bằng Python, quá đã phải không nào :)) Hãy tạo folder ./streamlit/scr/ cùng với ./streamlit/Dockerfile: FROMpython:3.10EXPOSE8501WORKDIR/usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . . và streamlit/requirements.txt: pandas plotly matplotlib numpy streamlit psycopg2-binary sqlalchemy python-dotenv Cuối cùng là ghi trong yml streamlit:build:./streamymlcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_network ","date":"31 Jul 2023","objectID":"/football_etl/:2:6","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#streamlit"},{"categories":["projects"],"content":"Hoàn chỉnh setupCuối cùng, file yaml sẽ trông như thế này: # version: '3.9'services:minio:hostname:minioimage:minio/miniocontainer_name:miniovolumes:- ./MinIO/storage:/data- ./MinIO/config:/root/.minioports:- \"9000:9000\"- \"9090:9090\"env_file:- ./.envcommand:server /data --console-address \":9090\"networks:- de_networkmc:image:minio/mccontainer_name:mchostname:mcenv_file:- ./.enventrypoint:\u003e/bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' \u0026\u0026 sleep 1;done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; \"depends_on:- minionetworks:- de_networkde_mysql:image:mysql:8.0container_name:de_mysqlports:- \"3306:3306\"volumes:- ./storage/mysql_data:/var/lib/mysql- ./dataset:/tmp/datasetenv_file:- ./.envnetworks:- de_networkde_psql:container_name:de_psqlimage:postgres:15-alpineenv_file:- ./.envports:- '5432:5432'volumes:- ./storage/postgres_data:/var/lib/postgresql/datanetworks:- de_networkstreamlit:build:./streamlitcontainer_name:streamlitimage:streamlit:latestcommand:\"streamlit run src/main.py\"ports:- \"8501:8501\"volumes:- \"./streamlit/src:/usr/src/app/src\"networks:- de_networkde_dagster:build:context:./dagster/container_name:de_dagsterimage:de_dagsterde_dagster_dagit:image:de_dagster:latestentrypoint:- dagit- -h- \"0.0.0.0\"- -p- \"3001\"- -w- workspace.yamlcontainer_name:de_dagster_dagitexpose:- \"3001\"ports:- \"3001:3001\"volumes:# Make docker client accessible so we can terminate containers from dagit- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_networkde_dagster_daemon:image:de_dagster:latestentrypoint:- dagster-daemon- runcontainer_name:de_dagster_daemonvolumes:# Make docker client accessible so we can launch containers using host docker- /var/run/docker.sock:/var/run/docker.sock- ./dagster_home:/opt/dagster/dagster_homeenv_file:- ./.envnetworks:- de_network# Pipelinesetl_pipeline:build:context:./etl_pipelinedockerfile:./Dockerfilecontainer_name:etl_pipelineimage:etl_pipeline:latestvolumes:- ./etl_pipeline:/opt/dagster/appenv_file:- ./.envnetworks:- de_networknetworks:de_network:driver:bridgename:de_network Và .env file: # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_DB=football POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_HOST_AUTH_METHOD=trust # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=football # MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=football MYSQL_ROOT_PASSWORD=admin123 MYSQL_USER=admin MYSQL_PASSWORD=admin123 # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=warehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 Warning Nếu bạn chỉ đọc phần Hoàn chỉnh setup thì có thể hệ thống vễ sẽ gặp lỗi vì có thể thiếu các configuration cần thiết cho dagster, dagit hay pipeline. Bạn cần đọc qua phần Dagster, Dagit, Pipeline Chạy thử: sau khi hoàn tất toàn bộ set up dài ngoằn, cũng đã đến lúc chạy chương trình thôi. Note nho nhỏ Nếu bạn đã build lần lượt các images rồi, thì chỉ cần compose up thôi, nếu không, hãy build bằng lệnh docker compose build trước khi chạy compose up. docker compose --env-file .env up -d Lại là một tip có thể hữu ích Để đơn giản hóa việc ghi lệnh dài dòng, hãy tạo một make file tên Makefile với nội dung sau: include .env build: docker compose build up: docker compose --env-file .env up -d down: docker compose --env-file .env down restart: make down \u0026\u0026 make up to_psql: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} psql_create: docker exec -ti de_psql psql postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB} -f /tmp/psql_schema.sql to_mysql: docker exec -it de_mysql mysql --local-infile=1 -u\"${MYSQL_U","date":"31 Jul 2023","objectID":"/football_etl/:2:7","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#hoàn-chỉnh-setup"},{"categories":["projects"],"content":"To be ContinueBài đến đây cũng quá là dài rồi, mình sẽ viết tiếp ở phần 2 :))) Chúc bạn một ngày tốt lành -Mew- ","date":"31 Jul 2023","objectID":"/football_etl/:3:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#to-be-continue"},{"categories":["projects"],"content":"Related Content Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... Football ETL Analysis P2 Continuous of Football ETL series Read more... ","date":"31 Jul 2023","objectID":"/football_etl/:0:0","series":["Football ETL"],"tags":["data engineer","etl","dagster","docker","Tech","data pipeline","streamlit","MySQL","PostgreSQL","MinIO"],"title":"Football ETL Analysis","uri":"/football_etl/#related-content"},{"categories":[],"content":"Data Engineering Football ETL Analysis A Data Engineer project building pipeline to analyze football data Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#data-engineering"},{"categories":[],"content":"Machine learning Basic Stock Analysis Basic Analyzing VN30 stock using PCA and K-Means Read more... ","date":"31 Jul 2023","objectID":"/projects/:0:0","series":[],"tags":[],"title":"Projects","uri":"/projects/#machine-learning-basic"},{"categories":null,"content":" Hello! Mình tên là Vĩnh Phong hay chính là OG (tác giả của các blogs ở trang này) Hiện tại mình là sinh viên ngành Khoa học Dữ liệu (Data Science) tại Đại học Khoa học Tự nhiên, ĐHQG-HCM (HCMUS). Tuy nhiên mình cũng yêu thích công nghệ, code và mình đang trên con đường học tập mỗi ngày để trở thành một Data Engineer. OG hồi cuối lớp 12OG hồi cúi lớp 12 :)) \" OG hồi cuối lớp 12 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#"},{"categories":null,"content":"Mình của hiện tạiĐầu tiên quan trọng nhất chính là việc học tại HCMUS. Ngoài ra, mình còn tự học về các chủ đề liên quan như Data pipeline, Data Streaming,… Việc luyện tập, học hỏi cũng ăn sâu dô máu mình lúc nào không hay :)) Mình đã từng đọc thấy đâu đó rằng: The most beautiful thing about learning is that no one take that away from you Và điều đó đã truyền cảm hứng mình rất nhiều, thúc đẩy bản thân tự học mỗi ngày và tự làm mới bản thân từng chút một. Tiếp đến chính là xây dựng trang web này và viết các bài blogs giúp cho các bạn có thể học thêm kiến thức ngành, biết thêm điều thú vị và thư giãn. Hiện tại thì OG vẫn còn ngồi trên ghế giảng đường, và vừa đặt những viên gạch đầu tiên trên con đường tự trưởng thành. Hãy luôn theo dõi OG nhé! -Mew- Contact me: Mail: phonghuynh9403@gmail.com Linkedin: Huỳnh Lưu Vĩnh Phong Facebook: Phong Huynh Instagram: phong_huynh Hoặc bạn cũng có thể ghé thăm Github của mình: PhongHuynh0394 ","date":"30 Jul 2023","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#mình-của-hiện-tại"}]